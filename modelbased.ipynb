{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisCGit/6.484-reinforcement-learning/blob/main/modelbased.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CewK6baf1nzF"
      },
      "source": [
        "# Spring 2022 6.484 Computational Sensorimotor Learning Assignment 6\n",
        "\n",
        "In this assignment, we will implement model-based control algorithms for Cartpole swing-up.\n",
        "\n",
        "You will need to **answer the bolded questions** and **fill in the missing code snippets** (marked by **TODO**).\n",
        "\n",
        "There are **210** total points to be had in this PSET, plus 10 bonus points for filling out the survey.  `ctrl-f` for \"pts\" to ensure you don't miss questions.\n",
        "\n",
        "**_Please fill in your name below:_**\n",
        "\n",
        "**Name**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f0fX3X691oJ"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y ffmpeg > /dev/null 2>&1\n",
        "!pip install gym > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoXRypPY-rv5"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.axes import Axes\n",
        "from matplotlib import rc\n",
        "import random\n",
        "from torch import nn\n",
        "from gym.envs.registration import registry, register\n",
        "\n",
        "rc('animation', html='jshtml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Lokf2a19Dm"
      },
      "source": [
        "# Environment\n",
        "\n",
        "For this assignment, our task will be to swing-up and balance Cartpole (note that this is different from the standard cartpole gym env, where the stick starts in the upwards orientation).  As a backbone to our gym environment, we provide the class `CartpoleDynamics` which implements the ground truth cartpole dynamics in Pytorch (for speed).\n",
        "\n",
        "Note that in the controls literature, $q$ is commnly referred to as the state and $u$ as the control or input or action.\n",
        "\n",
        "Note that our observation space in this environment is of size 4 and the action space is of size 1.\n",
        "\n",
        "*Cartpole plotting and animation code inspired by [Shunichi09/PythonLinearNonLinearControl](https://github.com/Shunichi09/PythonLinearNonlinearControl)* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJsKFPO7-sKM"
      },
      "outputs": [],
      "source": [
        "class CartpoleDynamics:\n",
        "    def __init__(self,\n",
        "                 timestep=0.02,\n",
        "                 m_p=0.5,\n",
        "                 m_c=0.5,\n",
        "                 l=0.6,\n",
        "                 g=-9.81,\n",
        "                 u_range=15):\n",
        "        \n",
        "        self.m_p  = m_p\n",
        "        self.m_c  = m_c\n",
        "        self.l    = l\n",
        "        self.g    = -g\n",
        "        self.dt   = timestep\n",
        "        \n",
        "        self.u_range = u_range\n",
        "\n",
        "        self.u_lb = torch.tensor([-1]).float()\n",
        "        self.u_ub = torch.tensor([1]).float()\n",
        "        self.q_shape = 4\n",
        "        self.u_shape = 1\n",
        "    \n",
        "    def _qdotdot(self, q, u):\n",
        "        x, theta, xdot, thetadot = q.T\n",
        "\n",
        "        if len(u.shape) == 2:\n",
        "            u = torch.flatten(u)\n",
        "        \n",
        "        x_dotdot = (\n",
        "            u + self.m_p * torch.sin(theta) * (\n",
        "                self.l * torch.pow(thetadot,2) + self.g * torch.cos(theta)\n",
        "            )\n",
        "        ) / (self.m_c + self.m_p * torch.sin(theta)**2)\n",
        "        \n",
        "        theta_dotdot = (\n",
        "            -u*torch.cos(theta) -\n",
        "            self.m_p * self.l * torch.pow(thetadot,2) * torch.cos(theta) * torch.sin(theta) - \n",
        "            (self.m_c + self.m_p) * self.g * torch.sin(theta)\n",
        "        ) / (self.l * (self.m_c + self.m_p * torch.sin(theta)**2))\n",
        "                \n",
        "        return torch.stack((x_dotdot, theta_dotdot), dim=-1)\n",
        "    \n",
        "    def _euler_int(self, q, qdotdot):\n",
        "        qdot_new = q[...,2:] + qdotdot * self.dt\n",
        "        q_new = q[...,:2] + self.dt * qdot_new\n",
        "\n",
        "        return torch.cat((q_new, qdot_new), dim=-1)\n",
        "    \n",
        "    def step(self, q, u):\n",
        "\n",
        "        # Check for numpy array\n",
        "        if isinstance(q, np.ndarray):\n",
        "            q = torch.from_numpy(q)\n",
        "        if isinstance(u, np.ndarray):\n",
        "            u = torch.from_numpy(u)\n",
        "\n",
        "        scaled_u = u * float(self.u_range)    \n",
        "            \n",
        "        # Check for shape issues\n",
        "        if len(q.shape) == 2:\n",
        "            q_dotdot = self._qdotdot(q, scaled_u)\n",
        "        elif len(q.shape) == 1:\n",
        "            q_dotdot = self._qdotdot(q.reshape(1,-1), scaled_u)\n",
        "        else:\n",
        "            raise RuntimeError('Invalid q shape')\n",
        "            \n",
        "        new_q = self._euler_int(q, q_dotdot)\n",
        "\n",
        "        if len(q.shape) == 1:\n",
        "            new_q = new_q[0]\n",
        "            \n",
        "        return new_q\n",
        "    \n",
        "    # given q [bs, q_shape] and u [bs, t, u_shape] run the trajectories\n",
        "    def run_batch_of_trajectories(self, q, u):\n",
        "        qs = [q]\n",
        "        \n",
        "        for t in range(u.shape[1]):\n",
        "            qs.append(self.step(qs[-1], u[:,t]))\n",
        "                \n",
        "        return torch.stack(qs, dim=1)\n",
        "    \n",
        "    # given q [bs, t, q_shape] and u [bs, t, u_shape] calculate the rewards\n",
        "    def reward(self, q, u):\n",
        "        if isinstance(q, np.ndarray):\n",
        "            q = torch.from_numpy(q)\n",
        "        if isinstance(u, np.ndarray):\n",
        "            u = torch.from_numpy(u)\n",
        "\n",
        "        angle_term = 0.5*(1-torch.cos(q[...,1]))\n",
        "        pos_term = -0.5*torch.pow(q[...,0],2)\n",
        "        ctrl_cost = -0.001*(u**2).sum(dim=-1)\n",
        "                        \n",
        "        return angle_term + pos_term + ctrl_cost\n",
        "\n",
        "class CartpoleGym(gym.Env):\n",
        "    def __init__(self, timestep_limit=200):\n",
        "        self.dynamics = CartpoleDynamics()\n",
        "        \n",
        "        self.timestep_limit = timestep_limit\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.q_sim = np.zeros(4)\n",
        "        self.timesteps = 0\n",
        "\n",
        "        self.traj = [self.get_observation()]\n",
        "\n",
        "        return self.traj[-1]\n",
        "\n",
        "    def get_observation(self):\n",
        "        return self.q_sim\n",
        "            \n",
        "    def step(self, action):\n",
        "        action = np.clip(action, self.action_space.low, self.action_space.high)[0]\n",
        "        \n",
        "        new_q = self.dynamics.step(\n",
        "            self.q_sim, action\n",
        "        )\n",
        "        \n",
        "        if not isinstance(action, torch.Tensor):\n",
        "            action = torch.tensor(action)\n",
        "        \n",
        "        reward = self.dynamics.reward(\n",
        "            new_q, action\n",
        "        ).numpy()\n",
        "        \n",
        "        self.q_sim = new_q.numpy()\n",
        "        done = self.is_done()\n",
        "        \n",
        "        self.timesteps += 1\n",
        "\n",
        "        self.traj.append(self.q_sim)\n",
        "        \n",
        "        return self.q_sim, reward, done, {}\n",
        "    \n",
        "    def is_done(self):\n",
        "        # Kill trial when too much time has passed\n",
        "        if self.timesteps >= self.timestep_limit:\n",
        "            return True\n",
        "                \n",
        "        return False\n",
        "    \n",
        "    def plot_func(self, to_plot, i=None):\n",
        "        def _square(center_x, center_y, shape, angle):\n",
        "            trans_points = np.array([\n",
        "                [shape[0], shape[1]],\n",
        "                [-shape[0], shape[1]],\n",
        "                [-shape[0], -shape[1]],\n",
        "                [shape[0], -shape[1]],\n",
        "                [shape[0], shape[1]]\n",
        "            ]) @ np.array([\n",
        "                [np.cos(angle), np.sin(angle)],\n",
        "                [-np.sin(angle), np.cos(angle)]\n",
        "            ]) + np.array([center_x, center_y])\n",
        "\n",
        "            return trans_points[:, 0], trans_points[:, 1]\n",
        "        \n",
        "        if isinstance(to_plot, Axes):\n",
        "            imgs = dict(\n",
        "                cart=to_plot.plot([], [], c=\"k\")[0],\n",
        "                pole=to_plot.plot([], [], c=\"k\", linewidth=5)[0],\n",
        "                center=to_plot.plot([], [], marker=\"o\", c=\"k\",\n",
        "                                          markersize=10)[0]\n",
        "            )\n",
        "\n",
        "            x_width = max(1,max(np.abs(t[0]) for t in self.traj) * 1.3)\n",
        "\n",
        "            # centerline\n",
        "            to_plot.plot(np.linspace(-x_width, x_width, num=50), np.zeros(50),\n",
        "                         c=\"k\", linestyle=\"dashed\")\n",
        "\n",
        "            # set axis\n",
        "            to_plot.set_xlim([-x_width, x_width])\n",
        "            to_plot.set_ylim([-self.dynamics.l*1.2, self.dynamics.l*1.2])\n",
        "\n",
        "            return imgs\n",
        "\n",
        "        curr_x = self.traj[i]\n",
        "\n",
        "        cart_size = (0.15, 0.1)\n",
        "\n",
        "        cart_x, cart_y = _square(curr_x[0], 0.,\n",
        "                                cart_size, 0.)\n",
        "\n",
        "        pole_x = np.array([curr_x[0], curr_x[0] + self.dynamics.l\n",
        "                           * np.cos(curr_x[1]-np.pi/2)])\n",
        "        pole_y = np.array([0., self.dynamics.l\n",
        "                           * np.sin(curr_x[1]-np.pi/2)])\n",
        "\n",
        "        to_plot[\"cart\"].set_data(cart_x, cart_y)\n",
        "        to_plot[\"pole\"].set_data(pole_x, pole_y)\n",
        "        to_plot[\"center\"].set_data(self.traj[i][0], 0.)\n",
        "    \n",
        "    def render(self, mode=\"human\"):\n",
        "        self.anim_fig = plt.figure()\n",
        "\n",
        "        self.axis = self.anim_fig.add_subplot(111)\n",
        "        self.axis.set_aspect('equal', adjustable='box')\n",
        "\n",
        "        imgs = self.plot_func(self.axis)\n",
        "        _update_img = lambda i: self.plot_func(imgs, i)\n",
        "\n",
        "        Writer = animation.writers['ffmpeg']\n",
        "        writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
        "\n",
        "        ani = FuncAnimation(\n",
        "            self.anim_fig, _update_img, interval=self.dynamics.dt*1000, \n",
        "            frames=len(self.traj)-1\n",
        "        )\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "        return ani\n",
        "        \n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return gym.spaces.Box(low=self.dynamics.u_lb.numpy(), high=self.dynamics.u_ub.numpy())\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        return gym.spaces.Box(\n",
        "            low= np.array([-np.inf, -np.inf, -np.inf, -np.inf]),\n",
        "            high=np.array([np.inf,   np.inf,  np.inf,  np.inf])\n",
        "        )\n",
        "\n",
        "env_name = 'CartpoleSwingUp-v0'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{__name__}:CartpoleGym',\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA8sEUMr3bXM"
      },
      "source": [
        "As a demonstration, let's try to plot a random policy on this environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoJDghtC3hfM"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartpoleSwingUp-v0')\n",
        "\n",
        "q = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    q, r, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG0zJF2Z7uer"
      },
      "source": [
        "# Model Predictive Control\n",
        "\n",
        "Model-based RL works by decomposing policy into two components:\n",
        "\n",
        "1. A model of the MDP, learned from data\n",
        "2. A planner, that given the learned MDP, can produce optimal action estimates.\n",
        "\n",
        "Let's begin by implementing step 2: the planner.  As we're in the world of continuous control, we will implement a *Model Predictive Controller*: at every timestep we will solve for an optimal trajectory, than take the first action.\n",
        "\n",
        "One simple MPC method is Cross Entropy Maximization (CEM), as used in [PETS](https://arxiv.org/pdf/1805.12114.pdf).  The idea here is to continuously optimize a rolling Gaussian input trajectory segment.  At each timestep, advance the trajectory segment by one, generate a population of new input segments by sampling from the last input segment, and evaluate each trajectory.  Then, select the top `es_elites` trajectories from that population with the highest reward, fit the input trajectory segment to their distribution, and repeat for `es_generations` generations.  If the input trajectory variance reaches below a threshold `es_epsilon`, exit early.\n",
        "\n",
        "Implement a CEM MPC controller, using the given parameters defined in `__init__` and both `self.model.run_batch_of_trajectories` and `self.model.reward` to evaluate trajectories.\n",
        "\n",
        "Note that `run_batch_of_trajectories` takes as input a batch of initial states $(bs, u\\_size)$ and a trajectory of actions $(bs, t, q\\_size)$ and returns a batch of trajectories $(bs, t + 1)$. Each trajectory in the batch is generated by sequentially applying each action in the trajectory to updated states (starting with the initial inputted state). `reward` takes a sequence of states $(bs, t, q\\_size)$ and a sequence of actions $(bs, t, u\\_size)$ applied to get to that state and returns a reward $(bs, t)$, consisting of a term based off the position, angle, and action magnitude. \n",
        "\n",
        "**(50 pts)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWg2PAViMfBi"
      },
      "outputs": [],
      "source": [
        "class MPC:\n",
        "    def __init__(self,\n",
        "        model,\n",
        "        horizon        = 25,\n",
        "        es_epsilon     = 0.001,\n",
        "        es_alpha       = 0.1,\n",
        "        es_generations = 5,\n",
        "        es_popsize     = 200,\n",
        "        es_elites      = 40\n",
        "    ):\n",
        "                \n",
        "        self.model          = model\n",
        "        \n",
        "        self.horizon        = horizon        # planning horizon\n",
        "        self.es_epsilon     = es_epsilon     # variance threshold\n",
        "        self.es_alpha       = es_alpha       # new distribution rolling average coefficient\n",
        "        self.es_generations = es_generations # num generations for ES optimizer\n",
        "        self.es_popsize     = es_popsize     # popsize for ES optimizer\n",
        "        self.es_elites      = es_elites      # num of elites from which to resample\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Initialize action trajectory distribution\n",
        "        self.sol_mean = ((self.model.u_lb + self.model.u_ub) / 2).expand(self.horizon,-1)\n",
        "        self.sol_var = ((self.model.u_ub - self.model.u_lb) / 16).expand(self.horizon,-1)\n",
        "        \n",
        "        self.timestep = 0\n",
        "    \n",
        "    def action(self, q):\n",
        "        # Remove last taken action and add 0 to end of buffer\n",
        "        self.sol_mean = torch.cat([\n",
        "            self.sol_mean[1:],\n",
        "            torch.zeros(self.model.u_shape).reshape(1,-1)\n",
        "        ])        \n",
        "                \n",
        "        # Generate standard diagonal normal distribution from which we sample trajectory noise        \n",
        "        u_dist = torch.distributions.normal.Normal(\n",
        "            loc=torch.zeros_like(self.sol_mean), \n",
        "            scale=torch.ones_like(self.sol_var)\n",
        "        )\n",
        "        \n",
        "        var = self.sol_var\n",
        "        for n in range(self.es_generations):            \n",
        "            # Terminate if variance drops below threshold\n",
        "            if torch.max(var) < self.es_epsilon:\n",
        "                print(f'var below threshold! exiting {n}')\n",
        "                break\n",
        "\n",
        "            lb_dist = self.sol_mean - self.model.u_lb\n",
        "            ub_dist = self.model.u_ub - self.sol_mean\n",
        "            constrained_var = torch.min(\n",
        "                torch.min((lb_dist / 2)**2, (ub_dist / 2)**2), var\n",
        "            )\n",
        "            \n",
        "            ### TODO: perform one ES trajectory optimization step, by\n",
        "            ### 1. sampling a trajectory\n",
        "            ### 2. evaluating the fitness of that trajectory on the model\n",
        "            ### 3. re-fitting self.sol_mean for the top N elites of the model\n",
        "            ### (50 pts)\n",
        "            \n",
        "            ### ENDTODO\n",
        "\n",
        "            n += 1\n",
        "        \n",
        "        self.timestep += 1\n",
        "        return self.sol_mean[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9L_xx8r_rbr"
      },
      "source": [
        "Now, let's evaluate your MPC controller on the ground truth cartpole dynamics `CartpoleDynamics`.  The cartpole should swing up on the first (and only) epoch; there's no learning here, as the dynamics model is known."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts9u71A3tL04",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "mpc = MPC(CartpoleDynamics())\n",
        "e = gym.make('CartpoleSwingUp-v0')\n",
        "\n",
        "q, r, done = e.reset(), 0, False\n",
        "\n",
        "while not done:\n",
        "    q, reward, done, _ = e.step(mpc.action(q))\n",
        "    r += reward\n",
        "    \n",
        "print('Got reward:', r)\n",
        "e.render()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_4LJFXYe22R"
      },
      "source": [
        "**Question**: Try varying the time horizon for MPC planning.  What behaviors do you see? (10 pts)\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "**Question**: Try varying the number of random seeds for MPC (`es_popsize`).  What behaviors do you see? (10 pts)\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x02gg7eKAOfi"
      },
      "source": [
        "# Learned Dynamics Model\n",
        "\n",
        "Now that we've debugged our MPC module, let's implement a learned model, so we no longer need the ground truth dynamics.  Our goal here is to learn some model $\\hat f(q_t, u_t) = \\hat q_{t+1}$, rather than implementing it analytically. \n",
        "\n",
        "Some hints:\n",
        "\n",
        "- Consider normalizing values as necessary.\n",
        "- For angles, consider replacing the angle in your feature space with `sin` and `cos` of the angle\n",
        "- Consider learning the model $\\hat f$ as $\\hat f(q_t, u_t) = \\hat g(q_t, u_t) + q_t$, rather than predicting new states from scratch (i.e., learn some residual instead)\n",
        "\n",
        "**(50 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rENQVcjOtUA-"
      },
      "outputs": [],
      "source": [
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "  \n",
        "class LearnedCartpoleDynamics:\n",
        "    def __init__(self, seed, u_range=15):\n",
        "        set_random_seed(seed)\n",
        "        self.u_range = u_range\n",
        "\n",
        "        self.u_lb = torch.tensor([-u_range]).float()\n",
        "        self.u_ub = torch.tensor([u_range]).float()\n",
        "        self.u_shape = 1\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(6, 48),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 48),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 4),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.optim = torch.optim.Adam(self.model.parameters())\n",
        "        self.loss = nn.MSELoss()\n",
        "        \n",
        "    def step(self, q, u):\n",
        "        ### TODO: implement forward dynamics call of f(q, u) -> qprime\n",
        "\n",
        "        ### ENDTODO\n",
        "\n",
        "\n",
        "    # given q [n, q_shape] and u [n, t] run the trajectories\n",
        "    def run_batch_of_trajectories(self, q, u):\n",
        "        qs = [q]\n",
        "        \n",
        "        for t in range(u.shape[1]):\n",
        "            qs.append(self.step(qs[-1], u[:,t]))\n",
        "                \n",
        "        return torch.stack(qs, dim=1)\n",
        "\n",
        "    def train(self, q_t_traj, q_tplusone_traj, u_traj):\n",
        "        batch_size = 16\n",
        "        num_batches = 1024\n",
        "\n",
        "        all_batch_idxs = np.random.randint(\n",
        "            len(q_t_traj), size=(num_batches, batch_size)\n",
        "        )\n",
        "\n",
        "        for b in range(num_batches):\n",
        "            batch_idxs = all_batch_idxs[b]\n",
        "            q, qprime, u = torch.from_numpy(q_t_traj[batch_idxs]).float(), torch.from_numpy(q_tplusone_traj[batch_idxs]).float(), torch.from_numpy(u_traj[batch_idxs]).float()\n",
        "\n",
        "            ### TODO: train your forward dynamics model by minimizing the mean squared error between\n",
        "            ###       predicted future states (given a current state and control) and actual future states\n",
        "            ###\n",
        "            ### note: we've already defined the optimizer in self.optim and the loss function in self.loss\n",
        "            self.optim.zero_grad()\n",
        "\n",
        "            qprime_hat = self.step(q, u)\n",
        "\n",
        "            loss = self.loss(qprime, qprime_hat)\n",
        "            loss.backward()\n",
        "            self.optim.step()\n",
        "            ### ENDTODO\n",
        "\n",
        "    def reward(self, q, u):\n",
        "        if isinstance(q, np.ndarray):\n",
        "            q = torch.from_numpy(q)\n",
        "        if isinstance(u, np.ndarray):\n",
        "            u = torch.from_numpy(u)\n",
        "\n",
        "        angle_term = 0.5*(1-torch.cos(q[...,1]))\n",
        "        pos_term = -0.5*torch.pow(q[...,0],2)\n",
        "        ctrl_cost = -0.001*(u**2).sum(dim=-1)\n",
        "                        \n",
        "        return angle_term + pos_term + ctrl_cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8hLquA-BRhF"
      },
      "source": [
        "Let's see how we do!  Run the below cell to run a trial where you \n",
        "1. roll-out an episode using the MPC controller on the learned dynamics model\n",
        "2. use the newly gathered experience to update the dynamics model\n",
        "3. goto step 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mCH3VzccNts",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "dynamics_model = LearnedCartpoleDynamics(seed=0)\n",
        "mpc = MPC(dynamics_model)\n",
        "e = CartpoleGym()\n",
        "\n",
        "all_q, all_q_prime, all_u = None, None, None\n",
        "for epoch in range(100):\n",
        "    q_traj = [e.reset()]\n",
        "    u_traj = []\n",
        "\n",
        "    r = 0\n",
        "    while True:\n",
        "        u_traj.append(mpc.action(q_traj[-1])[0].item())\n",
        "        q, reward, done, _ = e.step(u_traj[-1])\n",
        "        r += reward\n",
        "        q_traj.append(q)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    q_traj = np.array(q_traj)\n",
        "    u_traj = np.array(u_traj)\n",
        "\n",
        "    print(f'[Epoch {epoch}] Got reward {r}')\n",
        "    q, q_prime, u = q_traj[:-1], q_traj[1:], u_traj\n",
        "\n",
        "    if all_q is None:\n",
        "        all_q, all_q_prime, all_u = q, q_prime, u\n",
        "    else:\n",
        "        all_q = np.concatenate((all_q, q))\n",
        "        all_q_prime = np.concatenate((all_q_prime, q_prime))\n",
        "        all_u = np.concatenate((all_u, u))\n",
        "\n",
        "    dynamics_model.train(all_q, all_q_prime, all_u)\n",
        "\n",
        "    if r > 80:\n",
        "        break\n",
        "\n",
        "e.render()    \n",
        "e.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slBXL4pkBatV"
      },
      "source": [
        "**Question:** What is the sample complexity of the model-based algorithm? (10 pts)\n",
        "\n",
        "**Answer:** \n",
        "\n",
        "**Question:** Compare the sample complexity of our model-based approach to methods from previous problem sets, such as PPO. (10 pts)\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5xX12AGtmU-"
      },
      "source": [
        "# Ensembling\n",
        "\n",
        "Now, instead of using just one single learned forward dynamics model, let's utilize an **ensemble** of forward dynamics models as done in [PETS](https://arxiv.org/pdf/1805.12114.pdf). We will have multiple dynamics model networks, as defined in the last section, that are randomly initialized. \n",
        "\n",
        "Like before, for each of these models we will generate trajectories given the same initial state and sampled control trajectory. We will rank all of our models by their **best** performing elite trajectories, and then use the elites of the **worst** performing model to update our sampling each generation. This conservative approach should lead to more stable behavior.\n",
        "\n",
        "**(50 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoT5xoYvtmU-"
      },
      "outputs": [],
      "source": [
        "class EnsembleMPC:\n",
        "    def __init__(self,\n",
        "        models,\n",
        "        horizon        = 25,\n",
        "        es_epsilon     = 0.001,\n",
        "        es_alpha       = 0.1,\n",
        "        es_generations = 5,\n",
        "        es_popsize     = 200,\n",
        "        es_elites      = 40\n",
        "    ):\n",
        "                \n",
        "        self.models          = models\n",
        "        self.num_in_ensemble = len(self.models)\n",
        "        \n",
        "        self.horizon        = horizon        # planning horizon\n",
        "        self.es_epsilon     = es_epsilon     # variance threshold\n",
        "        self.es_alpha       = es_alpha       # new distribution rolling average coefficient\n",
        "        self.es_generations = es_generations # num generations for ES optimizer\n",
        "        self.es_popsize     = es_popsize     # popsize for ES optimizer\n",
        "        self.es_elites      = es_elites      # num of elites from which to resample\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Initialize action trajectory distribution\n",
        "        self.sol_mean = ((self.models[0].u_lb + self.models[0].u_ub) / 2).expand(self.horizon,-1)\n",
        "        self.sol_var = ((self.models[0].u_ub - self.models[0].u_lb) / 16).expand(self.horizon,-1)\n",
        "        \n",
        "        self.timestep = 0\n",
        "    \n",
        "    def action(self, q):\n",
        "        # Remove last taken action and add 0 to end of buffer\n",
        "        self.sol_mean = torch.cat([\n",
        "            self.sol_mean[1:],\n",
        "            torch.zeros(self.models[0].u_shape).reshape(1,-1)\n",
        "        ])        \n",
        "                \n",
        "        # Generate standard diagonal normal distribution from which we sample trajectory noise        \n",
        "        u_dist = torch.distributions.normal.Normal(\n",
        "            loc=torch.zeros_like(self.sol_mean), \n",
        "            scale=torch.ones_like(self.sol_var)\n",
        "        )\n",
        "        \n",
        "        var = self.sol_var\n",
        "        for n in range(self.es_generations):            \n",
        "            # Terminate if variance drops below threshold\n",
        "            if torch.max(var) < self.es_epsilon:\n",
        "                print(f'var below threshold! exiting {n}')\n",
        "                break\n",
        "\n",
        "            lb_dist = self.sol_mean - self.models[0].u_lb\n",
        "            ub_dist = self.models[0].u_ub - self.sol_mean\n",
        "            constrained_var = torch.min(\n",
        "                torch.min((lb_dist / 2)**2, (ub_dist / 2)**2), var\n",
        "            )\n",
        "            \n",
        "            ### TODO: perform one ES trajectory optimization step, by\n",
        "            ### 1. sampling a trajectory from each model using the same initial state and \n",
        "            ###    sampled control trajectory\n",
        "            ### 2. evaluating the fitness of that trajectory on all models\n",
        "            ### 3. calculating the average reward of the top N elites from each model\n",
        "            ### 4. re-fitting self.sol_mean for the top N elites of the model with the lowest\n",
        "            ###    average elite reward (i.e., taking a conservative action)\n",
        "            ### (50 pts)\n",
        "            \n",
        "            ### ENDTODO\n",
        "\n",
        "            n += 1\n",
        "        \n",
        "        self.timestep += 1\n",
        "        return self.sol_mean[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "SsrrsWxotmU_"
      },
      "outputs": [],
      "source": [
        "num_in_ensemble = 5\n",
        "dynamics_models = [LearnedCartpoleDynamics(seed=idx) for idx in range(num_in_ensemble)]\n",
        "mpc_ensemble = EnsembleMPC(dynamics_models)\n",
        "e = CartpoleGym()\n",
        "\n",
        "all_q, all_q_prime, all_u = None, None, None\n",
        "\n",
        "for epoch in range(100):\n",
        "    q_traj = [e.reset()]\n",
        "    u_traj = []\n",
        "\n",
        "    r = 0\n",
        "    while True:\n",
        "        u_traj.append(mpc_ensemble.action(q_traj[-1])[0].item())\n",
        "        q, reward, done, _ = e.step(u_traj[-1])\n",
        "        r += reward\n",
        "        q_traj.append(q)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    q_traj = np.array(q_traj)\n",
        "    u_traj = np.array(u_traj)\n",
        "\n",
        "    print(f'[Epoch {epoch}] Got reward {r}')\n",
        "    q, q_prime, u = q_traj[:-1], q_traj[1:], u_traj\n",
        "\n",
        "    if all_q is None:\n",
        "        all_q, all_q_prime, all_u = q, q_prime, u\n",
        "    else:\n",
        "        all_q = np.concatenate((all_q, q))\n",
        "        all_q_prime = np.concatenate((all_q_prime, q_prime))\n",
        "        all_u = np.concatenate((all_u, u))\n",
        "\n",
        "    for idx in range(num_in_ensemble):\n",
        "        dynamics_models[idx].train(all_q, all_q_prime, all_u)\n",
        "\n",
        "    if r > 80:\n",
        "        break\n",
        "\n",
        "e.render()\n",
        "e.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sScihbGItmVA"
      },
      "source": [
        "**Question:** What is the sample complexity of the ensemble model-based algorithm? (10 pts)\n",
        "\n",
        "**Answer:** \n",
        "\n",
        "**Question:** How does the ensemble method compare to the method without ensembling? (10 pts)\n",
        "\n",
        "**Answer:** \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ4i-oSCtmVA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHrrywYWg8Gt"
      },
      "source": [
        "# Feedback Survey (optional)\n",
        "\n",
        "Please enter the bonus code you get after filling out the [anonymous assignment survey](https://docs.google.com/forms/d/e/1FAIpQLSdrLaE4j_PErZU4nSyLYPGhUBREzhqfs79rhWUfRoGFy2BjSA/viewform?usp=sf_link). (10 pts).\n",
        "\n",
        "**Bonus code**: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SJ0McixtmVB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "modelbased.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}