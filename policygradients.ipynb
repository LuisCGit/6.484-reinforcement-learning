{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisCGit/6.484-reinforcement-learning/blob/main/policygradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AGouWesMiIR"
      },
      "source": [
        "# Spring 2022 6.884 Computational Sensorimotor Learning Assignment 2\n",
        "\n",
        "In this assignment, we will implement model-free RL algorithms from scratch to solve `DoorKeyEnv5x5`.  We will cover:\n",
        "\n",
        "\n",
        "* REINFORCE\n",
        "* Vanilla Policy Gradient (VPG)\n",
        "* Generalized Advantage Estimation (GAE)\n",
        "* Proximal Policy Optimization (PPO)\n",
        "\n",
        "You will need to **answer the bolded questions** and **fill in the missing code snippets** (marked by **TODO**).\n",
        "\n",
        "There are (approximately) **150** total points to be had in this PSET.  `ctrl-f` for \"pts\" to ensure you don't miss questions.\n",
        "\n",
        "**_Please fill in your name below:_**\n",
        "\n",
        "**Name**: Luis Costa Laveron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRdsOe1INwSL"
      },
      "source": [
        "## Setup\n",
        "\n",
        "The following code sets up requirements, imports, and helper functions (you can ignore this)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jX9eHiRsbDdX"
      },
      "outputs": [],
      "source": [
        "!pip install gym-minigrid &>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oZDM2oukbDdc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torch.nn.functional as F\n",
        "import gym_minigrid\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from tqdm.notebook import tqdm\n",
        "from gym_minigrid.envs.doorkey import DoorKeyEnv\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P6yYDc3bN6DU"
      },
      "outputs": [],
      "source": [
        "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
        "def init_params(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Linear\") != -1:\n",
        "        m.weight.data.normal_(0, 1)\n",
        "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "def preprocess_obss(obss, device=None):\n",
        "    if isinstance(obss, dict):\n",
        "        images = np.array([obss[\"image\"]])\n",
        "    else:\n",
        "        images = np.array([o[\"image\"] for o in obss])\n",
        "    \n",
        "    return torch.tensor(images, device=device, dtype=torch.float)\n",
        "\n",
        "class DoorKeyEnv5x5(DoorKeyEnv):\n",
        "    def __init__(self):\n",
        "        super().__init__(size=5)\n",
        "    \n",
        "    def _reward(self):\n",
        "        \"\"\"\n",
        "        Compute the reward to be given upon success\n",
        "        \"\"\"\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Dl6tYW2aJ3Ji"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self,\n",
        "                score_threshold=0.93,\n",
        "                discount=0.995,\n",
        "                lr=1e-3,\n",
        "                max_grad_norm=0.5,\n",
        "                log_interval=10,\n",
        "                max_episodes=2000,\n",
        "                gae_lambda=0.95,\n",
        "                use_critic=False,\n",
        "                clip_ratio=0.2,\n",
        "                target_kl=0.01,\n",
        "                train_ac_iters=5,\n",
        "                use_discounted_reward=False,\n",
        "                entropy_coef=0.01,\n",
        "                use_gae=False):\n",
        "        \n",
        "        self.score_threshold = score_threshold\n",
        "        self.discount = discount\n",
        "        self.lr = lr\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.log_interval = log_interval\n",
        "        self.max_episodes = max_episodes\n",
        "        self.use_critic = use_critic\n",
        "        self.clip_ratio = clip_ratio\n",
        "        self.target_kl = target_kl\n",
        "        self.train_ac_iters = train_ac_iters\n",
        "        self.gae_lambda=gae_lambda\n",
        "        self.use_discounted_reward=use_discounted_reward\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.use_gae = use_gae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98WOwXCKP54C"
      },
      "source": [
        "# Task (Environment)\n",
        "\n",
        "In this assignment, we will work with the `DoorKeyEnv5x5` environment from [gym_miniworld](https://github.com/maximecb/gym-minigrid). This environment is a $5\\times 5$ gridworld. The agent needs to pick up the key, open the door, and then go the the green cell. The agent gets a $+1$ reward if it reaches the green cell, and a $0$ reward otherwise.\n",
        "\n",
        "The environment is visually shown below:\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CvgnCnitP54D"
      },
      "outputs": [],
      "source": [
        "env = DoorKeyEnv5x5()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD17fnXWP54D"
      },
      "source": [
        "**Question**: What does `env.reset()` return? What does each item returned mean? What's the shape of the image in the observation? How about the action space? What does each action mean? (Hint: You may find the source code of gym_minigrid helpful.) (10 pts)\n",
        "\n",
        "**Answer**: `env.reset()` resets the state of the grid world and returns a first observation. The observation comes in the form of a dictionary with the following items:\n",
        "\n",
        "*   'direction' : the agent's direction/orientation (acting as a compass). This is an integer, where 0 is facing right, 1 is down, 2 is left and 3 is up.\n",
        "*   'image' : a partially observable view of the environment. It is a 3D array with dimensions for width, height and channels (the channels have type, color and state information)\n",
        "*   'mission' : a textual mission string (instructions for the agent)\n",
        "\n",
        "* There are 7 available actions to the agent:\n",
        "  * 0 : turn left\n",
        "  * 1 : turn right\n",
        "  * 2 : move forward \n",
        "  * 3 : pick up an object \n",
        "  * 4 : drop an object \n",
        "  * 5 : toggle/activate an object \n",
        "  * 6 : done completing task \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBTY5ky9N8uC"
      },
      "source": [
        "# Model\n",
        "\n",
        "In policy gradients, we will directly learn a policy: i.e, for each state, predict an action!  We call this policy network the *actor*.\n",
        "\n",
        "Our *actor* will take in as input the `DoorKeyEnv5x5` observation (a 7x7x3 image), and output a [Categorical distribution](https://pytorch.org/docs/stable/distributions.html#categorical) over all possible actions.  To choose an action, we will sample from this distribution.  We suggest implementing the actor network to contain a few convolutional layers followed by a few fully-connected layers.\n",
        "\n",
        "In addition to the actor network, later questions in the PSET require estimating the value network, called the *critic*.  The critic estimates total future reward, much like DQN will in a future problem set, but is notably *on-policy*, meaning it's reward estimates are conditioned on the actor. We will use the critic to reduce variance in the policy gradient estimate. \n",
        "\n",
        "For now, fill in the **TODOS** to implement the actor-critic model below.  We suggest having separate actor and critic networks, as this has been shown to empirically improve performance.\n",
        "\n",
        "As this will have to be correct for your policy gradients algorithms to work, we will not grade this code independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fTpcS-cYbDdd"
      },
      "outputs": [],
      "source": [
        "# class ACModel(nn.Module):\n",
        "#     def __init__(self, num_actions, use_critic=False):\n",
        "#         super().__init__()\n",
        "#         self.use_critic = use_critic\n",
        "#         self.num_actions = num_actions\n",
        "\n",
        "#         ##### TODO: initialize actor and critic networks #######\n",
        "#         self.layer_dict_actor = nn.ModuleDict()  \n",
        "\n",
        "#         # Define actor's model\n",
        "#         num_filters = 32\n",
        "#         kernel_size = 3\n",
        "#         out = torch.zeros((1,3,7,7)) # create dummy inputs to be used to infer shapes of layers\n",
        "#         self.layer_dict_actor['convolution_1'] = nn.Conv2d(in_channels=out.shape[1], out_channels=num_filters,\n",
        "#                                                           kernel_size=kernel_size, stride=1)\n",
        "#         out = self.layer_dict_actor['convolution_1'].forward(out)\n",
        "#         #self.layer_dict_actor['batchnorm_1'] = nn.BatchNorm2d(num_features=out.shape[1])\n",
        "#         #out = F.leaky_relu(self.layer_dict_actor['batchnorm_1'].forward(out))\n",
        "#         self.layer_dict_actor['convolution_2'] = nn.Conv2d(in_channels=out.shape[1], out_channels=num_filters,\n",
        "#                                                           kernel_size=kernel_size, stride=1)\n",
        "#         out = self.layer_dict_actor['convolution_2'].forward(out)\n",
        "#         # self.layer_dict_actor['batchnorm_2'] = nn.BatchNorm2d(num_features=out.shape[1])\n",
        "#         # out = F.leaky_relu(self.layer_dict_actor['batchnorm_2'].forward(out))\n",
        "\n",
        "#         out = out.view(out.shape[0], -1)  # flatten outputs from (b, c, h, w) to (b, c*h*w)\n",
        "#         self.layer_dict_actor['fully_connected'] = nn.Linear(in_features=out.shape[1],  # initialize the prediction output linear layer\n",
        "#                                       out_features=self.num_actions)\n",
        "#         out = self.layer_dict_actor['fully_connected'].forward(out)\n",
        "\n",
        "#         # Define critic's model\n",
        "#         if self.use_critic:\n",
        "#           pass\n",
        "#         else:\n",
        "#           self.layer_dict_critic = nn.ModuleDict()\n",
        "\n",
        "#         ########################################################\n",
        "\n",
        "#         # Initialize parameters correctly (don't remove this!)\n",
        "#         self.apply(init_params)\n",
        "\n",
        "#     def forward(self, obs):\n",
        "#         conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into expected order\n",
        "#         dist, value = None, None\n",
        "#         ##### TODO: produce Categorical action distribtuion and critic value output #####\n",
        "#         ##### if self.use_critic is false, return all zeros for value ###################\n",
        "#         out = self.layer_dict_actor['convolution_1'].forward(conv_in)\n",
        "#         #out = F.leaky_relu(self.layer_dict_actor['batchnorm_1'].forward(out))\n",
        "#         out = F.relu(out)\n",
        "#         out = self.layer_dict_actor['convolution_2'].forward(out)\n",
        "#         out = F.relu(out)\n",
        "#         out = out.view(out.shape[0], -1)  # flatten outputs from (b, c, h, w) to (b, c*h*w)\n",
        "#         #out = F.leaky_relu(self.layer_dict_actor['batchnorm_2'].forward(out))\n",
        "#         out = self.layer_dict_actor['fully_connected'].forward(out)\n",
        "#         probs = F.softmax(out,dim=1)\n",
        "#         dist = torch.distributions.categorical.Categorical(probs=probs)\n",
        "#         if self.use_critic:\n",
        "#           pass\n",
        "#         else:\n",
        "#           value = 0\n",
        "#         ##################################################################################\n",
        "\n",
        "#         return dist, value\n",
        "\n",
        "class ACModel(nn.Module):\n",
        "    def __init__(self, num_actions, use_critic=False):\n",
        "        super().__init__()\n",
        "        self.use_critic = use_critic\n",
        "\n",
        "        # Define actor's model\n",
        "        self.image_conv_actor = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, (2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2)),\n",
        "            nn.Conv2d(16, 32, (2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, (2, 2)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, num_actions)\n",
        "        )\n",
        "\n",
        "        # Define critic's model\n",
        "        if self.use_critic:\n",
        "            self.image_conv_critic = nn.Sequential(\n",
        "                nn.Conv2d(3, 16, (2, 2)),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d((2, 2)),\n",
        "                nn.Conv2d(16, 32, (2, 2)),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(32, 64, (2, 2)),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            self.critic = nn.Sequential(\n",
        "                nn.Linear(64, 64),\n",
        "                nn.Tanh(),\n",
        "                nn.Linear(64, 1)\n",
        "            )\n",
        "\n",
        "        # Initialize parameters correctly\n",
        "        self.apply(init_params)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        conv_in = obs.transpose(1, 3).transpose(2, 3) # reshape into expected order\n",
        "\n",
        "        dist, value = None, None\n",
        "\n",
        "        x = self.image_conv_actor(conv_in)\n",
        "        embedding = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        x = self.actor(embedding)\n",
        "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
        "\n",
        "        if self.use_critic:\n",
        "            y = self.image_conv_critic(conv_in)\n",
        "            embedding = y.reshape(y.shape[0], -1)\n",
        "\n",
        "            value = self.critic(embedding).squeeze(1)\n",
        "        else:\n",
        "            value = torch.zeros((x.shape[0], 1), device=x.device)\n",
        "\n",
        "        return dist, value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9mwBFZ3RowL"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "The following code runs the actor critic model `acmodel` for one episode, and returns a dictionary with all the relevant information from the rollout.  It relies on placeholders below for `compute_advantage_gae` and `compute_discounted_return`: you can ignore these for now, and just evaluate through to the next section.  However, it might be useful to review this code just to make sure you understand what's going on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d-Wv0yZDR-cZ"
      },
      "outputs": [],
      "source": [
        "def compute_advantage_gae(values, rewards, T, gae_lambda, discount):\n",
        "    advantages = torch.zeros_like(values)\n",
        "    #### TODO: populate GAE in advantages over T timesteps (10 pts) ############\n",
        "    # print(\"advantages shape\", advantages.shape)\n",
        "    # print(\"gae_lambda: \", gae_lambda)\n",
        "    # print(\"T: \", T)\n",
        "    # N = len(rewards)\n",
        "    # for t in range(N):\n",
        "    #   total = 0 \n",
        "    #   for k in range(1,T+1):\n",
        "    #     R = rewards[t] + sum([rewards[i]*discount**(i-t) for i in range(t+1,t+k)])\n",
        "    #     R += values[t+k]*discount**(k)\n",
        "    #     total += gae_lambda*R\n",
        "\n",
        "    #   advantages[t] = R - values[t]\n",
        "\n",
        "    # deltas = torch.zeros_like(values)\n",
        "    # for t in range(len(rewards)-1):\n",
        "    #   deltas[t] = rewards[t] + discount*values[t+1] - values[t]\n",
        "    # deltas[-1] = rewards[-1]\n",
        "    # for t in range(T):\n",
        "    #   for l in range(t,len(deltas)):\n",
        "    #     advantages[t] += ((discount*gae_lambda)**(l-t))*deltas[l]\n",
        "\n",
        "    \n",
        "    ############################################################################\n",
        "    \n",
        "    return advantages[:T]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "67XzIFyRSARg"
      },
      "outputs": [],
      "source": [
        "def compute_discounted_return(rewards, discount, device=None):\n",
        "    returns = torch.zeros(*rewards.shape, device=device)   \n",
        "    returns[-1] = rewards[-1]\n",
        "\n",
        "    for i in range(len(returns)-2,-1,-1):\n",
        "      returns[i] = rewards[i] + discount*returns[i+1]\n",
        "    ######################################################################\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = torch.tensor([1, 2, 3, 4, 5])\n",
        "discount = 0.95\n",
        "returns = torch.zeros(*rewards.shape)   \n",
        "returns[-1] = rewards[-1]\n",
        "\n",
        "for i in range(len(returns)-2,-1,-1):\n",
        "  returns[i] = rewards[i] + discount*returns[i+1]"
      ],
      "metadata": {
        "id": "I9LQo6qLdfwK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ekC6vsRzbDdd"
      },
      "outputs": [],
      "source": [
        "def collect_experiences(env, acmodel, args, device=None):\n",
        "    \"\"\"Collects rollouts and computes advantages.\n",
        "    Returns\n",
        "    -------\n",
        "    exps : dict\n",
        "        Contains actions, rewards, advantages etc as attributes.\n",
        "        Each attribute, e.g. `exps['reward']` has a shape\n",
        "        (self.num_frames, ...).\n",
        "    logs : dict\n",
        "        Useful stats about the training process, including the average\n",
        "        reward, policy loss, value loss, etc.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    MAX_FRAMES_PER_EP = 300\n",
        "    shape = (MAX_FRAMES_PER_EP, )\n",
        "\n",
        "    actions = torch.zeros(*shape, device=device, dtype=torch.int)\n",
        "    values = torch.zeros(*shape, device=device)\n",
        "    rewards = torch.zeros(*shape, device=device)\n",
        "    log_probs = torch.zeros(*shape, device=device)\n",
        "    obss = [None]*MAX_FRAMES_PER_EP\n",
        "\n",
        "    obs = env.reset()\n",
        "\n",
        "    total_return = 0\n",
        "\n",
        "    T = 0\n",
        "\n",
        "    while True:\n",
        "        # Do one agent-environment interaction\n",
        "\n",
        "        preprocessed_obs = preprocess_obss(obs, device=device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            dist, value = acmodel(preprocessed_obs)\n",
        "        action = dist.sample()[0]\n",
        "\n",
        "\n",
        "        obss[T] = obs\n",
        "        obs, reward, done, _ = env.step(action.item())\n",
        "\n",
        "\n",
        "        # Update experiences values\n",
        "        #print()\n",
        "        actions[T] = action\n",
        "        values[T] = value\n",
        "        rewards[T] = reward\n",
        "        log_probs[T] = dist.log_prob(action)\n",
        "\n",
        "\n",
        "        total_return += reward\n",
        "        T += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    discounted_reward = compute_discounted_return(rewards[:T], args.discount, device)\n",
        "    exps = dict(\n",
        "        obs = preprocess_obss([\n",
        "            obss[i]\n",
        "            for i in range(T)\n",
        "        ], device=device),\n",
        "        action = actions[:T],\n",
        "        value  = values[:T],\n",
        "        reward = rewards[:T],\n",
        "        advantage = discounted_reward-values[:T],\n",
        "        log_prob = log_probs[:T],\n",
        "        discounted_reward = discounted_reward,\n",
        "        advantage_gae=compute_advantage_gae(values, rewards, T, args.gae_lambda, args.discount)\n",
        "    )\n",
        "\n",
        "    logs = {\n",
        "        \"return_per_episode\": total_return,\n",
        "        \"num_frames\": T\n",
        "    }\n",
        "\n",
        "    return exps, logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfKUViEGbDde"
      },
      "source": [
        "# REINFORCE\n",
        "\n",
        "Now comes the fun part!  Using the `collect_experiences` function and `ACModel`, we will implement vanilla policy gradients.  The following function takes in an `optimizer`, `ACModel`, batch of experience `sb`, and some arguments `args` (see `Config` in setup for fields and default values), and should perform a policy gradients parameter update using the observed experience.\n",
        "\n",
        "Fill in todos below to implement vanilla policy gradients (20 pts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bzrKmjywSE0H"
      },
      "outputs": [],
      "source": [
        "def update_parameters_reinforce(optimizer, acmodel, sb, args):\n",
        "    def _compute_policy_loss(logps, returns):\n",
        "        policy_loss = torch.tensor(0)\n",
        "\n",
        "        #### TODO: complete policy loss (10 pts) ###\n",
        "        # print(\"logps : \", logps)\n",
        "        # print(\"returns: \", returns)\n",
        "        policy_loss = (logps*returns).mean()*-1 #-1 for gradient ascent?\n",
        "        ############################################\n",
        "\n",
        "        return policy_loss\n",
        "\n",
        "\n",
        "    logps, reward = None, None\n",
        "    \n",
        "    ### TODO: compute logps and reward from acmodel, sb['obs'], sb['action'], and sb['reward'] ###\n",
        "    ### If args.use_discounted_reward is True, use sb['discounted_reward'] instead. ##############\n",
        "    ### (10 pts) #########################################\n",
        "    dist, value = acmodel.forward(sb['obs'])\n",
        "    actions = sb['action']\n",
        "    logps = dist.log_prob(actions)\n",
        "    if args.use_discounted_reward:\n",
        "      reward = sb['discounted_reward']\n",
        "    else:\n",
        "      reward = sb['reward']\n",
        "      reward = torch.flip(torch.cumsum(torch.flip(reward,dims=[0]),dim=0),dims=[0])\n",
        "\n",
        "    ##############################################################################################\n",
        "\n",
        "    policy_loss = _compute_policy_loss(logps, reward)\n",
        "    update_policy_loss = policy_loss.item()\n",
        "\n",
        "    # Update actor-critic\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    \n",
        "    # Perform gradient clipping for stability\n",
        "    for p in acmodel.parameters():\n",
        "        if p.grad is None:\n",
        "            print(\"Make sure you're not instantiating any critic variables when the critic is not used\")\n",
        "    update_grad_norm = sum(p.grad.data.norm(2) ** 2 for p in acmodel.parameters()) ** 0.5\n",
        "    torch.nn.utils.clip_grad_norm_(acmodel.parameters(), args.max_grad_norm)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log some values\n",
        "    logs = {\n",
        "        \"policy_loss\": update_policy_loss,\n",
        "        \"grad_norm\": update_grad_norm\n",
        "    }\n",
        "\n",
        "    return logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC--D8zMVBtW"
      },
      "source": [
        "Now, let's try to run our implementation.  The following experiment harness is written for you, and will run sequential episodes of policy gradients until `args.max_episodes` timesteps are exceeded or the rolling average reward (over the last 100 episodes is greater than `args.score_threshold`. It is expected to get highly variable results, and we'll visualize some of this variability at the end.\n",
        "\n",
        "The method accepts as arguments a `Config` object `args`, and a `parameter_update` method (such as `update_parameters_reinforce`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HiPziB2AbDdf"
      },
      "outputs": [],
      "source": [
        "def run_experiment(args, parameter_update):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    env = DoorKeyEnv5x5()\n",
        "\n",
        "    acmodel = ACModel(env.action_space.n, use_critic=args.use_critic)\n",
        "    acmodel.to(device)\n",
        "\n",
        "    is_solved = False\n",
        "    \n",
        "    SMOOTH_REWARD_WINDOW = 50\n",
        "\n",
        "    pd_logs, rewards = [], [0]*SMOOTH_REWARD_WINDOW\n",
        "    \n",
        "    optimizer = torch.optim.Adam(acmodel.parameters(), lr=args.lr)\n",
        "    num_frames = 0\n",
        "\n",
        "    pbar = tqdm(range(args.max_episodes))\n",
        "    for update in pbar:\n",
        "        exps, logs1 = collect_experiences(env, acmodel, args, device)\n",
        "        logs2 = parameter_update(optimizer, acmodel, exps, args)\n",
        "\n",
        "        logs = {**logs1, **logs2}\n",
        "\n",
        "        num_frames += logs[\"num_frames\"]\n",
        "        \n",
        "        rewards.append(logs[\"return_per_episode\"])\n",
        "        \n",
        "        smooth_reward = np.mean(rewards[-SMOOTH_REWARD_WINDOW:])\n",
        "\n",
        "        data = {'episode':update, 'num_frames':num_frames, 'smooth_reward':smooth_reward,\n",
        "                'reward':logs[\"return_per_episode\"], 'policy_loss':logs[\"policy_loss\"]}\n",
        "        \n",
        "        if args.use_critic:\n",
        "            data['value_loss'] = logs[\"value_loss\"]\n",
        "\n",
        "        pd_logs.append(data)\n",
        "\n",
        "        pbar.set_postfix(data)\n",
        "\n",
        "        # Early terminate\n",
        "        if smooth_reward >= args.score_threshold:\n",
        "            is_solved = True\n",
        "            break\n",
        "\n",
        "    if is_solved:\n",
        "        print('Solved!')\n",
        "    \n",
        "    return pd.DataFrame(pd_logs).set_index('episode')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt63uqbfVmxl"
      },
      "source": [
        "## Run Reinforce\n",
        "\n",
        "Great!  Now let's run our implementation, and see how we do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "mPxIilNE4NXf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "53d9de2b07684a9fb7f49de014f7ae6c",
            "4e0792ef31a741afb81de3ba7ec5c1f5",
            "34f2fb36defd4ab6896b2948b5834609",
            "fef9dce8ea274550bbb4d743db3a50ed",
            "a905757959d84e019b51b509c9ac3ea4",
            "0a4205e0e89544a7be6264343b2345a3",
            "1e05d4eb4d1c4ae486054c594cedfea8",
            "2362cc0b657a451699889026db669409",
            "c11ad007e85f42b581d628372264c74b",
            "5ceba644f71e435d90c8fcd842ae0cd7",
            "910f8a0c0325416d89126ff1ae5ec40c"
          ]
        },
        "outputId": "514c2ef8-725b-4d53-e92b-40dfc5a04166"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53d9de2b07684a9fb7f49de014f7ae6c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efc809f7350>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "args = Config()\n",
        "#args.max_episodes = args.max_episodes // 2 #delete later\n",
        "#args.lr = 1e-4\n",
        "df = run_experiment(args, update_parameters_reinforce)\n",
        "df.plot(x='num_frames', y=['reward','smooth_reward'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-cVKxayYY1B"
      },
      "source": [
        "## REINFORCE with Discounted Reward\n",
        "\n",
        "Uh oh! Even after 300,000 steps, our policy does not converge. One reason for failure is the way rewards are generated in the real-world. In an ideal world, the agent would be rewarded at every timestep in a manner that perfectly corresponded to the quality of the action taken in a particular state.  However, this is rarely the case; for example, in Doorkey we only get reward at the very end of the episode (i.e., the sparse reward scenario).\n",
        "\n",
        "In DQN, we tackle this with a discount factor `gamma` on future rewards.  In policy gradients, we'll simply rewrite all of our step rewards to be discounted from the past episode reward.\n",
        "\n",
        "Fill in `compute_discounted_return` code block above, then run code cell below to see the effect of discounted reward trajectories.  This should converge, so if it doesn't, you've made an error (although try re-running the cell a few times first to make sure it's not a bad random seed). (10 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4fDIhCpeZxdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "71215a59ab634960a22089ade1953422",
            "1dd40e6247174deda469b4cb72071565",
            "37ad94a42b074c019d583f2ad34a92d4",
            "df1bac7401a247288e75f44b54ff51a4",
            "f8bc776d9bf44ac09906e53e5aa5422e",
            "19fb945f660241f59f1672269fe3f84b",
            "a04b4b3d02244f6fb3a873fb1301b6f2",
            "cfb672bac2734568a8384b3f679d2a5c",
            "1f58194fc861433385802dfc0a25c6ff",
            "cb32e2e03f084abc97b58c2dc78ef372",
            "9e3acfda096b4d65a819ee98a39757cd"
          ]
        },
        "outputId": "90f926ce-a248-45d7-d4b8-e3309db41215"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71215a59ab634960a22089ade1953422",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solved!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0fb3466350>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "args = Config(use_discounted_reward=True)\n",
        "#args.max_episodes = args.max_episodes // 2 #delete later\n",
        "#args.lr = 1e-4\n",
        "df = run_experiment(args, update_parameters_reinforce)\n",
        "df.plot(x='num_frames', y=['reward', 'smooth_reward'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lWHQpkubqpM"
      },
      "source": [
        "# Vanilla Policy Gradients\n",
        "\n",
        "You may have noticed that the REINFORCE training curve is extremely unstable. It's time to bring in our *critic*!  We can prove from the Expected Grad-Log-Prob (EGLP) lemma that we can subtract any function $b(x)$ from our reward without changing our policy in expectation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8PdtlQTbDdg"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OSRQMGGqfUj"
      },
      "source": [
        "### Baseline Proof\n",
        "\n",
        "**Question**: Prove that adding a baseline doesn't change the policy in expectation using the EGLP lemma.  This can be a loose proof as long as you convey the intutition. (10 pts)\n",
        "\n",
        "**Proof**: "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the linearity of expectation we have\n",
        "$$\n",
        "\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}\\left[\\sum_{t=0}^T\\nabla_\\theta\\log \\pi_θ(a_t|s_t)\\left(\\sum_{t'=t}^TR(s_{t'},a_{t'},s_{t'+1})-b(s_t)\\right)\\right] \\\\\n",
        "= \\mathbb{E}\\left[\\sum_{t=0}^T\\nabla_\\theta\\log \\pi_θ(a_t|s_t)\\left(\\sum_{t'=t}^TR(s_{t'},a_{t'},s_{t'+1})\\right)\\right] - \\mathbb{E}\\left[\\sum_{t=0}^T\\nabla_\\theta\\log \\pi_θ(a_t|s_t)b(s_t)\\right]\n",
        "= \\mathbb{E}\\left[\\sum_{t=0}^T\\nabla_\\theta\\log \\pi_θ(a_t|s_t)\\left(\\sum_{t'=t}^TR(s_{t'},a_{t'},s_{t'+1})\\right)\\right] - \\sum_{t=0}^T\\mathbb{E}\\left[\\nabla_\\theta\\log \\pi_θ(a_t|s_t)b(s_t)\\right]\n",
        "$$\n",
        "Therefore, to show that the expected value of the policy remains unchanged it suffices to show that \n",
        "$$\n",
        "\\mathbb{E}\\left[\\nabla_\\theta\\log \\pi_θ(a_t|s_t)b(s_t)\\right] = 0\n",
        "$$\n",
        "Using the definition of expectation we get that\n",
        "$$\n",
        "\\mathbb{E}\\left[\\nabla_\\theta\\log \\pi_θ(a_t|s_t)b(s_t)\\right] = \\int\\nabla_\\theta\\log \\pi_θ(a_t|s_t)b(s_t)\\pi_θ(a_t|s_t)dθ \\\\\n",
        "= b(s_t)\\int\\nabla_\\theta\\log \\pi_θ(a_t|s_t)\\pi_θ(a_t|s_t)dθ \\quad\\text{(b independent of $\\theta$)} \\\\\n",
        "= b(s_t)\\int ∇_θ\\pi_θ(a_t|s_t)dθ \\quad\\text{(derivative of log)} \\\\\n",
        "=  b(s_t)∇_θ1 \\quad\\text{(distributions integrate to 1)} \\\\\n",
        "= 0 \n",
        "$$"
      ],
      "metadata": {
        "id": "CMy4JmvlfUsj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNXJ4XqNbs0n"
      },
      "source": [
        "Empirically, using the on-policy value function as the baseline ($b$) reduces variance in the policy gradient sample estimate, leading to faster and more stable learning.  We can estimate the $b$ using an L2 loss to the true rewards (or in our case, the discounted rewards), and constitutes an additional loss term in the overall objective. The baseline substracted return term, $R(s_{t'},a_{t'},s_{t' + 1}) - b(s_t)$ is already computed for you, and is referred to as the *advantage*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "mM4Rqfe9bDdi"
      },
      "outputs": [],
      "source": [
        "def update_parameters_with_baseline(optimizer, acmodel, sb, args):\n",
        "    def _compute_policy_loss_with_baseline(logps, advantages):\n",
        "        ### TODO: implement the policy loss (5 pts) ##############\n",
        "        returns = torch.flip(torch.cumsum(torch.flip(advantages,dims=[0]),dim=0),dims=[0])\n",
        "        policy_loss = (logps*returns).mean()*-1\n",
        "        #policy_loss = (logps*advantages).sum()*-1\n",
        "        ##################################################\n",
        "        return policy_loss\n",
        "    \n",
        "    def _compute_value_loss(values, returns):\n",
        "        ### TODO: implement the value loss (5 pts) ###############\n",
        "        #returns = torch.flip(torch.cumsum(torch.flip(returns,dims=[0]),dim=0),dims=[0])\n",
        "        value_loss = ((values-returns)**2).mean()\n",
        "        ##################################################\n",
        "\n",
        "        return value_loss\n",
        "\n",
        "    logps, advantage, values, reward = None, None, None, None\n",
        "    ### TODO: populate the policy and value loss computation fields using acmodel, sb['obs'], sb['action], and sb['discounted_reward']\n",
        "    ### For the advantage term, use sb['advantage_gae'] if args.use_gae is True, and sb['advantage'] otherwise.\n",
        "    ### 10 pts\n",
        "    if args.use_gae:\n",
        "      advantage = sb['advantage_gae'] \n",
        "    else:\n",
        "      advantage = sb['advantage']\n",
        "\n",
        "    dist, values = acmodel.forward(sb['obs'])\n",
        "    actions = sb['action']\n",
        "    logps = dist.logits\n",
        "    logps = logps.gather(1, actions.long().unsqueeze(1)).reshape(-1)\n",
        "    reward = sb['discounted_reward']     \n",
        "    ####################################################################################################\n",
        "\n",
        "    policy_loss = _compute_policy_loss_with_baseline(logps, advantage)\n",
        "    value_loss = _compute_value_loss(values, reward)\n",
        "    loss = policy_loss + value_loss\n",
        "\n",
        "    update_policy_loss = policy_loss.item()\n",
        "    update_value_loss = value_loss.item()\n",
        "\n",
        "    # Update actor-critic\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    update_grad_norm = sum(p.grad.data.norm(2) ** 2 for p in acmodel.parameters()) ** 0.5\n",
        "    torch.nn.utils.clip_grad_norm_(acmodel.parameters(), args.max_grad_norm)\n",
        "    optimizer.step()\n",
        "\n",
        "    # Log some values\n",
        "\n",
        "    logs = {\n",
        "        \"policy_loss\": update_policy_loss,\n",
        "        \"value_loss\": update_value_loss,\n",
        "        \"grad_norm\": update_grad_norm\n",
        "    }\n",
        "\n",
        "    return logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcftj9PpbDdi"
      },
      "source": [
        "## Run REINFORCE with baseline\n",
        "\n",
        "If you did everything right, you should be able to run the below cell to run the vanilla policy gradients implementation with baseline.  This should be somewhat more stable than without the baseline, and likely converge faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ygISg6VJbDdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "3647c8100c2f4ea7b5d9cb599bf00d1e",
            "e83f686965ba4d87a591bc22ec90afe2",
            "5177ee8914134196ad39198715886568",
            "55a901b501884954b731de10ec52db4d",
            "8032f8b507bd48c086b4361a4f53d420",
            "d36734eebb584cc2a07f142ad54d4865",
            "261632401b9d436ca7077670e96ea65f",
            "e7f9cc0557cc49f4a42120358547fd5d",
            "89faa19b06794088bc80d005b79d5577",
            "ffd958083bc94c4ea56c50f3922b272f",
            "bb16a6db0e954bbca7c4b317845a0230"
          ]
        },
        "outputId": "9b6f940c-37f4-474a-c9fc-e409822a74f7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3647c8100c2f4ea7b5d9cb599bf00d1e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0fb319ff50>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "args = Config(use_critic=True)\n",
        "df_baseline = run_experiment(args, update_parameters_with_baseline)\n",
        "df_baseline.plot(x='num_frames', y=['reward', 'smooth_reward'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfHluYAfTKC"
      },
      "source": [
        "# Reinforce with GAE\n",
        "\n",
        "The advantage we computed above seemed to work, and hopefully improved our results!  Fortunately, we can do even better.  The paper [Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438) describes a nifty method for building a strong advantage estimate (see formula 16 in the paper) that empirically outperforms a naive subtraction (and includes reward shaping).\n",
        "\n",
        "Fill in the `compute_advantage_gae` method above according to the formula in the paper (10 pts), and then run the below cell.  GAE should further improve convergence time and stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "vcfHPVV5JmGb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385,
          "referenced_widgets": [
            "87a9279cb32a48cc96bf8c77fb0f3b19",
            "6f03ab8571a044ba95d0f2ac5876fbd5",
            "07c98fd2dd9e429ca522817e7c4ac751",
            "d284ae2eb35d409384d211256f7de450",
            "6b89df6459df463b91f528ce905d413e",
            "098092a767a8452a945ec81c3c3f98c6",
            "6d92cb1ecec147bcace9f90eb32e92d5",
            "2f12c806df474375ada421b2376140fb",
            "6f53b4125f9c4f6ab63f7d71cbe7b92c",
            "881b2a6517a54d2fa5a0bc01464dc6c9",
            "0ab6ac119bb042628cf5c831baa37141"
          ]
        },
        "outputId": "0ff8780d-1fd1-4d89-d3c8-aa6c153b6e9c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87a9279cb32a48cc96bf8c77fb0f3b19",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-936fb59043a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_critic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_gae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_parameters_with_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_gae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'num_frames'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'smooth_reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-6d492cef3196>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(args, parameter_update)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mexps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlogs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-de4f421233b5>\u001b[0m in \u001b[0;36mcollect_experiences\u001b[0;34m(env, acmodel, args, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mdiscounted_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscounted_reward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0madvantage_gae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_advantage_gae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgae_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-5d2b54583013>\u001b[0m in \u001b[0;36mcompute_advantage_gae\u001b[0;34m(values, rewards, T, gae_lambda, discount)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0madvantages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgae_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "args = Config(use_critic=True, use_gae=True)\n",
        "df_gae = run_experiment(args, update_parameters_with_baseline)\n",
        "df_gae.plot(x='num_frames', y=['reward', 'smooth_reward'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BoRwiFabDdi"
      },
      "source": [
        "# Proximal Policy Optimization\n",
        "\n",
        "Our work is not yet done!  There are some surprisingly powerful additional tweaks we can make to our GAE implementation to further improve performance.\n",
        "\n",
        "The current standard in policy gradients today is [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347), which improves on GAE by taking multiply policy update steps per minibatch, enabled by policy update clipping (this is a specific variant called *PPO-Clip*).  This leads to greater sample efficiency, as larger steps can be taken from the same data samples.\n",
        "\n",
        "We've implemented most of PPO for you: all that's left for you are the policy and value loss computations (note that you'll have to evaluate the `acmodel` each time you compute them).  Note that for the policy loss, we also ask that you return the approximate KL divergence between the new and old action distributions notated as `approx_kl`; this is used to facilitate an early stopping condition in policy updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFLxFbVwkBrk"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Eb84WAVpbDdi"
      },
      "outputs": [],
      "source": [
        "def update_parameters_ppo(optimizer, acmodel, sb, args):\n",
        "    def _compute_policy_loss_ppo(obs, old_logp, actions, advantages):\n",
        "        policy_loss, approx_kl = 0, 0\n",
        "\n",
        "        ### TODO: implement PPO policy loss computation (30 pts).  #######\n",
        "        print(\"old_logp: \", old_logp)\n",
        "        epsilon = 0.2\n",
        "        dist, _ = acmodel.forward(obs)\n",
        "        new_logp = dist.log_prob(actions)\n",
        "        print(\"new_logp: \", new_logp)\n",
        "        print(\"advantages: \", advantages)\n",
        "        losses = torch.minimum( advantages * new_logp /old_logp, torch.clip(advantages,min = (1-epsilon)*advantages, max = (1+epsilon)*advantages))\n",
        "        print(\"losses: \", losses)\n",
        "        policy_loss = losses.sum()\n",
        "\n",
        "        r = new_logp / old_logp \n",
        "        approx_kl = ((r-1) - torch.log(r)).mean()\n",
        "        ##################################################################\n",
        "        \n",
        "        return policy_loss, approx_kl\n",
        "    \n",
        "    def _compute_value_loss(obs, returns):\n",
        "        ### TODO: implement PPO value loss computation (10 pts) ##########\n",
        "        _, values = acmodel.forward(obs)\n",
        "        value_loss = ((values-returns)**2).sum()\n",
        "        ##################################################################\n",
        "\n",
        "        return value_loss\n",
        "\n",
        "\n",
        "    dist, _ = acmodel(sb['obs'])            \n",
        "    old_logp = dist.log_prob(sb['action']).detach()\n",
        "    \n",
        "    advantage = sb['advantage_gae'] if args.use_gae else sb['advantage']\n",
        "\n",
        "    policy_loss, _ = _compute_policy_loss_ppo(sb['obs'], old_logp, sb['action'], advantage)\n",
        "    value_loss = _compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
        "\n",
        "    for i in range(args.train_ac_iters):\n",
        "        optimizer.zero_grad()\n",
        "        loss_pi, approx_kl = _compute_policy_loss_ppo(sb['obs'], old_logp, sb['action'], advantage)\n",
        "        loss_v = _compute_value_loss(sb['obs'], sb['discounted_reward'])\n",
        "\n",
        "        loss = loss_v + loss_pi\n",
        "        if approx_kl > 1.5 * args.target_kl:\n",
        "            break\n",
        "        \n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "    \n",
        "    update_policy_loss = policy_loss.item()\n",
        "    update_value_loss = value_loss.item()\n",
        "\n",
        "    logs = {\n",
        "        \"policy_loss\": update_policy_loss,\n",
        "        \"value_loss\": update_value_loss,\n",
        "    }\n",
        "\n",
        "    return logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ElBRzBVdbDdj",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aca5775bc6fd49f097907307d56ec056",
            "33e1e7ad5a384f75a66f3fe7d863dbf8",
            "88ab82cdd85e41299258f8c9e4423a80",
            "68674dc46bce4ef4b4fcf2e233ea7d9a",
            "7a7ee546174a472e917eee802e4cadf1",
            "d651677b953e43aca2ec4b30a9417bbc",
            "d2a45bbc0d0446059a0b60ab86e8b4af",
            "eda2928f4807487c8a0ef7d692e9ce8b",
            "ef3e68f2f91d445d9f63a23c86e97f45",
            "e14fe6afafe94591b3852375999d698f",
            "b38109ba9c2d4b3c9918c030445a3bce"
          ]
        },
        "outputId": "d40ba04e-b1ab-4eae-c612-fc564f219281"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aca5775bc6fd49f097907307d56ec056",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        -1.8081, -1.6503, -1.8672, -1.9955, -2.1013, -2.0721, -2.0474, -2.1416,\n",
            "        -1.9905, -1.8244])\n",
            "new_logp:  tensor([-1.9905, -1.9905, -2.0731, -1.8244, -1.8244, -2.1416, -2.0144, -2.0512,\n",
            "        -1.9119, -1.9964, -2.0822, -1.9955, -2.0474, -1.8784, -1.8792, -1.9550,\n",
            "        -2.1316, -2.1316, -1.8803, -2.0907, -2.0935, -1.8672, -1.8225, -2.0822,\n",
            "        -1.8225, -2.0822, -1.7461, -1.7997, -1.7418, -1.9905, -1.8244, -1.9905,\n",
            "        -1.8784, -1.9550, -2.0907, -1.7812, -1.6779, -1.8792, -1.8081, -2.0387,\n",
            "        -1.7722, -1.9132, -2.1007, -1.8483, -1.8483, -1.8081, -1.6503, -1.9778,\n",
            "        -1.6503, -2.0935, -1.8672, -1.7461, -1.9964, -1.7461, -1.9964, -1.9230,\n",
            "        -1.8225, -2.0822, -1.9955, -2.0474, -2.0144, -2.0961, -2.0961, -2.0512,\n",
            "        -1.9964, -2.0454, -2.0570, -2.0454, -1.9230, -2.0570, -2.0454, -1.7461,\n",
            "        -2.0512, -2.0512, -1.7418, -2.0144, -1.7997, -2.0961, -1.9964, -1.9230,\n",
            "        -2.0822, -1.8225, -1.7461, -2.0512, -1.9119, -1.7997, -1.7418, -1.8784,\n",
            "        -1.9550, -1.8803, -2.0567, -2.0567, -2.1316, -1.6779, -1.8081, -1.9132,\n",
            "        -1.7722, -2.1007, -2.0869, -1.9704, -1.8792, -1.8792, -1.8483, -1.8483,\n",
            "        -1.9550, -2.0907, -1.7812, -2.1316, -1.6779, -1.8792, -2.0869, -1.8081,\n",
            "        -1.7722, -2.1210, -1.6503, -1.8672, -1.8225, -2.0454, -2.0822, -1.9955,\n",
            "        -2.0721, -1.8363, -1.8363, -2.0622, -1.6550, -1.7461, -1.9964, -1.9230,\n",
            "        -1.9955, -2.1013, -1.6550, -2.0454, -1.9230, -1.7461, -1.7997, -1.9964,\n",
            "        -2.0454, -2.0570, -1.8225, -1.9955, -1.8363, -1.8363, -2.0622, -1.6550,\n",
            "        -2.0570, -2.0822, -2.0454, -2.0454, -2.0454, -1.9955, -2.1013, -2.0622,\n",
            "        -1.9308, -2.0622, -2.0721, -1.8363, -1.8363, -2.0721, -2.0474, -1.8784,\n",
            "        -1.8792, -1.8792, -1.8081, -1.9132, -1.7722, -2.1210, -2.0387, -2.0387,\n",
            "        -2.1374, -1.7722, -2.1374, -1.7722, -2.1007, -1.8081, -2.1210, -1.6503,\n",
            "        -1.8672, -2.0822, -1.9230, -2.0454, -2.0454, -2.0454, -1.9230, -2.0822,\n",
            "        -1.8225, -1.7461, -1.9119, -1.9119, -1.7997, -1.7997, -1.9119, -2.0961,\n",
            "        -2.0961, -1.7997, -1.7997, -2.0512, -1.7997, -1.9964, -1.9230, -2.0454,\n",
            "        -2.0570, -1.9230, -1.7461, -1.9119, -1.9119, -1.7997, -2.0851, -2.0961,\n",
            "        -1.7997, -1.7418, -1.7570, -1.8363, -1.6550, -2.0570, -2.0454, -1.9230,\n",
            "        -1.8225, -2.0570, -1.8225, -1.9230, -2.0570, -2.0822, -1.7461, -2.0851,\n",
            "        -1.7997, -2.0851, -1.7418, -2.0731, -1.8784, -1.9550, -1.6779, -1.8792,\n",
            "        -2.1136, -1.9529, -2.0382, -2.1284, -1.7773, -1.6737, -1.8795, -1.8792,\n",
            "        -1.8081, -1.6503, -1.8672, -1.9955, -2.1013, -2.0721, -2.0474, -2.1416,\n",
            "        -1.9905, -1.8244], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868])\n",
            "new_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868])\n",
            "new_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868])\n",
            "new_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868])\n",
            "new_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868])\n",
            "new_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868])\n",
            "new_logp:  tensor([-2.0684, -1.6628, -1.8946, -2.0868, -1.8358, -2.0588, -1.7479, -1.7418,\n",
            "        -2.1518, -2.0837, -1.8029, -1.8525, -2.0929, -1.7784, -2.1131, -1.6628,\n",
            "        -1.9381, -1.6628, -2.0003, -1.9381, -2.1226, -1.9022, -1.6628, -1.8946,\n",
            "        -1.7479, -2.0961, -2.0851, -2.0851, -1.7418, -1.8525, -1.8818, -2.0929,\n",
            "        -2.1059, -2.0468, -1.8846, -2.1059, -2.1195, -1.8846, -1.9898, -2.1059,\n",
            "        -1.8196, -1.9717, -2.0640, -1.8575, -2.0196, -1.7741, -2.1249, -1.9676,\n",
            "        -2.0354, -2.0354, -1.6505, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969,\n",
            "        -2.0829, -1.9717, -2.0499, -2.0427, -2.1662, -2.0722, -1.7560, -2.0913,\n",
            "        -2.0638, -2.0007, -1.9163, -1.8398, -1.7386, -2.0638, -2.0638, -1.7312,\n",
            "        -2.0136, -1.7597, -1.8196, -2.0829, -2.0560, -2.0560, -1.9163, -1.7386,\n",
            "        -2.0628, -1.7070, -1.7741, -2.1249, -1.8868, -1.8643, -1.9713, -2.0344,\n",
            "        -1.8110, -2.1178, -2.1151, -1.9146, -2.0718, -1.9809, -2.1102, -2.1057,\n",
            "        -1.9676, -1.9195, -1.8186, -1.8186, -1.8186, -2.0949, -2.2411, -1.5954,\n",
            "        -1.7448, -1.9163, -1.9163, -2.0677, -2.0677, -2.0560, -2.0829, -1.7386,\n",
            "        -1.7312, -1.7054, -1.7580, -1.7028, -2.0225, -1.7028, -1.7028, -2.0225,\n",
            "        -1.7580, -1.7673, -2.1202, -2.2026, -1.7258, -2.0565, -2.2651, -1.7665,\n",
            "        -1.7186, -1.8398, -1.8398, -2.0560, -2.0829, -1.9717, -2.1203, -1.7331,\n",
            "        -2.0368, -1.7767, -1.7497, -2.2411, -1.8026, -2.1538, -1.5953, -1.8345,\n",
            "        -2.2278, -2.1395, -1.8345, -1.8345, -2.0217, -1.8569, -1.8455, -2.1699,\n",
            "        -1.5953, -2.1257, -2.0217, -1.8569, -2.1067, -1.9501, -1.8078, -2.1029,\n",
            "        -2.0215, -2.1035, -2.2294, -2.1035, -1.9657, -2.2007, -1.6459, -2.2294,\n",
            "        -1.8159, -2.1035, -2.0966, -1.8159, -1.8908, -2.0546, -2.0546, -1.8473,\n",
            "        -1.6792, -2.0963, -1.7656, -1.6160, -2.0364, -2.1235, -2.1525, -1.7500,\n",
            "        -1.9316, -1.9828, -2.0531, -2.1179, -2.1179, -2.1179, -1.6851, -1.7479,\n",
            "        -1.7527, -2.1252, -2.0744, -1.7527, -1.7527, -1.8245, -2.1316, -2.1367,\n",
            "        -2.1367, -2.1316, -2.1968, -1.6185, -1.8473, -1.6792, -2.0963, -2.0963,\n",
            "        -1.7656, -2.2519, -2.1571, -2.0872, -2.0946, -2.0318, -2.0946, -1.7669,\n",
            "        -2.0872, -1.7669, -2.0504, -1.6546, -1.8882, -1.9828, -2.0537, -1.9116,\n",
            "        -2.0537, -2.1179, -1.8655, -2.0537, -2.0347, -1.8807, -1.7669, -2.0504,\n",
            "        -2.0767, -2.2519, -1.6160, -1.6185, -1.6792, -1.9742, -1.8473, -1.6792,\n",
            "        -1.9742, -1.6792, -1.7500, -1.9742, -1.8874, -2.0690, -2.1968, -1.8667,\n",
            "        -1.7789, -2.0868], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211])\n",
            "new_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211])\n",
            "new_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211])\n",
            "new_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211])\n",
            "new_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211])\n",
            "new_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211])\n",
            "new_logp:  tensor([-2.0570, -1.7461, -1.8212, -2.0637, -1.7485, -2.0716, -1.8212, -2.0907,\n",
            "        -1.8955, -2.0637, -1.8212, -1.9866, -1.9230, -2.0454, -1.7461, -2.0637,\n",
            "        -2.0637, -1.9866, -1.8225, -1.7461, -2.0907, -2.0637, -2.0907, -1.9866,\n",
            "        -1.9230, -1.7461, -1.7485, -1.7697, -1.8210, -1.7771, -1.9337, -1.9337,\n",
            "        -1.7748, -1.6601, -1.8225, -1.9955, -1.6601, -1.8225, -1.9230, -1.9955,\n",
            "        -1.7748, -2.1202, -2.1042, -2.1042, -1.7748, -1.7748, -1.7748, -2.0687,\n",
            "        -1.6601, -1.9955, -1.6601, -2.0454, -1.9955, -1.9337, -1.7748, -2.1042,\n",
            "        -2.1042, -1.9337, -1.6601, -2.0822, -1.9955, -1.7748, -2.1202, -2.1042,\n",
            "        -1.7748, -1.7748, -2.1042, -2.1202, -2.0599, -1.9740, -1.7697, -2.0716,\n",
            "        -2.0637, -1.8955, -2.0907, -1.8955, -2.0907, -1.9866, -1.9230, -1.8225,\n",
            "        -1.9955, -1.7748, -1.6601, -1.9955, -1.7748, -1.9337, -2.0687, -1.7748,\n",
            "        -1.6601, -1.9230, -1.9230, -2.0454, -2.0454, -2.0570, -2.0822, -1.9955,\n",
            "        -2.0687, -1.6601, -1.9955, -1.6601, -1.8225, -1.9230, -2.0822, -1.7461,\n",
            "        -2.0637, -1.7485, -2.0716, -1.8212, -2.0637, -2.0907, -1.8955, -1.7485,\n",
            "        -1.7697, -1.7697, -1.9740, -1.7771, -2.1202, -1.6601, -2.0454, -1.9955,\n",
            "        -2.0599, -2.1937, -2.1681, -1.7546, -2.0815, -2.1649, -2.0158, -1.7591,\n",
            "        -2.1024, -1.6702, -1.7336, -1.7560, -2.1192, -1.7312, -1.7715, -2.1197,\n",
            "        -1.8152, -2.0677, -2.0677, -1.9717, -2.0640, -2.0196, -1.7759, -2.1096,\n",
            "        -1.9648, -2.1520, -1.6475, -2.0128, -1.9648, -1.6475, -1.7881, -1.9070,\n",
            "        -2.1284, -1.9070, -2.1117, -1.7773, -1.7773, -1.6737, -2.1130, -1.7783,\n",
            "        -1.7412, -2.0196, -1.6475, -1.9648, -1.6475, -2.1096, -1.8114, -1.9726,\n",
            "        -2.0493, -2.1115, -2.0493, -2.0493, -2.0737, -1.7164, -2.1649, -2.0158,\n",
            "        -2.2179, -2.2179, -1.6990, -1.7092, -2.0279, -1.7092, -1.7447, -1.7447,\n",
            "        -2.2682, -1.6990, -2.0427, -2.0907, -1.8212, -1.8955, -2.0907, -1.8955,\n",
            "        -1.8212, -2.0907, -1.8212, -1.7485, -2.2682, -2.2446, -2.0446, -1.7312,\n",
            "        -1.7591, -1.8940, -2.1416, -1.7997, -2.0211, -2.1131, -2.1435, -1.7136,\n",
            "        -2.0148, -1.8987, -2.0382, -2.1117, -2.1117, -2.0382, -2.0382, -1.9070,\n",
            "        -1.7773, -2.0382, -2.1284, -2.0834, -2.1395, -2.1416, -2.2105, -1.7997,\n",
            "        -1.9242, -2.1236, -2.0677, -1.8497, -1.9242, -2.0431, -2.0211, -1.6354,\n",
            "        -2.1236, -1.8497, -2.0211, -2.1131, -1.7480, -2.1934, -2.1435, -1.8103,\n",
            "        -2.2317, -2.1131, -2.0463, -2.0832, -1.9648, -1.6686, -1.9234, -2.0640,\n",
            "        -1.9648, -2.0211], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007])\n",
            "new_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007])\n",
            "new_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007])\n",
            "new_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007])\n",
            "new_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007])\n",
            "new_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007])\n",
            "new_logp:  tensor([-1.7635, -2.0531, -2.0245, -2.1009, -1.8610, -1.9800, -1.9716, -2.0326,\n",
            "        -1.6517, -2.1213, -2.0111, -1.7913, -1.6737, -2.0190, -2.0190, -1.8795,\n",
            "        -1.9704, -1.8792, -1.8792, -1.8792, -1.9550, -1.8803, -2.0567, -1.6779,\n",
            "        -1.8081, -1.9132, -2.1007, -2.0869, -1.9550, -2.1316, -2.0907, -1.9924,\n",
            "        -1.7812, -1.8391, -1.8391, -2.0907, -2.0827, -2.0827, -1.7812, -1.8803,\n",
            "        -1.8391, -2.1316, -2.1316, -2.0567, -1.8803, -2.0907, -1.8672, -1.9955,\n",
            "        -2.0721, -1.9308, -2.0474, -1.8784, -2.0869, -1.9704, -1.9550, -1.8391,\n",
            "        -2.0281, -2.0567, -1.6779, -1.8081, -2.1007, -2.1136, -2.1130, -1.8152,\n",
            "        -1.7386, -1.7532, -2.0234, -1.7386, -1.7532, -1.7532, -2.0671, -1.9170,\n",
            "        -2.1001, -2.0234, -1.7386, -2.0671, -2.0234, -1.9163, -1.8398, -2.0560,\n",
            "        -2.0829, -1.9163, -1.8398, -2.0677, -1.9163, -1.9163, -2.0677, -1.9163,\n",
            "        -2.0829, -2.0560, -1.9163, -1.9163, -2.0677, -2.0677, -1.8398, -1.8398,\n",
            "        -1.9163, -1.8398, -2.0677, -1.9717, -1.9079, -1.9079, -1.6837, -2.0677,\n",
            "        -1.8398, -1.7386, -1.9170, -1.7532, -2.0881, -1.7532, -1.7532, -1.7532,\n",
            "        -1.7532, -2.0881, -2.0671, -1.7532, -2.0881, -1.7452, -1.8229, -1.9990,\n",
            "        -1.8092, -1.8092, -2.1074, -1.7618, -2.0515, -1.7957, -2.0515, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0880, -1.8186, -1.8751, -2.0007, -1.7479,\n",
            "        -2.0783, -2.0880, -2.0007, -2.0659, -1.9828, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0868, -1.9828, -2.0960, -2.0515, -2.0960, -1.6902, -2.0588, -1.8952,\n",
            "        -1.8952, -1.7479, -2.0560, -2.0007, -1.9828, -1.9143, -2.0515, -2.1131,\n",
            "        -1.6902, -2.0868, -1.8952, -1.8358, -1.7479, -1.7595, -1.7618, -2.0424,\n",
            "        -1.7618, -2.0424, -2.0626, -2.0007, -1.8952, -1.8952, -1.8952, -1.8358,\n",
            "        -1.8952, -2.0659, -1.7479, -2.0880, -2.0880, -1.8186, -2.0560, -1.7595,\n",
            "        -1.9990, -2.0626, -2.0880, -2.0560, -2.0783, -1.8186, -2.0560, -2.0880,\n",
            "        -2.0007, -2.0588, -2.0588, -1.8952, -1.8358, -1.8358, -2.0588, -1.9828,\n",
            "        -1.7957, -1.9143, -2.0424, -1.9990, -1.8092, -1.7618, -2.1131, -2.0960,\n",
            "        -2.0515, -1.7957, -1.7957, -2.0960, -1.6902, -2.0588, -1.8358, -1.7479,\n",
            "        -1.8186, -2.0783, -2.0560, -2.0560, -2.0007, -1.7479, -1.8751, -1.8186,\n",
            "        -1.8751, -2.0880, -2.0783, -1.8751, -2.0783, -2.0783, -2.0783, -2.0880,\n",
            "        -1.7595, -1.8092, -1.7784, -1.8092, -2.0626, -1.8186, -2.0560, -1.8186,\n",
            "        -1.8186, -1.8186, -2.0783, -1.8186, -2.0007, -1.7479, -2.0880, -1.8186,\n",
            "        -1.8186, -2.0007], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678])\n",
            "new_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678])\n",
            "new_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678])\n",
            "new_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678])\n",
            "new_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678])\n",
            "new_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678])\n",
            "new_logp:  tensor([-2.0281, -1.8391, -2.1316, -1.8803, -1.8391, -2.0907, -2.0946, -2.1212,\n",
            "        -1.7814, -2.0567, -2.1316, -2.0281, -1.6779, -1.9898, -1.7784, -2.0684,\n",
            "        -1.6628, -2.1212, -1.8946, -1.8952, -1.8952, -2.0659, -2.0588, -2.0868,\n",
            "        -1.9828, -2.0576, -1.9227, -2.1115, -2.0389, -2.0837, -1.8525, -2.0929,\n",
            "        -1.9483, -1.8391, -2.0907, -1.8391, -1.9381, -1.9022, -1.6628, -1.7814,\n",
            "        -2.0907, -1.8946, -2.0659, -1.7479, -1.9964, -1.8358, -2.0659, -1.8358,\n",
            "        -1.9828, -1.6525, -2.0588, -1.8952, -2.0588, -1.8952, -2.0659, -2.0588,\n",
            "        -1.9828, -1.9227, -1.6525, -1.9828, -2.0389, -2.1518, -2.1518, -1.9600,\n",
            "        -1.7867, -1.6525, -1.9828, -1.8568, -2.0631, -1.9227, -2.1115, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.0576, -1.9227, -2.0631, -1.9227, -2.0576,\n",
            "        -1.9227, -2.1115, -1.6525, -1.8358, -2.0659, -2.0868, -1.7479, -1.9964,\n",
            "        -1.7479, -2.0851, -1.9119, -2.0961, -1.7418, -2.1518, -1.7867, -2.1115,\n",
            "        -1.8568, -1.6525, -1.7479, -1.7997, -2.0961, -1.7418, -1.9600, -2.0837,\n",
            "        -1.8525, -2.0929, -1.9483, -1.6779, -1.7784, -2.1226, -2.0684, -2.1131,\n",
            "        -1.9022, -1.7579, -1.6628, -2.0003, -1.8946, -1.9828, -1.6525, -1.8358,\n",
            "        -1.9828, -1.8568, -2.0389, -1.7867, -1.9227, -2.0389, -2.0463, -1.9119,\n",
            "        -2.0851, -2.0512, -1.9119, -1.7418, -1.7867, -2.1115, -1.6525, -1.8952,\n",
            "        -1.8358, -2.0659, -2.0659, -2.0659, -1.9828, -1.9227, -1.6525, -2.0659,\n",
            "        -1.7479, -2.0851, -1.7418, -1.7867, -1.6525, -2.0868, -1.8952, -2.0868,\n",
            "        -1.7479, -2.0961, -1.7418, -1.7867, -2.0631, -2.1115, -2.1115, -2.1115,\n",
            "        -2.1115, -2.0576, -2.0389, -1.9600, -1.7867, -1.6525, -1.8358, -2.0868,\n",
            "        -1.7479, -2.0512, -2.0851, -2.0851, -2.0851, -1.9964, -1.8952, -1.7479,\n",
            "        -1.9119, -2.0512, -1.7997, -1.7418, -2.0837, -1.8029, -2.0837, -1.9600,\n",
            "        -2.1518, -1.8525, -1.9483, -2.0567, -2.1316, -2.0567, -1.8803, -2.0567,\n",
            "        -2.0907, -1.8946, -2.0588, -2.0659, -1.8358, -2.0588, -1.7479, -2.0961,\n",
            "        -1.9964, -1.8952, -2.0588, -1.8952, -2.0659, -1.8952, -1.9828, -1.9227,\n",
            "        -2.0631, -2.0576, -2.0631, -2.0576, -1.6525, -1.8358, -2.0588, -1.9828,\n",
            "        -1.8568, -2.0576, -1.8568, -1.9227, -2.0631, -1.8568, -1.9227, -2.0389,\n",
            "        -1.7867, -1.9227, -2.0631, -1.8568, -2.0576, -1.9227, -1.6525, -2.0659,\n",
            "        -2.0868, -1.7479, -1.9119, -2.0512, -1.7418, -1.8029, -2.0463, -1.9964,\n",
            "        -1.9828, -1.6525, -1.8952, -1.9828, -2.1115, -2.0389, -1.9600, -1.8029,\n",
            "        -1.8525, -1.8678], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952])\n",
            "new_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952])\n",
            "new_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952])\n",
            "new_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952])\n",
            "new_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952])\n",
            "new_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952])\n",
            "new_logp:  tensor([-1.7997, -2.0512, -2.0961, -1.9119, -2.0512, -2.0512, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0961, -2.0961, -1.7418, -2.0837, -1.7867, -2.0631, -2.0389,\n",
            "        -1.8525, -1.9483, -1.8391, -2.0907, -1.8946, -2.0588, -1.8952, -1.7479,\n",
            "        -2.0512, -1.7418, -1.8029, -1.8029, -1.7867, -2.1115, -2.0576, -2.1115,\n",
            "        -1.9227, -2.1115, -2.0576, -1.8568, -2.1115, -1.9227, -2.0631, -2.1115,\n",
            "        -1.6525, -1.8952, -1.9828, -1.6525, -2.0588, -1.7479, -2.0961, -2.0851,\n",
            "        -2.0512, -1.9119, -2.0512, -1.9964, -2.0868, -2.0588, -2.0659, -1.7479,\n",
            "        -1.9964, -1.7479, -1.9119, -1.9964, -2.0868, -1.8358, -2.0868, -1.8952,\n",
            "        -1.9828, -2.1115, -2.1115, -2.0389, -2.1518, -2.0463, -2.0512, -2.0851,\n",
            "        -2.0961, -2.0851, -1.9119, -1.9964, -2.0868, -1.9828, -1.8568, -2.0389,\n",
            "        -2.0837, -1.7867, -1.6525, -1.7479, -1.7418, -1.8029, -1.7867, -2.0389,\n",
            "        -1.9600, -2.1518, -1.8029, -2.0837, -2.0463, -1.7418, -1.7867, -1.8568,\n",
            "        -2.0576, -1.6525, -1.9828, -2.0631, -1.6525, -1.8358, -1.8952, -2.0659,\n",
            "        -2.0868, -2.0659, -1.7479, -1.7997, -1.9964, -1.7479, -1.7418, -1.8029,\n",
            "        -2.1518, -2.0463, -2.0512, -2.0851, -2.0961, -1.9964, -2.0868, -1.9828,\n",
            "        -2.0631, -2.0576, -2.0631, -2.1115, -2.0631, -2.0389, -1.8525, -1.7784,\n",
            "        -1.6628, -2.1212, -2.0003, -2.1212, -1.7814, -2.0907, -1.8946, -1.8358,\n",
            "        -1.9828, -1.8568, -2.1115, -1.8568, -2.0576, -2.0631, -1.6525, -1.8952,\n",
            "        -2.0588, -1.7479, -1.9964, -1.8358, -1.8952, -1.8952, -2.0868, -1.7479,\n",
            "        -1.9119, -1.9119, -1.9119, -1.7997, -1.7418, -1.8029, -2.0463, -2.0512,\n",
            "        -2.0512, -1.9964, -1.9828, -2.1115, -1.9227, -1.9227, -1.9227, -2.0631,\n",
            "        -1.8568, -2.0576, -1.9227, -2.1115, -1.8568, -1.6525, -1.7479, -1.7418,\n",
            "        -2.0837, -2.0463, -1.7997, -1.7418, -1.8029, -1.9600, -1.9600, -1.8525,\n",
            "        -1.9483, -1.8803, -1.6779, -1.8818, -1.8818, -1.8678, -1.8818, -1.8818,\n",
            "        -1.9898, -1.9483, -2.0281, -2.0281, -1.8391, -1.8391, -2.1316, -1.6779,\n",
            "        -1.7784, -1.7579, -2.1055, -1.8818, -1.7784, -1.7579, -1.7579, -1.7579,\n",
            "        -1.9022, -2.1131, -1.6628, -2.0003, -2.0003, -2.0003, -1.7814, -2.0907,\n",
            "        -2.0946, -1.9381, -1.7579, -1.6628, -1.7814, -1.8391, -2.0567, -1.6779,\n",
            "        -2.0929, -2.0929, -1.9898, -1.9898, -2.1059, -2.1195, -1.8846, -1.7784,\n",
            "        -2.0684, -1.6628, -2.1212, -1.8946, -1.7479, -2.0961, -2.0851, -1.9964,\n",
            "        -1.8952, -2.0868, -2.0868, -2.0868, -1.8952, -1.7479, -1.9119, -1.7997,\n",
            "        -1.9964, -1.8952], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531])\n",
            "new_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531])\n",
            "new_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531])\n",
            "new_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531])\n",
            "new_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531])\n",
            "new_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531])\n",
            "new_logp:  tensor([-2.0721, -2.1013, -1.8363, -2.0474, -2.0731, -1.8784, -1.9550, -2.0907,\n",
            "        -1.9924, -1.7812, -1.8803, -2.1316, -1.8803, -2.0567, -2.1316, -2.0281,\n",
            "        -1.6779, -1.8483, -2.1136, -1.8795, -1.8081, -1.7722, -2.0387, -2.1007,\n",
            "        -1.8483, -1.9550, -2.0567, -2.0281, -1.8803, -2.0281, -1.8803, -2.0567,\n",
            "        -1.6779, -1.9704, -1.9550, -2.0907, -1.9778, -2.1210, -2.1007, -1.8483,\n",
            "        -2.0869, -1.8792, -1.8081, -1.9132, -1.9132, -2.0387, -1.6503, -1.7812,\n",
            "        -2.0281, -2.0567, -1.6779, -2.1136, -1.7783, -2.1234, -1.8152, -1.9163,\n",
            "        -1.9163, -1.8398, -1.7386, -1.7452, -1.8229, -2.1074, -2.1946, -1.8229,\n",
            "        -1.7618, -1.6902, -1.8358, -1.8358, -1.9828, -1.7957, -1.7957, -2.0960,\n",
            "        -2.0515, -1.9143, -2.0515, -2.0515, -2.0960, -1.6902, -1.7479, -1.8186,\n",
            "        -1.8186, -2.0880, -1.7595, -1.7618, -2.1131, -1.9143, -2.0515, -1.6902,\n",
            "        -1.8952, -2.0868, -1.7479, -2.0560, -2.0560, -1.8751, -1.8751, -2.0560,\n",
            "        -2.0880, -2.0880, -2.0783, -1.8186, -2.0560, -2.0783, -2.0560, -2.0880,\n",
            "        -1.8751, -1.7595, -2.0626, -2.0007, -1.7479, -2.0783, -1.7595, -2.0626,\n",
            "        -1.8751, -2.0007, -1.9828, -1.7957, -1.9143, -2.0515, -1.6902, -2.0588,\n",
            "        -2.0588, -2.0659, -1.9828, -2.0424, -1.9990, -2.1074, -2.1946, -1.8229,\n",
            "        -1.8092, -1.7618, -2.0960, -2.0515, -2.0424, -2.1946, -2.0575, -2.0671,\n",
            "        -2.1001, -1.7452, -1.7726, -2.0893, -1.8949, -1.7783, -2.1248, -1.8901,\n",
            "        -2.0945, -2.0554, -2.0554, -1.8901, -2.0945, -2.0718, -1.9800, -1.8625,\n",
            "        -1.8600, -1.7783, -2.0945, -2.0945, -2.1248, -1.8901, -2.0945, -1.6921,\n",
            "        -1.9221, -2.0554, -2.0945, -1.7825, -2.0718, -1.9716, -2.0326, -1.8190,\n",
            "        -2.1009, -2.1174, -2.1174, -2.0935, -2.1174, -1.8965, -2.0868, -2.0588,\n",
            "        -1.8358, -2.0588, -2.0868, -1.9828, -2.0245, -2.1009, -2.1009, -2.1009,\n",
            "        -1.7635, -1.9088, -1.9088, -2.0456, -2.0456, -1.6828, -1.8952, -1.8358,\n",
            "        -1.9828, -1.6828, -1.8358, -2.0659, -2.0868, -1.8952, -1.7479, -2.0646,\n",
            "        -2.0687, -2.0935, -1.9023, -1.7970, -2.0002, -1.9828, -2.0245, -1.8610,\n",
            "        -1.7783, -1.8901, -1.8901, -1.7825, -2.0945, -1.6921, -1.9221, -2.0945,\n",
            "        -2.0554, -2.0718, -1.8600, -1.8625, -2.1213, -1.8949, -1.9800, -1.8625,\n",
            "        -2.0960, -1.8600, -2.0960, -1.7783, -1.7825, -2.0945, -1.8901, -1.6921,\n",
            "        -1.8965, -1.8952, -1.9828, -1.9088, -1.9088, -2.0245, -1.7635, -1.6828,\n",
            "        -1.9828, -1.8614, -2.0456, -2.1142, -1.9088, -1.9088, -2.0456, -1.9088,\n",
            "        -1.9088, -2.0531], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673])\n",
            "new_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673])\n",
            "new_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673])\n",
            "new_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673])\n",
            "new_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673])\n",
            "new_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673])\n",
            "new_logp:  tensor([-2.0554, -2.0718, -1.9713, -2.0344, -1.8110, -2.1178, -2.1151, -2.0938,\n",
            "        -1.8503, -1.7581, -2.0925, -1.6461, -1.8643, -1.8520, -1.9809, -2.0900,\n",
            "        -1.8520, -2.1102, -1.7969, -1.9163, -2.0560, -1.7386, -1.9357, -2.0913,\n",
            "        -1.9357, -2.0638, -2.0007, -1.7386, -1.7560, -1.9357, -1.9357, -1.7312,\n",
            "        -2.0825, -1.8468, -2.0574, -1.7485, -2.0574, -2.0704, -1.7485, -2.1835,\n",
            "        -1.8468, -2.0126, -1.7514, -2.1165, -1.9088, -1.6846, -2.0588, -1.7479,\n",
            "        -2.0704, -1.7485, -1.8389, -1.8389, -1.7514, -1.8297, -2.0712, -1.9088,\n",
            "        -1.6846, -2.0588, -1.9828, -2.0358, -2.0126, -2.0981, -1.7677, -2.0126,\n",
            "        -2.0574, -2.0704, -2.0704, -2.0704, -2.0704, -1.8955, -1.8212, -1.7485,\n",
            "        -2.0126, -2.0981, -1.8389, -2.0981, -2.1835, -2.0136, -1.7597, -2.1223,\n",
            "        -1.7442, -1.9195, -1.9195, -2.0354, -2.0949, -2.1524, -2.1435, -1.8050,\n",
            "        -1.7853, -2.2048, -1.6142, -2.1184, -2.1386, -2.1184, -1.7994, -1.8358,\n",
            "        -1.9828, -2.1253, -1.7660, -2.1253, -2.0830, -1.6793, -1.9824, -1.9964,\n",
            "        -2.0868, -1.8952, -2.0588, -1.9828, -1.6937, -1.7479, -2.0851, -2.0851,\n",
            "        -1.7997, -2.0512, -1.7997, -2.0512, -1.7997, -1.7997, -2.0512, -2.0851,\n",
            "        -2.0512, -1.7418, -1.9824, -1.7997, -1.7418, -1.7924, -2.1253, -2.1253,\n",
            "        -1.7660, -1.7527, -2.1333, -1.7527, -2.0830, -2.2605, -2.2605, -2.2605,\n",
            "        -1.9824, -2.0512, -1.7418, -1.8664, -1.9116, -1.9545, -2.0890, -2.1435,\n",
            "        -2.0094, -1.9070, -1.9070, -1.6737, -2.1435, -1.8616, -1.7386, -2.0670,\n",
            "        -1.6983, -2.1350, -1.9717, -2.0640, -2.0640, -2.0196, -2.0912, -1.8056,\n",
            "        -1.6495, -2.0424, -1.8553, -1.8553, -2.1702, -1.7566, -1.8553, -2.1702,\n",
            "        -2.1702, -2.0537, -2.0299, -1.7673, -1.7566, -1.8553, -1.8553, -2.0960,\n",
            "        -1.6495, -1.6902, -1.8358, -1.8358, -1.8358, -2.0659, -1.9828, -2.0424,\n",
            "        -2.0960, -1.8553, -1.6495, -1.7957, -2.1131, -1.6902, -1.7479, -2.1730,\n",
            "        -2.1860, -2.1363, -2.0588, -1.9828, -1.9143, -2.1131, -2.0424, -1.8553,\n",
            "        -1.6495, -1.6902, -1.8952, -1.7479, -2.1730, -2.1363, -1.8358, -1.8952,\n",
            "        -1.8358, -1.8358, -1.8358, -2.0659, -1.8358, -1.8952, -2.0659, -1.7479,\n",
            "        -1.7673, -1.8553, -1.8553, -1.6495, -1.7957, -2.0424, -2.1763, -2.0912,\n",
            "        -2.1828, -1.8056, -2.1702, -1.6495, -2.0960, -2.0424, -1.8553, -2.0960,\n",
            "        -2.1702, -1.7566, -1.7566, -1.7566, -2.1702, -1.6495, -1.9143, -1.9143,\n",
            "        -1.6902, -1.8358, -2.0659, -2.0588, -1.7479, -2.1363, -2.0659, -1.7479,\n",
            "        -2.1730, -1.7673], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525])\n",
            "new_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525])\n",
            "new_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525])\n",
            "new_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525])\n",
            "new_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525])\n",
            "new_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525])\n",
            "new_logp:  tensor([-1.9061, -1.8110, -1.9061, -1.6461, -2.0900, -1.8643, -1.8643, -1.7961,\n",
            "        -1.8901, -1.7825, -2.1248, -2.0718, -2.1102, -1.8868, -2.1102, -1.7969,\n",
            "        -1.9163, -2.0677, -2.0829, -1.8398, -1.9717, -1.6670, -2.0560, -1.9163,\n",
            "        -1.8398, -2.0677, -1.9717, -1.8948, -1.8503, -1.8503, -2.0427, -1.7630,\n",
            "        -1.6670, -1.9163, -1.9163, -1.8398, -1.8398, -1.9717, -2.0666, -1.8503,\n",
            "        -1.8503, -1.6670, -2.0560, -2.0560, -2.0560, -1.8398, -1.9717, -2.0499,\n",
            "        -2.0427, -1.7597, -2.1195, -1.7442, -2.0354, -2.1142, -2.1142, -1.6505,\n",
            "        -2.1249, -2.1057, -1.9676, -2.0949, -1.7148, -2.1524, -2.0148, -1.7497,\n",
            "        -1.8026, -1.5953, -2.0217, -1.5953, -2.0217, -1.8569, -2.1067, -1.6122,\n",
            "        -1.8585, -1.8585, -1.8127, -2.0014, -1.8522, -1.8522, -2.2292, -2.1067,\n",
            "        -1.6122, -2.1078, -2.2007, -2.1146, -2.1078, -1.6459, -1.8908, -2.0314,\n",
            "        -2.1307, -2.1307, -1.8473, -1.8473, -1.8874, -1.8473, -1.6792, -1.9742,\n",
            "        -2.0546, -1.8473, -2.0314, -1.8874, -1.8473, -2.0690, -1.6185, -1.6792,\n",
            "        -2.0963, -1.7500, -1.7656, -1.7115, -1.7115, -2.2519, -1.7115, -1.8212,\n",
            "        -1.6122, -2.0014, -1.6122, -1.8127, -2.1078, -1.8127, -2.1078, -1.8585,\n",
            "        -1.8127, -1.6459, -2.0966, -1.8908, -1.6792, -1.7500, -2.0963, -1.9316,\n",
            "        -1.8358, -1.9828, -1.8539, -2.0300, -1.8245, -2.1367, -1.6185, -1.8874,\n",
            "        -2.0546, -2.0546, -1.6792, -2.1525, -2.0232, -1.9316, -1.7479, -1.7527,\n",
            "        -1.7527, -2.1252, -2.1955, -2.1179, -2.0588, -2.0868, -2.0588, -2.0588,\n",
            "        -2.0659, -1.8952, -1.7479, -1.7455, -1.6968, -1.6968, -1.7527, -2.1619,\n",
            "        -1.6338, -2.0531, -1.8539, -1.6851, -1.8358, -2.0659, -2.0868, -1.8952,\n",
            "        -2.0588, -2.0588, -2.0868, -1.7479, -2.1924, -2.1179, -2.0588, -1.8358,\n",
            "        -2.0588, -1.7479, -1.7527, -1.7527, -1.6338, -2.1179, -2.1179, -2.1179,\n",
            "        -1.9051, -2.0466, -2.1179, -2.0531, -2.0531, -1.6851, -1.7479, -2.1955,\n",
            "        -1.6968, -1.6968, -2.0744, -1.7455, -1.7527, -1.6338, -2.0300, -1.6338,\n",
            "        -1.9051, -2.0531, -2.1179, -2.0531, -1.9051, -2.0466, -2.0466, -2.0300,\n",
            "        -2.1619, -2.1025, -1.7527, -2.1619, -2.1756, -1.7527, -2.1025, -2.1619,\n",
            "        -1.8245, -1.8667, -1.7789, -1.8358, -2.0659, -2.0659, -1.8952, -2.0659,\n",
            "        -2.0659, -1.7479, -1.9088, -1.9838, -2.0659, -2.0868, -1.8952, -1.7479,\n",
            "        -2.0691, -2.0936, -2.0936, -1.9088, -2.0953, -1.9838, -1.7479, -1.7411,\n",
            "        -1.6389, -2.1914, -1.7658, -1.8077, -2.0692, -1.6389, -1.8388, -2.0963,\n",
            "        -1.7500, -2.1525], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402])\n",
            "new_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402])\n",
            "new_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402])\n",
            "new_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402])\n",
            "new_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402])\n",
            "new_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402])\n",
            "new_logp:  tensor([-1.7784, -1.9990, -2.0626, -2.0783, -1.8751, -2.0880, -2.0783, -2.0783,\n",
            "        -1.8186, -2.0880, -1.8751, -2.0880, -1.8186, -2.0783, -2.0783, -1.8186,\n",
            "        -1.8751, -2.0783, -1.8751, -1.8186, -2.0783, -1.8186, -1.7595, -2.1074,\n",
            "        -1.9990, -1.7618, -1.7957, -2.0424, -1.7618, -1.9143, -1.9143, -1.6902,\n",
            "        -1.9828, -1.6902, -1.9828, -1.9143, -2.1131, -2.0960, -1.7957, -2.1131,\n",
            "        -1.6902, -2.0588, -1.8358, -2.0868, -2.0868, -1.7479, -1.8751, -1.7595,\n",
            "        -2.1946, -1.7726, -1.7913, -2.1117, -2.0834, -1.7913, -1.6737, -1.8152,\n",
            "        -2.0829, -2.0560, -1.7386, -1.7452, -1.7726, -1.9689, -2.1260, -1.7470,\n",
            "        -2.1532, -2.1234, -2.1197, -1.8152, -2.0560, -1.9163, -1.7386, -2.0671,\n",
            "        -2.0671, -1.9170, -2.0671, -1.7532, -1.9170, -2.0881, -2.0234, -1.9163,\n",
            "        -2.0677, -1.9163, -2.0829, -2.0829, -1.9163, -2.0677, -1.9163, -2.0829,\n",
            "        -2.0560, -1.7386, -1.9170, -2.1001, -2.1001, -1.7452, -1.7585, -2.0470,\n",
            "        -2.0196, -1.7726, -2.0111, -1.8054, -2.0633, -1.7361, -2.0638, -1.7312,\n",
            "        -2.0535, -1.7560, -1.9357, -2.0913, -1.7312, -2.0066, -2.1681, -2.0824,\n",
            "        -1.8463, -1.8210, -2.1018, -2.1937, -2.1681, -2.1681, -1.7546, -2.0815,\n",
            "        -1.7164, -1.7236, -2.0343, -1.8940, -2.2105, -2.2105, -2.1416, -1.5833,\n",
            "        -1.9070, -2.1117, -2.1117, -1.7773, -2.1117, -2.0834, -1.7997, -1.6686,\n",
            "        -1.7939, -1.7401, -1.8103, -1.7837, -2.0211, -1.8402],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829])\n",
            "new_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829])\n",
            "new_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829])\n",
            "new_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829])\n",
            "new_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829])\n",
            "new_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n",
            "old_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829])\n",
            "new_logp:  tensor([-2.0868, -1.7479, -1.7997, -1.7997, -1.7418, -1.8029, -1.8029, -2.1518,\n",
            "        -2.0837, -1.7867, -1.9227, -2.0389, -2.1518, -1.7867, -2.1115, -2.0576,\n",
            "        -2.1115, -2.1115, -2.0576, -2.1115, -2.0389, -2.1518, -2.0463, -1.7418,\n",
            "        -1.9600, -1.7867, -2.0576, -2.1115, -2.0576, -1.8568, -2.1115, -1.8568,\n",
            "        -2.1115, -2.0576, -2.1115, -2.0389, -2.0463, -1.9119, -1.7997, -1.9119,\n",
            "        -1.7418, -1.8029, -1.9600, -2.0837, -2.0463, -1.9964, -2.0659, -1.7479,\n",
            "        -1.7418, -1.8525, -2.0929, -1.9483, -1.6779, -2.1059, -1.9503, -2.0834,\n",
            "        -1.7733, -2.1117, -1.9070, -2.1117, -2.0834, -2.1057, -1.7733, -1.9070,\n",
            "        -2.1284, -2.1117, -1.7773, -2.0834, -2.1249, -2.1057, -2.1057, -1.8868,\n",
            "        -1.7961, -1.7825, -1.7825, -1.6921, -1.9074, -2.0659, -1.7479, -2.0215,\n",
            "        -1.7479, -2.0532, -2.0982, -2.1225, -2.1225, -1.9216, -1.7797, -2.0532,\n",
            "        -1.7051, -1.9449, -2.0761, -2.1225, -2.0215, -2.0868, -2.0588, -1.8358,\n",
            "        -1.7479, -2.1225, -1.7797, -1.7051, -1.8330, -1.9809, -1.9713, -1.6461,\n",
            "        -1.8643, -1.9809, -2.0900, -2.0900, -1.9713, -2.0925, -2.1151, -1.9074,\n",
            "        -2.0659, -1.7479, -1.7051, -1.8017, -2.0245, -1.9449, -1.8017, -1.8614,\n",
            "        -1.9088, -1.6828, -1.8952, -2.0588, -2.0659, -2.0868, -2.0868, -1.8358,\n",
            "        -2.0588, -2.0868, -1.7479, -2.0532, -1.7051, -2.0633, -1.8330, -1.8520,\n",
            "        -1.8520, -1.9809, -1.9809, -1.9713, -1.8110, -2.0344, -1.9061, -2.0344,\n",
            "        -2.0925, -2.1178, -1.6461, -2.1102, -1.8868, -1.7961, -1.8901, -1.6921,\n",
            "        -1.9074, -1.7479, -1.9216, -1.7797, -2.1225, -2.0982, -1.9216, -1.7051,\n",
            "        -1.9449, -2.1314, -2.1314, -1.8330, -1.8643, -2.1102, -1.8868, -1.9713,\n",
            "        -2.0925, -2.0925, -1.6461, -2.0900, -1.9809, -1.8643, -2.0900, -1.9809,\n",
            "        -1.9809, -2.0900, -1.7961, -2.0945, -2.0718, -1.8520, -1.9713, -1.8110,\n",
            "        -2.0344, -1.9061, -1.8110, -1.9061, -1.9061, -1.6461, -1.8643, -2.0900,\n",
            "        -1.7961, -2.1248, -2.0718, -1.8643, -1.9713, -1.6461, -1.8520, -1.9809,\n",
            "        -2.1102, -2.1249, -2.0259, -1.9676, -1.9195, -1.6505, -1.7969, -1.8398,\n",
            "        -1.7386, -2.1192, -1.9357, -1.7560, -2.1192, -1.9357, -1.7312, -2.1662,\n",
            "        -1.7630, -1.8948, -1.6670, -1.7386, -1.7560, -1.7560, -2.0638, -2.0638,\n",
            "        -1.7560, -2.0913, -2.0007, -2.0560, -1.9717, -2.0427, -1.7630, -1.6670,\n",
            "        -1.9163, -1.7386, -2.0638, -2.1192, -2.1192, -1.7560, -1.7312, -2.0825,\n",
            "        -2.0722, -1.9357, -1.7560, -2.0913, -1.7312, -1.7597, -1.8846, -2.1059,\n",
            "        -1.8196, -2.0829], grad_fn=<SqueezeBackward1>)\n",
            "advantages:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "losses:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MinimumBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-947b86b3e675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_critic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gae\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_ppo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_parameters_ppo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_ppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'num_frames'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'smooth_reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-6d492cef3196>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(args, parameter_update)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mexps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_experiences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlogs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-de4f421233b5>\u001b[0m in \u001b[0;36mcollect_experiences\u001b[0;34m(env, acmodel, args, device)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "args = Config(use_critic=True, use_gae=True)\n",
        "df_ppo = run_experiment(args, update_parameters_ppo)\n",
        "df_ppo.plot(x='num_frames', y=['reward', 'smooth_reward'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC25PuG1oE0z"
      },
      "source": [
        "# Fancy Plots\n",
        "\n",
        "If you've gotten to this point, congrats: you've succesfully implemented REINFORCE, VPG, GAE, and PPO!  While we've been able to anecdotally compare their performance, we don't have any sense of *scientific rigor*.  Notably, given the variance you've likely seen between runs of these models, a single run may not reflect how strong a model really is.\n",
        "\n",
        "For this problem, train each of these 4 methods using multiple seeds (at least 5, but more if you feel you need them).  Then, generate a high quality reward curve plot comparing each algorithm.  The plot should be clean and legible, and clearly demonstrate the performance and variance of each of the approaches.  As an example, see Figure 3 of the PPO paper (although we're only evaluating on a single environment).\n",
        "\n",
        "Be creative and make something pretty: it matters for good science!\n",
        "\n",
        "**Note:** you should leave a few hours for this to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXgEr5bDpCB6"
      },
      "outputs": [],
      "source": [
        "### TODO: generate your master comparison plot (30 pts)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "policygradients",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53d9de2b07684a9fb7f49de014f7ae6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e0792ef31a741afb81de3ba7ec5c1f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_34f2fb36defd4ab6896b2948b5834609",
              "IPY_MODEL_fef9dce8ea274550bbb4d743db3a50ed",
              "IPY_MODEL_a905757959d84e019b51b509c9ac3ea4"
            ]
          }
        },
        "4e0792ef31a741afb81de3ba7ec5c1f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34f2fb36defd4ab6896b2948b5834609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0a4205e0e89544a7be6264343b2345a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1e05d4eb4d1c4ae486054c594cedfea8"
          }
        },
        "fef9dce8ea274550bbb4d743db3a50ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2362cc0b657a451699889026db669409",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c11ad007e85f42b581d628372264c74b"
          }
        },
        "a905757959d84e019b51b509c9ac3ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5ceba644f71e435d90c8fcd842ae0cd7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/2000 [39:50&lt;00:00,  1.20s/it, episode=1999, num_frames=499413, smooth_reward=0, reward=0, policy_loss=-]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_910f8a0c0325416d89126ff1ae5ec40c"
          }
        },
        "0a4205e0e89544a7be6264343b2345a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1e05d4eb4d1c4ae486054c594cedfea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2362cc0b657a451699889026db669409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c11ad007e85f42b581d628372264c74b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ceba644f71e435d90c8fcd842ae0cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "910f8a0c0325416d89126ff1ae5ec40c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71215a59ab634960a22089ade1953422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1dd40e6247174deda469b4cb72071565",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_37ad94a42b074c019d583f2ad34a92d4",
              "IPY_MODEL_df1bac7401a247288e75f44b54ff51a4",
              "IPY_MODEL_f8bc776d9bf44ac09906e53e5aa5422e"
            ]
          }
        },
        "1dd40e6247174deda469b4cb72071565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37ad94a42b074c019d583f2ad34a92d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_19fb945f660241f59f1672269fe3f84b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 33%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a04b4b3d02244f6fb3a873fb1301b6f2"
          }
        },
        "df1bac7401a247288e75f44b54ff51a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cfb672bac2734568a8384b3f679d2a5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 658,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f58194fc861433385802dfc0a25c6ff"
          }
        },
        "f8bc776d9bf44ac09906e53e5aa5422e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cb32e2e03f084abc97b58c2dc78ef372",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 658/2000 [02:21&lt;02:01, 11.02it/s, episode=658, num_frames=103138, smooth_reward=0.94, reward=1, policy_loss=0.536]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e3acfda096b4d65a819ee98a39757cd"
          }
        },
        "19fb945f660241f59f1672269fe3f84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a04b4b3d02244f6fb3a873fb1301b6f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cfb672bac2734568a8384b3f679d2a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f58194fc861433385802dfc0a25c6ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb32e2e03f084abc97b58c2dc78ef372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e3acfda096b4d65a819ee98a39757cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3647c8100c2f4ea7b5d9cb599bf00d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e83f686965ba4d87a591bc22ec90afe2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5177ee8914134196ad39198715886568",
              "IPY_MODEL_55a901b501884954b731de10ec52db4d",
              "IPY_MODEL_8032f8b507bd48c086b4361a4f53d420"
            ]
          }
        },
        "e83f686965ba4d87a591bc22ec90afe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5177ee8914134196ad39198715886568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d36734eebb584cc2a07f142ad54d4865",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_261632401b9d436ca7077670e96ea65f"
          }
        },
        "55a901b501884954b731de10ec52db4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e7f9cc0557cc49f4a42120358547fd5d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89faa19b06794088bc80d005b79d5577"
          }
        },
        "8032f8b507bd48c086b4361a4f53d420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ffd958083bc94c4ea56c50f3922b272f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/2000 [13:34&lt;00:00,  2.28it/s, episode=1999, num_frames=498916, smooth_reward=0, reward=0, policy_loss=0.0223, value_loss=7.69e-6]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb16a6db0e954bbca7c4b317845a0230"
          }
        },
        "d36734eebb584cc2a07f142ad54d4865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "261632401b9d436ca7077670e96ea65f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7f9cc0557cc49f4a42120358547fd5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89faa19b06794088bc80d005b79d5577": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ffd958083bc94c4ea56c50f3922b272f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb16a6db0e954bbca7c4b317845a0230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87a9279cb32a48cc96bf8c77fb0f3b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6f03ab8571a044ba95d0f2ac5876fbd5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_07c98fd2dd9e429ca522817e7c4ac751",
              "IPY_MODEL_d284ae2eb35d409384d211256f7de450",
              "IPY_MODEL_6b89df6459df463b91f528ce905d413e"
            ]
          }
        },
        "6f03ab8571a044ba95d0f2ac5876fbd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07c98fd2dd9e429ca522817e7c4ac751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_098092a767a8452a945ec81c3c3f98c6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 86%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d92cb1ecec147bcace9f90eb32e92d5"
          }
        },
        "d284ae2eb35d409384d211256f7de450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2f12c806df474375ada421b2376140fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1715,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f53b4125f9c4f6ab63f7d71cbe7b92c"
          }
        },
        "6b89df6459df463b91f528ce905d413e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_881b2a6517a54d2fa5a0bc01464dc6c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1715/2000 [37:32&lt;06:44,  1.42s/it, episode=1714, num_frames=427512, smooth_reward=0, reward=0, policy_loss=-.00911, value_loss=3.22e-7]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ab6ac119bb042628cf5c831baa37141"
          }
        },
        "098092a767a8452a945ec81c3c3f98c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d92cb1ecec147bcace9f90eb32e92d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f12c806df474375ada421b2376140fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f53b4125f9c4f6ab63f7d71cbe7b92c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "881b2a6517a54d2fa5a0bc01464dc6c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ab6ac119bb042628cf5c831baa37141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aca5775bc6fd49f097907307d56ec056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_33e1e7ad5a384f75a66f3fe7d863dbf8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_88ab82cdd85e41299258f8c9e4423a80",
              "IPY_MODEL_68674dc46bce4ef4b4fcf2e233ea7d9a",
              "IPY_MODEL_7a7ee546174a472e917eee802e4cadf1"
            ]
          }
        },
        "33e1e7ad5a384f75a66f3fe7d863dbf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88ab82cdd85e41299258f8c9e4423a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d651677b953e43aca2ec4b30a9417bbc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 21%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2a45bbc0d0446059a0b60ab86e8b4af"
          }
        },
        "68674dc46bce4ef4b4fcf2e233ea7d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eda2928f4807487c8a0ef7d692e9ce8b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef3e68f2f91d445d9f63a23c86e97f45"
          }
        },
        "7a7ee546174a472e917eee802e4cadf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e14fe6afafe94591b3852375999d698f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 422/2000 [04:33&lt;15:54,  1.65it/s, episode=421, num_frames=102627, smooth_reward=0.1, reward=0, policy_loss=0, value_loss=1.29]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b38109ba9c2d4b3c9918c030445a3bce"
          }
        },
        "d651677b953e43aca2ec4b30a9417bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2a45bbc0d0446059a0b60ab86e8b4af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eda2928f4807487c8a0ef7d692e9ce8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef3e68f2f91d445d9f63a23c86e97f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e14fe6afafe94591b3852375999d698f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b38109ba9c2d4b3c9918c030445a3bce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}