{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisCGit/6.484-reinforcement-learning/blob/main/dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5R7gbE9Emuy"
      },
      "source": [
        "# Spring 2022 6.484 Computational Sensorimotor Learning Assignment 8\n",
        "\n",
        "In this assignment, we will implement Deep Q Network algorithm.\n",
        "\n",
        "You will need to <font color='blue'>answer the bolded questions</font> and <font color='blue'>fill in the missing code snippets (marked by **TODO**)</font>.\n",
        "\n",
        "There are (approximately) 140 total points to be had in this PSET.  `ctrl-f` for \"pts\" to ensure you don't miss questions.\n",
        "\n",
        "**_Please fill in your name below:_**\n",
        "\n",
        "**Name**: Luis Costa Laveron\n",
        "\n",
        "For this assignment, you might find GPU machines to be helpful. You can enable GPUs by:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- Select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "<font color='red'> Make sure you set `show_progress=False` in `dqn_sweep` when you are ready to submit the homework to remove unneccessary outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8NC_sZo0hsw"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqJSim8fwhmL"
      },
      "source": [
        "!pip install gym-minigrid > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdvIlE6-cYvu"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import gym_minigrid\n",
        "from tqdm.notebook import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from torch import optim\n",
        "from typing import Any\n",
        "from copy import deepcopy\n",
        "from gym.wrappers import Monitor\n",
        "from collections import deque\n",
        "from gym_minigrid.wrappers import ImgObsWrapper\n",
        "from gym_minigrid.envs.doorkey import DoorKeyEnv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAzQA6W6ck4C"
      },
      "source": [
        "def set_random_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "# set random seed\n",
        "seed = 0\n",
        "set_random_seed(seed=seed)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1uI2PDywHM6"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "stolen from https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=8nj5sjsk15IT\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env_monitor(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "def plot(logs, x_key, y_key, legend_key, **kwargs):\n",
        "    nums = len(logs[legend_key].unique())\n",
        "    palette = sns.color_palette(\"hls\", nums)\n",
        "    if 'palette' not in kwargs:\n",
        "        kwargs['palette'] = palette\n",
        "    sns.lineplot(x=x_key, y=y_key, data=logs, hue=legend_key, **kwargs)\n",
        "\n",
        "\n",
        "class DoorKeyEnv5x5(DoorKeyEnv):\n",
        "    def __init__(self):\n",
        "        super().__init__(size=5)\n",
        "    \n",
        "    def _reward(self):\n",
        "        \"\"\"\n",
        "        Compute the reward to be given upon success\n",
        "        \"\"\"\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44DsxgFf0n8x"
      },
      "source": [
        "# Task (Environment)\n",
        "\n",
        "In this assignment, we will work with the `DoorKeyEnv5x5` environment from [gym_miniworld](https://github.com/maximecb/gym-minigrid). This environment is a $5\\times 5$ gridworld that we previously used in the policy gradients assignment. The agent needs to pick up the key, open the door, and then go the the green cell. The agent gets a $+1$ reward if it reaches the green cell, and a $0$ reward otherwise.\n",
        "\n",
        "The environment is visually shown below:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVlbJsQ13KwM"
      },
      "source": [
        "env = DoorKeyEnv5x5()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xY4P9aE35Ge"
      },
      "source": [
        "As a refresher, `env.reset()` returns an observation variable, which is a dictionary that contains the current view of the environment (as an image)., agent's facing direction (0: right, 1: down, 2: left, 3: up), and a mission description as a string. The current view of the environment has a similar format as a normal 3-channel image (shape: $7\\times7\\times3$), but it is not an actual image. The values are not pixels. Each value represents a state of the object on that location. Checkout [the source code](https://github.com/maximecb/gym-minigrid/blob/master/gym_minigrid/minigrid.py) for more details. The action space is $7$-dimensional: 0: turn left, 1: turn right, 2: forward, 3: pickup an object, 4: drop an object, 5: activate an object, 6: done completing task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2UqI0tiFXik"
      },
      "source": [
        "# Deep Q Network\n",
        "In the last assignment, we used tabular Q learning for learning to solve FrozenLake. The tabular approach requires a table to store the $Q$ values for all the possible states and actions pairs. This is only feasible when the state space is discrete and low-dimensional. However, in many more practical domains, the state space is high-dimensional, such as images. For example, in video game environments, each frame can be a state. Even a small change in the pixel value (the agent moves by a little bit) will result in a new state. In such scenarios, it becomes impractical to create a $Q$ table for such a big state space as it would require a huge amount of RAM. To overcome this issue, we will make use of function approximators (e.g., neural networks) to approximate the $Q$ table. This leads to the development of an algorithm called _Deep $Q$ Network (DQN)_. Let's implement this. Feel free to refer to the original [DQN](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXNkyaKT7niv"
      },
      "source": [
        "## Define the Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-CX5FsiEUYe"
      },
      "source": [
        "class DQNetwork(nn.Module):\n",
        "    def __init__(self, action_dim):\n",
        "        super().__init__()\n",
        "        #### A simple convoluntional network that takes\n",
        "        #### as input the observation (image), and outputs the \n",
        "        #### Q values for each possible action.\n",
        "        self.conv_net = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, (3, 3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, (3, 3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, (3, 3)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, ob):\n",
        "        #### TODO: get the Q values for each action given the input\n",
        "        #### the input shape is: [batch_size, H, W, 3]\n",
        "        #### output shape should be: [batch_size, # of actions]\n",
        "        assert False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl3aWF7T6df_"
      },
      "source": [
        "**Q1.1 (5 pts)**: What does each argument in `nn.Conv2d(3, 16, (3, 3))` mean?\n",
        "\n",
        "**A**: \n",
        "\n",
        "**Q1.2 (10 pts)**: Fill in the missing code for `DQNetwork`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a__tgvy7sTn"
      },
      "source": [
        "## Build a replay buffer\n",
        "\n",
        "**Q2.1 (20 pts)**: Fill in the missing code for `CyclicBuffer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEjSZI4tFNv_"
      },
      "source": [
        "# create a replay buffer\n",
        "class CyclicBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = []\n",
        "        self.capacity = capacity\n",
        "        self.cur_pos = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.buffer[item]\n",
        "\n",
        "    def append(self, data):\n",
        "        #### TODO: add data to the buffer\n",
        "        #### if the buffer is not full yet, you can simply append the data to the buffer\n",
        "        #### otherwise, you need to replace the oldest data with the current data (FIFO)\n",
        "        #### Hint: you may find self.cur_pos useful, it can be used as a position index\n",
        "        assert False\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        #### TODO: sample a batch from the buffer\n",
        "        assert False\n",
        "\n",
        "    def get_all(self):\n",
        "        return deepcopy(self.buffer)\n",
        "\n",
        "    def clear(self):\n",
        "        self.buffer.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYgaycSQ7xKU"
      },
      "source": [
        "## Create a DQN agent\n",
        "\n",
        "Note: In the original DQN paper, the authors proposed to use a hard update on the target $Q$ network every $K$ steps. Later on, people find that using polyak averaging to soft update the target $Q$ network leads to more stable training. We will implement both these ideas in `DQNAgent`.  \n",
        "\n",
        "In the experiments in the following sections, we will use the soft update scheme to update our target $Q$ network. The soft update has the following formula:\n",
        "$$\\theta_{Q_{tgt}} = \\tau\\theta_{Q_{tgt}}+(1-\\tau)\\theta_Q $$\n",
        "\n",
        "where $\\tau$ is typically set to be very close to $1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXSuFJWR8EZv"
      },
      "source": [
        "@dataclass\n",
        "class DQNAgent:\n",
        "    env: gym.Env\n",
        "    learning_rate: float\n",
        "    gamma: float\n",
        "    memory_size: int\n",
        "    initial_epsilon: float\n",
        "    min_epsilon: float\n",
        "    max_epsilon_decay_steps: int\n",
        "    warmup_steps: int\n",
        "    batch_size: int\n",
        "    target_update_freq: int\n",
        "    enable_double_q: bool = False\n",
        "    disable_target_net: bool = False\n",
        "    device: str = None\n",
        "    tau: float = 0.995\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        if self.device is None:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        # self.qnet.to(self.device)\n",
        "        # self.target_qnet.to(self.device)\n",
        "        #### TODO: create a Deep Q network instance, \n",
        "        #### a replay buffer with capacity=self.memory_size\n",
        "        #### an Adam optimizer with lr=self.learning_rate.\n",
        "        #### make sure the networks are on the correct device.\n",
        "        #### Note that we are using a Huber loss as the loss function (already defined for you)\n",
        "        self.qnet = None\n",
        "        self.target_qnet = None\n",
        "        self.memory = None\n",
        "        self.optim = None\n",
        "        ####\n",
        "        self.loss_criterion = nn.HuberLoss()\n",
        "        self.epsilon = self.initial_epsilon\n",
        "        self.ep_reduction = (self.epsilon - self.min_epsilon) / float(self.max_epsilon_decay_steps)\n",
        "        if self.disable_target_net:\n",
        "            #### TODO: set self.target_update_freq such that the \n",
        "            #### target Q network will always be same as the Q network\n",
        "            #### You don't need to fill in this value until Q4.2\n",
        "        \n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def get_action(self, ob, greedy_only=False):\n",
        "        ob = torch.from_numpy(np.expand_dims(ob, axis=0)).float().to(self.device)\n",
        "        q_val = self.qnet(ob)\n",
        "        action = self.epsilon_greedy_policy(q_val, greedy_only=greedy_only)\n",
        "        return action\n",
        "\n",
        "    def epsilon_greedy_policy(self, q_values, greedy_only=False):\n",
        "        #### TODO: epsilon greedy exploration\n",
        "        #### we have an extra flag `greedy_only` here, \n",
        "        #### if greedy_only is True, then we need to return the action that \n",
        "        #### has the maximum Q values.\n",
        "        #### if greedy_only is False, we do epsilon greedy.\n",
        "        assert False\n",
        "\n",
        "    def add_to_memory(self, ob, next_ob, action, reward, done):\n",
        "        #### TODO: add data to the replay buffer\n",
        "        assert False\n",
        "    \n",
        "    def update_Q(self):\n",
        "        # we only start updating the Q network if there are enough samples in the replay buffer\n",
        "        if len(self.memory) < self.warmup_steps:\n",
        "            return 0\n",
        "\n",
        "        #### TODO: sample a batch of data from the replay buffer, and put\n",
        "        #### them on the correct device. you need to make sure the variables\n",
        "        #### are in the right tensor shape. (e.g., some scalars might need to be\n",
        "        #### of shape (batch size, 1) instead of (batch size))\n",
        "\n",
        "        #### TODO: update Q function with Bellman backup\n",
        "        ##### get Q(s_t, a_t)\n",
        "        \n",
        "        ##### TODO: get maxQ(s_{t+1}, a_{t+1})\n",
        "        ##### you will need to implement both DQN and double DQN here\n",
        "        ##### i.e., you need to check `if self.enable_double_q` in 4.4\n",
        "        #####\n",
        "        \n",
        "        ##### TODO: get the target Q value from the bellman equation\n",
        "\n",
        "        ##### TODO: update the Q network (i.e,. calculate the loss, take a gradient step)\n",
        "\n",
        "        ######\n",
        "\n",
        "        return loss.item()\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        #### TODO: linearly decay epsilon\n",
        "        #### reduce epsilon value by ep_reduction every time the function is called,\n",
        "        #### make sure epsilon is not smaller than self.min_epsilon\n",
        "        assert False\n",
        "    \n",
        "    def update_target_qnet(self, step, soft=True):\n",
        "        if not soft:\n",
        "            if step % self.target_update_freq == 0:\n",
        "                #### TODO: update the target Q function in a \"hard\" way\n",
        "                #### copy the parameter values in self.qnet into self.target_qnet\n",
        "                assert False\n",
        "        else:\n",
        "            #### TODO: soft update on taget Q network.\n",
        "            #### similar to polyak averaging, we update the target Q network slowly\n",
        "            #### $\\theta_Qtgt = \\tau*\\theta_Qtgt + (1-\\tau)*\\theta_Q\n",
        "            assert False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfM0YnBNCOIw"
      },
      "source": [
        "**Q3.1 (40 pts)**: Fill in the missing code in `DQNAgent` (you don't need to write the code for double DQN yet).\n",
        "\n",
        "**Q3.2 (5 pts)**: What is Huber loss? And what's the advantage of using Huber loss compared to MSE loss in updating the $Q$ function?\n",
        "\n",
        "**A**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZY-P0KF72iT"
      },
      "source": [
        "## Utils for running experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAh55V7VQPlM"
      },
      "source": [
        "# you don't need to modify the following code.\n",
        "@dataclass\n",
        "class DQNEngine:\n",
        "    env: gym.Env\n",
        "    agent: Any\n",
        "    max_steps: int\n",
        "    show_progress: bool = False\n",
        "    show_video: bool = False\n",
        "\n",
        "    def test(self, env=None, render=False):\n",
        "        env = self.env if env is None else env\n",
        "        ob = env.reset()\n",
        "        ret = 0\n",
        "        while True:\n",
        "            if render:\n",
        "                env.render()\n",
        "            action = self.agent.get_action(ob, greedy_only=True)\n",
        "            next_ob, reward, done, info = env.step(action)\n",
        "            ret += reward\n",
        "            ob = next_ob\n",
        "            if done:\n",
        "                break\n",
        "        return ret\n",
        "    \n",
        "    def run(self, n_runs=1):\n",
        "        rewards = []\n",
        "        log = []\n",
        "\n",
        "        for i in tqdm(range(n_runs), desc='Runs'):\n",
        "            ep_rewards = []\n",
        "            ep_steps = []\n",
        "            self.agent.reset()\n",
        "            # we plot the smoothed return values\n",
        "            smooth_ep_return = deque(maxlen=10)\n",
        "            ob = self.env.reset()\n",
        "            ret = 0\n",
        "            num_ep = 0\n",
        "            for t in tqdm(range(self.max_steps), desc='Step'):\n",
        "                if len(self.agent.memory) < self.agent.warmup_steps:\n",
        "                    action = self.env.action_space.sample()\n",
        "                else:\n",
        "                    action = self.agent.get_action(ob)\n",
        "                next_ob, reward, done, info = self.env.step(action)\n",
        "                true_done = done and not info.get('TimeLimit.truncated', False)\n",
        "                self.agent.add_to_memory(ob, next_ob, action, reward, true_done)\n",
        "                self.agent.update_Q()\n",
        "                ret += reward\n",
        "                ob = next_ob\n",
        "                if done:\n",
        "                    ob = self.env.reset()\n",
        "                    smooth_ep_return.append(ret)\n",
        "                    ep_rewards.append(np.mean(smooth_ep_return))\n",
        "                    ep_steps.append(t)\n",
        "                    ret = 0\n",
        "                    num_ep += 1\n",
        "                    if self.show_progress:\n",
        "                        print(f'Step:{t}  epsilon:{self.agent.epsilon}  '\n",
        "                            f'Smoothed Training Return:{np.mean(smooth_ep_return)}')\n",
        "                    if num_ep % 10 == 0:\n",
        "                        test_ret = self.test()\n",
        "                        if self.show_progress:\n",
        "                            print('==========================')\n",
        "                            print(f'Step:{t} Testing Return: {test_ret}')\n",
        "                self.agent.decay_epsilon()\n",
        "                self.agent.update_target_qnet(t, soft=not self.agent.disable_target_net)\n",
        "\n",
        "            rewards.append(ep_rewards)\n",
        "            run_log = pd.DataFrame({'return': ep_rewards,  \n",
        "                                    'steps': ep_steps,\n",
        "                                    'episode': np.arange(len(ep_rewards)), \n",
        "                                    'epsilon': self.agent.initial_epsilon})\n",
        "            log.append(run_log)\n",
        "        return log\n",
        "\n",
        "\n",
        "\n",
        "def dqn_sweep(agents, labels, n_runs=1, max_steps=1000000, show_progress=False):\n",
        "    logs = dict()\n",
        "    for idx, agent in enumerate(tqdm(agents)):\n",
        "        engine = DQNEngine(env=agent.env, agent=agent, \n",
        "                           max_steps=max_steps, show_progress=show_progress)\n",
        "        ep_log = engine.run(n_runs)\n",
        "        ep_log = pd.concat(ep_log, ignore_index=True)\n",
        "        ep_log['Agent'] = labels[idx]\n",
        "        logs[f'{idx}'] = ep_log\n",
        "    logs = pd.concat(logs, ignore_index=True)\n",
        "    return logs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8BzgBT98GNj"
      },
      "source": [
        "## Run experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjlyQjUsFH--"
      },
      "source": [
        "We have provided for you default hyperparameters to use in `get_default_config` to train your DQN agent. Within 100,000 steps your vanilla DQN should be getting reasonable performance and solving the task (smoothed return for the last 10,000 consecutive episodes is above 0.8). Use the same set of hyperparameters throughout the assignment unless we tell you to modify the config.\n",
        "\n",
        "You can use `show_progress = True` while testing your code for a faster feedback cycle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KeCN1oOFEoG"
      },
      "source": [
        "\n",
        "def get_default_config():\n",
        "    env = DoorKeyEnv5x5()\n",
        "    env = ImgObsWrapper(env)\n",
        "    set_random_seed(0)\n",
        "    config = dict(\n",
        "        env=env,\n",
        "        learning_rate=0.00025,\n",
        "        gamma=0.99,\n",
        "        memory_size=200000,\n",
        "        initial_epsilon=1.0,\n",
        "        min_epsilon=0.1,\n",
        "        max_epsilon_decay_steps=150000,\n",
        "        warmup_steps=500,\n",
        "        target_update_freq=2000,\n",
        "        batch_size=32,\n",
        "        device=None,\n",
        "        disable_target_net=False,\n",
        "        enable_double_q=False\n",
        "    )\n",
        "    return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tY6TcehHwlk"
      },
      "source": [
        "**Q4.1 (5 pts)**: Train your DQN agent and plot its return curve. We have provided you with a `plot` function where the `x_keys` should be `steps`, and `y_keys` should be `return`. Note that `return` corresponds to the return from an episode and that our environment only provides a reward of 1 when the goal is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVQCbVdswWKF"
      },
      "source": [
        "config = get_default_config()\n",
        "agent = DQNAgent(**config)\n",
        "dqn_logs = dqn_sweep([agent], ['dqn'], max_steps=100000, show_progress=False)\n",
        "\n",
        "#### TODO: plot the return curves\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1UE7q-qJQOR"
      },
      "source": [
        "Do we really need a target network? Let's see what if we don't use a target network.\n",
        "\n",
        "**Q4.2 (10 pts)**: Fill in the missing code in `reset` in `DQNAgent` for the case where we don't use a target Q network (i.e., when `disable_target_net = True`). Plot the return curve for this agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcCgC7LnxB6e"
      },
      "source": [
        "config = get_default_config()\n",
        "config['disable_target_net'] = True\n",
        "agent = DQNAgent(**config)\n",
        "no_tgt_logs = dqn_sweep([agent], ['no_tgt'], max_steps=100000, show_progress=False)\n",
        "\n",
        "#### TODO: plot the return curves\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLwb0ViOKgVx"
      },
      "source": [
        "Do we really need a replay buffer? Let's see what happens if we do not use a replay buffer. Since we are doing batch update on `Q` function, we can try using a very small replay buffer instead. We can make the size of the replay buffer same as the batch size.\n",
        "\n",
        "**Q4.3 (10 pts)**: Set the size of the replay buffer and `warmup_steps` to be the same as the batch size in the config. Run the training for this case and plot the return curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dh5dLfU4FLY"
      },
      "source": [
        "config = get_default_config()\n",
        "#### TODO: change some keys in config properly\n",
        "\n",
        "####\n",
        "agent = DQNAgent(**config)\n",
        "small_buffer_logs = dqn_sweep([agent], ['small_buffer'], max_steps=100000, show_progress=False)\n",
        "#### TODO: plot the return curves\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ux2zOdMIXCI"
      },
      "source": [
        "**Q4.4 (10 pts)**: Fill in the missing code for double DQN in `update_Q` in `DQNAgent` (when `enable_double_q=True`, your code should work for both cases). Train the double DQN agent, and plot the return curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU83g6GhxBcq"
      },
      "source": [
        "config = get_default_config()\n",
        "config['enable_double_q'] = True\n",
        "agent = DQNAgent(**config)\n",
        "doubleq_logs = dqn_sweep([agent], ['double_dqn'], max_steps=100000, show_progress=False)\n",
        "#### TODO: plot the return curves\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqGo5J2gNfST"
      },
      "source": [
        "**Q4.5 (10 pts)**: Plot all the logs (DQN, double DQN, no target network, small replay buffer) in the same figure. Note any differences in success performance between the variants. \n",
        "\n",
        "**A**:\n",
        "\n",
        "**Q4.6 (15 pts)**: Explain why each of our ablations (removing the target network, using a small buffer, and using double DQN) are more or less sucessfull than the vanilla DQN agent.\n",
        "\n",
        "**A**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLSDB7a84WFa"
      },
      "source": [
        "#### TODO: plot all the logs together\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NryAM1QVO26"
      },
      "source": [
        "# Survey (bonus, 10 pts)\n",
        "Please fill out [this anonymous survey](https://docs.google.com/forms/d/e/1FAIpQLSczytsld7GBnGokPzIrSH6zsbmhsOa6ivYYH-01qOianAtRWA/viewform?usp=sf_link) and enter the code below to receive credit. Thanks!\n",
        "\n",
        "**Bonus code**: "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission\n",
        "Run the below cell to generate an html file with your notebook. You can also follow the piazza instructions for PDF generation"
      ],
      "metadata": {
        "id": "GrNs2c8Y2YLR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3T2aE9bRwyh"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "!jupyter nbconvert --to html '/content/drive/My Drive/path_to_notebook.ipynb'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}