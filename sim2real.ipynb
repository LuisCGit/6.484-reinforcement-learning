{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisCGit/6.484-reinforcement-learning/blob/main/sim2real.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR66MOXFg_P"
      },
      "source": [
        "# Spring 2022 6.484 Computational Sensorimotor Learning Assignment 5\n",
        "\n",
        "In this assignment, we will tackle the sim2real gap in deep reinforcement learning. Since we don't have access to a real-world system, we will mimic sim-to-real transfer via sim-to-sim transfer (where one simulator is a replacement for the physical world).\n",
        "\n",
        "There are 290 total points on this problem set.\n",
        "\n",
        "**_Please fill in your name below:_**\n",
        "\n",
        "**Name**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AZMNnT3GPJ5"
      },
      "source": [
        "## Setup\n",
        "\n",
        "The following code sets up requirements, imports, and helper functions (you can ignore this)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptnCdKRYxo02"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install git+https://github.com/taochenshh/easyrl.git@sac > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZA8OkuBx5KN"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "from easyrl.agents.ppo_agent import PPOAgent\n",
        "from easyrl.configs import cfg\n",
        "from easyrl.configs import set_config\n",
        "from easyrl.configs.command_line import cfg_from_cmd\n",
        "from easyrl.engine.ppo_engine import PPOEngine\n",
        "from easyrl.models.categorical_policy import CategoricalPolicy\n",
        "from easyrl.models.diag_gaussian_policy import DiagGaussianPolicy\n",
        "from easyrl.models.mlp import MLP\n",
        "from easyrl.models.value_net import ValueNet\n",
        "from easyrl.runner.nstep_runner import EpisodicRunner\n",
        "from easyrl.utils.common import set_random_seed\n",
        "from easyrl.utils.gym_util import make_vec_env\n",
        "from easyrl.utils.common import load_from_json\n",
        "from gym.envs.classic_control.acrobot import AcrobotEnv\n",
        "from gym.envs.registration import registry, register\n",
        "from IPython import display as ipythondisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5pWMC9hx6VF"
      },
      "outputs": [],
      "source": [
        "if 'setup_display' in locals():\n",
        "    raise RuntimeError(\"Don't run this cell twice!\")\n",
        "\n",
        "setup_display = True\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWq4UpXWx7Vb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "stolen from https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=8nj5sjsk15IT\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env\n",
        "\n",
        "def read_tf_log(log_dir, scalar='train/episode_return/mean'):\n",
        "    log_dir = Path(log_dir)\n",
        "    log_files = list(log_dir.glob(f'**/events.*'))\n",
        "    if len(log_files) < 1:\n",
        "        return None\n",
        "    log_file = log_files[0]\n",
        "\n",
        "    event_acc = EventAccumulator(log_file.as_posix())\n",
        "    event_acc.Reload()\n",
        "    tags = event_acc.Tags()\n",
        "\n",
        "    scalar_return = event_acc.Scalars(scalar)\n",
        "    returns = [x.value for x in scalar_return]\n",
        "    steps = [x.step for x in scalar_return]\n",
        "\n",
        "    return steps, returns\n",
        "\n",
        "class AcrobotTargetEnv(AcrobotEnv):\n",
        "    book_or_nips = \"nips\"\n",
        "\n",
        "\n",
        "env_name = 'AcrobotTargetEnv-v0'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{__name__}:AcrobotTargetEnv',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlctfcIvGxuX"
      },
      "source": [
        "# Experiment Running\n",
        "\n",
        "We've provided the below code as-is to run your experiments using PPO.  Please do not modify this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2qx-TwFyy5T"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS\n",
        "def train_ppo(env_name='Acrobot-v1', max_steps=100000):    \n",
        "    set_config('ppo')\n",
        "    cfg.alg.num_envs = 1\n",
        "    cfg.alg.episode_steps = 1024\n",
        "    cfg.alg.log_interval = 1\n",
        "    cfg.alg.eval_interval = 20\n",
        "    \n",
        "    cfg.alg.max_steps = max_steps\n",
        "    cfg.alg.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    cfg.alg.env_name = env_name\n",
        "    cfg.alg.save_dir = Path.cwd().absolute().joinpath('data').as_posix()\n",
        "    cfg.alg.save_dir += '/' + env_name\n",
        "\n",
        "    setattr(cfg.alg, 'diff_cfg', dict(save_dir=cfg.alg.save_dir))\n",
        "\n",
        "    print(f'====================================')\n",
        "    print(f'      Device:{cfg.alg.device}')\n",
        "    print(f'      Total number of steps:{cfg.alg.max_steps}')\n",
        "    print(f'====================================')\n",
        "\n",
        "    set_random_seed(cfg.alg.seed)\n",
        "\n",
        "    env = make_vec_env(cfg.alg.env_name,\n",
        "                       cfg.alg.num_envs,\n",
        "                       seed=cfg.alg.seed)\n",
        "    env.reset()\n",
        "    ob_size = env.observation_space.shape[0]\n",
        "\n",
        "    actor_body = MLP(input_size=ob_size,\n",
        "                     hidden_sizes=[64, 64],\n",
        "                     output_size=64,\n",
        "                     hidden_act=nn.Tanh,\n",
        "                     output_act=nn.Tanh)\n",
        "\n",
        "    critic_body = MLP(input_size=ob_size,\n",
        "                     hidden_sizes=[64, 64],\n",
        "                     output_size=64,\n",
        "                     hidden_act=nn.Tanh,\n",
        "                     output_act=nn.Tanh)\n",
        "    \n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        act_size = env.action_space.n\n",
        "        actor = CategoricalPolicy(actor_body,\n",
        "                                 in_features=64,\n",
        "                                 action_dim=act_size)\n",
        "    elif isinstance(env.action_space, gym.spaces.Box):\n",
        "        act_size = env.action_space.shape[0]\n",
        "        actor = DiagGaussianPolicy(actor_body,\n",
        "                                   in_features=64,\n",
        "                                   action_dim=act_size,\n",
        "                                   tanh_on_dist=cfg.alg.tanh_on_dist,\n",
        "                                   std_cond_in=cfg.alg.std_cond_in)\n",
        "    else:\n",
        "        raise TypeError(f'Unknown action space type: {env.action_space}')\n",
        "\n",
        "    critic = ValueNet(critic_body, in_features=64)\n",
        "    agent = PPOAgent(actor=actor, critic=critic, env=env)\n",
        "    runner = EpisodicRunner(agent=agent, env=env)\n",
        "    engine = PPOEngine(agent=agent,\n",
        "                       runner=runner)\n",
        "    engine.train()\n",
        "\n",
        "    return agent, cfg.alg.save_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddqbjTGaHEah"
      },
      "source": [
        "# Acrobot Introduction\n",
        "\n",
        "Ben Bitdiddle just started his graduate program studying the *Acrobot*: a double pendulum commonly used as a benchmark in continuous control.  The goal of the benchmark is to find a policy that can swing the tip of the pendulum above the plane defined by the first joint (see the figure below), while only exerting torques on the second joint.  The sooner you hit the termination plane, the higher the reward.\n",
        "\n",
        "Lab time on the Acrobot is highly contested: Ben can only book 20 minute slots of physical time with the device. Thankfully, OpenAI gym provides a simulation environment for the Acrobot with the exact same physical parameters!\n",
        "\n",
        "Run the below cell to train a PPO policy in this simulation environment, and report whether Ben Bitdiddle should expect to be able to use the same method to train a policy on the real Acrobot.\n",
        "\n",
        "![](https://www.researchgate.net/publication/332779048/figure/fig8/AS:753590863937537@1556681471025/Acrobot-environment.ppm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igNYlqTNzSzh"
      },
      "outputs": [],
      "source": [
        "agent, save_dir = train_ppo(env_name=\"Acrobot-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry8pk56D62H2"
      },
      "outputs": [],
      "source": [
        "steps, returns = read_tf_log(save_dir)\n",
        "plt.plot(steps, returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-wup9HqLTcI"
      },
      "outputs": [],
      "source": [
        "# Displays a video of the policy.\n",
        "# Feel free to use this as a template for debugging.\n",
        "\n",
        "env = wrap_env(gym.make('Acrobot-v1'))\n",
        "\n",
        "num_steps = []\n",
        "for _ in range(10):\n",
        "    observation = env.reset()\n",
        "    step = 0\n",
        "    for i in range(1024):    \n",
        "        action = agent.get_action(observation)[0].tolist()\n",
        "        observation, reward, done, info = env.step(action)    \n",
        "        if done:\n",
        "            step = i\n",
        "            break\n",
        "    num_steps.append(step)\n",
        "\n",
        "env.close()\n",
        "\n",
        "print('Num steps:', num_steps)\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JydJYRtqJuun"
      },
      "source": [
        "**Question (10 pts):** Will Ben Bitdittle have enough time to train a policy on the real robot using the same PPO implementation as above? Note that each simulation step is equivalent to 0.2 seconds of time on the real robot.\n",
        "\n",
        "**Answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsihixQ1Kb9l"
      },
      "source": [
        "# The Sim2Real Gap\n",
        "\n",
        "Ben is devastated by the above result; how will he train a policy for the Acrobot if it won't converge in his allocated lab time?  \n",
        "\n",
        "One idea is to simply train a policy in simulation, then evaluate that policy on the real robot (which takes far less time than training from scratch).  Let's try this out, using the environment `AcrobotTargetEnv-v0` as a stand-in for the real world."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPGpKByoyLih"
      },
      "outputs": [],
      "source": [
        "### TODO: evaluate the agent from the simulation environment (\"Acrobot-v1\") in\n",
        "### the real world environment (\"AcrobotTargetEnv-v0\"). Be sure to run at least 10 trials\n",
        "### Report mean and standard deviation. (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8V27--aLto7"
      },
      "source": [
        "**Question (10 pts):** How does the policy trained in simulation perform when evaluated on the \"real\" robot?  If there's a difference in performance, postulate why that might be.\n",
        "\n",
        "**Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqFbqFqcNDlE"
      },
      "source": [
        "# System Identification\n",
        "\n",
        "One reason for the gap Ben observed is a mismatch between physical parameters in the simulation and in the real world.  If you look at the [code for the Acrobot simulation](https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py), you'll notice a series of parameters on lines 138-149 that define the simulator dynamics.  Perhaps a measurement error was made for one or more of these values, leading to a simulation that does not reflect reality.  One family of solutions for fixing these sorts of issues is *system identification*, which provides us tools for finding the correct values of these parameters from data. \n",
        "\n",
        "In this section, you will use a gradient-free numerical optimizer of choice to improve upon the parameters in the Acrobot simulation, with the goal of most closely matching the real world `AcrobotTargetEnv-v0` environment.\n",
        "\n",
        "Start by generating data of tuples (`env.state`, action, obs) from the real world environment, `AcrobotTargetEnv-v0`, over which to perform your system identification. Later, we'll want the best robot parameters such that when we apply the recorded action when the environment state is the recorded `env.state`, we get as close to the recorded obs as possible.\n",
        "\n",
        "You can generate as much data as you'd like as long as you don't use more than 20 minutes of robot time (e.g., $\\frac{20\\text{ minutes}}{0.2\\text{ seconds/timestep}} =6000\\text{ timesteps}$ )."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chxmu_KBPUsu"
      },
      "outputs": [],
      "source": [
        "q_shape = 4\n",
        "u_shape = 1\n",
        "obs_shape = 6\n",
        "dataset = {\n",
        "    \"env_state\": np.zeros((6000,q_shape)),\n",
        "    \"action\": np.zeros((6000,u_shape)),\n",
        "    \"new_obs\": np.zeros((6000,obs_shape)),\n",
        "}\n",
        "\n",
        "### TODO: generate your data for system ID (10 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_nKxSPDWrgE"
      },
      "source": [
        "Now, try to find parameters that yield new observations that best match your dataset.  There are a number of optimization methods to do this, but for our purposes let's do a simple random search of values in the neighborhood of those that we already have. Specifically, let's randomly and individually scale these parameters from 90% to 110% of their default values and see what combination yields the best performance.\n",
        "\n",
        "To evaluate your randomly sampled parameters, load the parameters into your `AcrobatSystemIDEnv` and see if for the same environment state and action, how much the next observations differ in terms of L2-norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIVuqN-0QxLk"
      },
      "outputs": [],
      "source": [
        "class AcrobotSystemIDEnv(AcrobotEnv):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        for param in ['dt', 'LINK_LENGTH_1', 'LINK_LENGTH_2', 'LINK_MASS_1', \n",
        "                      'LINK_MASS_2', 'LINK_COM_POS_1', 'LINK_COM_POS_2', \n",
        "                      'LINK_MOI', 'MAX_VEL_1', 'MAX_VEL_2']:\n",
        "            if param in kwargs:\n",
        "                setattr(self, param, kwargs[param])\n",
        "                del kwargs[param]\n",
        "\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "best_params = {\n",
        "    'dt' : 0,\n",
        "    'LINK_LENGTH_1' : 0,\n",
        "    'LINK_LENGTH_2' : 0,\n",
        "    'LINK_MASS_1' : 0,\n",
        "    'LINK_MASS_2' : 0,\n",
        "    'LINK_COM_POS_1' : 0,\n",
        "    'LINK_COM_POS_2' : 0,\n",
        "    'LINK_MOI' : 0,\n",
        "    'MAX_VEL_1' : 0,\n",
        "    'MAX_VEL_2' : 0,    \n",
        "}\n",
        "\n",
        "### TODO: find parameters of AcrobotSystemIDEnv that match the data in `dataset`,\n",
        "### and populate them into `best_params` (30 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNW1yYwyVm2i"
      },
      "source": [
        "Awesome!  Let's now try to train a policy in a simulation environment with those params loaded, `AcrobotSystemIDSolvedEnv`, and see if we do any better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGlDEVt2VFJu"
      },
      "outputs": [],
      "source": [
        "class AcrobotSystemIDSolvedEnv(AcrobotEnv):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        for param in best_params:\n",
        "            print('Setting', param, 'to', best_params[param])\n",
        "            setattr(self, param, best_params[param])\n",
        "\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "env_name = 'AcrobotSystemIDSolvedEnv-v0'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{__name__}:AcrobotSystemIDSolvedEnv',\n",
        ")\n",
        "\n",
        "sysid_agent, sysid_save_dir = train_ppo(env_name='AcrobotSystemIDSolvedEnv-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adcgdryusph8"
      },
      "outputs": [],
      "source": [
        "sysid_steps, sysid_returns = read_tf_log(sysid_save_dir)\n",
        "plt.plot(sysid_steps, sysid_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDG9bsMlWQXW"
      },
      "outputs": [],
      "source": [
        "### TODO: evaluate the agent from the sysid simulation environment \n",
        "### (\"AcrobotSystemIDSolvedEnv-v0\") in the real world environment\n",
        "### (\"AcrobotTargetEnv-v0\"). Be sure to run at least 10 trials.\n",
        "### Report mean and standard deviation. (10 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCbqafVCXNfT"
      },
      "source": [
        "**Question (10 pts):** How does the sysid agent perform when transferred on the \"real\" robot in comparison to our baseline agent trained in the \"sim\" (`Acrobot-v1`)?\n",
        "\n",
        "**Answer:** \n",
        "\n",
        "**Question (10 pts):** How does the sysid agent perform when evaluated in its training env, `AcrobotSystemIDSolvedEnv-v0`. You may find plotting the returns during training to be helpful using `read_tf_log`? \n",
        "\n",
        "**Answer:** \n",
        "\n",
        "**Question (10 pts):** Do you still observe a sim2real gap?  If so, why might that be, and how could it be further minimized? \n",
        "\n",
        "**Answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnHlR3QxME1B"
      },
      "source": [
        "# Domain Randomization\n",
        "\n",
        "Fortunately, Ben's professor has another solution to improve the quality of transfer between simulation and reality: *domain randomization*.  A good summary of the concept can be found in the [original paper](https://arxiv.org/abs/1710.06537)'s abstract:\n",
        "\n",
        "> Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this \"reality gap\". By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system.\n",
        "\n",
        "Let's now implement dynamics randomization for Acrobot. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2z9JFBdDR5i"
      },
      "outputs": [],
      "source": [
        "class AcrobotDREnv(AcrobotEnv):\n",
        "    def reset(self):\n",
        "        obs = super().reset()\n",
        "\n",
        "        ### TODO: randomize the dynamics parameters from the defaults slightly\n",
        "        ### similar to the sysid section (invidually and randomly scale 80% to 120%) (30 pts)\n",
        "\n",
        "        ### ENDTODO\n",
        "\n",
        "        return obs\n",
        "\n",
        "env_name = 'AcrobotDREnv-v0'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{__name__}:AcrobotDREnv',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot_4MYCV4s8u"
      },
      "outputs": [],
      "source": [
        "dr_agent, dr_save_dir = train_ppo(env_name='AcrobotDREnv-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYXzH_7Msph-"
      },
      "outputs": [],
      "source": [
        "dr_steps, dr_returns = read_tf_log(dr_save_dir)\n",
        "plt.plot(dr_steps, dr_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9whnEtC_aKik"
      },
      "outputs": [],
      "source": [
        "### TODO: evaluate the agent from the DR simulation environment \n",
        "### (\"AcrobotDREnv-v0\") in the real world environment\n",
        "### (\"AcrobotTargetEnv-v0\").  Make sure to run at least 10 trials.\n",
        "### Report mean and standard deviation. (10 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRvqbOi1ajx-"
      },
      "source": [
        "**Question (10 pts):** How does the DR agent perform when evaluated in its training env, `AcrobotDREnv-v0`. You may find plotting the returns during training to be helpful using `read_tf_log`?\n",
        "\n",
        "**Answer:** \n",
        "\n",
        "**Question (10 pts):**:  How does the DR agent perform in comparison to the sysid agent when transferred to our real world environment? Why is this so?\n",
        "\n",
        "**Answer**: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI7eLLlTrHcY"
      },
      "source": [
        "## Robustness vs Performance Tradeoff\n",
        "\n",
        "Next, try making your domain randomization less varied by narrowing the distributions of parameters you sample from. Perhaps we introduced too much randomization last round to train a more robust policy in exchange for some potential performance. Instead of scaling the parameters by 80% to 110%, only scale them from 95% to 105%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2zl9vrRrfkK"
      },
      "outputs": [],
      "source": [
        "class AcrobotDRCompareEnv(AcrobotEnv):\n",
        "    def reset(self):\n",
        "        obs = super().reset()\n",
        "\n",
        "        ### TODO: randomize the dynamics parameters (10 pts)\n",
        "\n",
        "        ### ENDTODO\n",
        "\n",
        "        return obs\n",
        "\n",
        "env_name = 'AcrobotDRCompareEnv-v0'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{__name__}:AcrobotDRCompareEnv',\n",
        ")\n",
        "\n",
        "dr_compare_agent, dr_compare_save_dir = train_ppo(env_name='AcrobotDRCompareEnv-v0')\n",
        "dr_compare_steps, dr_compare_returns = read_tf_log(dr_compare_save_dir)\n",
        "plt.plot(dr_compare_steps, dr_compare_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8KDRY6tr_p5"
      },
      "outputs": [],
      "source": [
        "### TODO: evaluate the dr_compare_agent in the real world environment\n",
        "### (\"AcrobotTargetEnv-v0\").  Make sure to run at least 10 trials.\n",
        "### Report mean and standard deviation. (10 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyK6vjIqsVsN"
      },
      "source": [
        "**Question (10 pts):** Does narrowing the range of domain randomization help with transfer to reality? Why?\n",
        "\n",
        "**Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHWNqcwSspiF"
      },
      "source": [
        "# Knowledge Distillation \n",
        "\n",
        "Unfortunately, the velocity sensors on the lab's Acrobat broke and are no longer usable. Thus, all of Ben's agents trained thus far that account for velocity in the state space can no longer function.\n",
        "\n",
        "Fortunately, Ben's professor has yet another idea to allow for transfer between simulation and reality: *knowledge distillation*. A summary of this idea can be found [here](https://intellabs.github.io/distiller/knowledge_distillation.html):\n",
        "\n",
        "> Knowledge distillation is model compression method in which a small model is trained to mimic a pre-trained, larger model (or ensemble of models). This training setting is sometimes referred to as \"teacher-student\", where the large model is the teacher and the small model is the student (we'll be using these terms interchangeably).\n",
        "\n",
        "This scenario is common when in simulation, we can have privileged information that may not be possible in reality (more sensors, perfect state estimation, etc.).\n",
        "\n",
        "In our case, we could train a teacher network in simulation with full access to a simulated robot with working velocity sensors. We would then train our student network with a more limited state space to predict actions that match that of the teacher. Let's try to implement this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZcyZPdHspiG"
      },
      "outputs": [],
      "source": [
        "# defining our environments\n",
        "class NoVelocityAcrobotEnv(AcrobotEnv):\n",
        "    book_or_nips = \"book\"    \n",
        "    \n",
        "    def __init__(self):\n",
        "        AcrobotEnv.__init__(self)\n",
        "        high = np.array(\n",
        "            [1.0, 1.0, 1.0, 1.0], dtype=np.float32\n",
        "        )\n",
        "        low = -high\n",
        "        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n",
        "        \n",
        "    def _get_ob(self):\n",
        "        s = self.state\n",
        "        assert s is not None, \"Call reset before using AcrobotEnv object.\"\n",
        "\n",
        "        obs = np.array(\n",
        "            [\n",
        "                np.cos(s[0]), np.sin(s[0]), np.cos(s[1]), np.sin(s[1]), # no velocity\n",
        "            ], dtype=np.float32\n",
        "        )\n",
        "                    \n",
        "        return obs\n",
        "\n",
        "class NoVelocityAcrobotTargetEnv(NoVelocityAcrobotEnv):\n",
        "    book_or_nips = \"nips\"    \n",
        "    \n",
        "        \n",
        "env_name = 'NoVelocityAcrobot-v0'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{__name__}:NoVelocityAcrobotEnv',\n",
        ")\n",
        "\n",
        "env_name = 'NoVelocityAcrobotTarget-v0'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{__name__}:NoVelocityAcrobotTargetEnv',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhZlocC8spiG"
      },
      "outputs": [],
      "source": [
        "# get our vanilla agent trained on the full simulated basic env\n",
        "# if you have this cached from Q1 no need to re-train, just run `teacher_agent = agent`\n",
        "# in practice, we could use one of our better agents (e.g., the agent trained on domain randomization\n",
        "# or sysid) but let's use the basic agent for simplicity\n",
        "\n",
        "teacher_agent, teacher_save_dir = train_ppo(env_name=\"Acrobot-v1\")\n",
        "teacher_steps, teacher_returns = read_tf_log(teacher_save_dir)\n",
        "plt.plot(teacher_steps, teacher_returns)\n",
        "\n",
        "# teacher_agent = agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VI85BBTspiG"
      },
      "source": [
        "Now that we have our teacher, let's generate a dataset of observations and policy outputs that we'll try to get our student to later match. Note that solely training on executed *actions* is insufficient, so we'll need to store the parameters of our `Categorical` distribution (the logits representing each of the 3 discrete actions) for every given observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA1Jpj4BspiG"
      },
      "outputs": [],
      "source": [
        "num_transitions = 20000\n",
        "\n",
        "stud_dataset = {\n",
        "    'obs': np.zeros((num_transitions, 6)),\n",
        "    'dist_params': np.zeros((num_transitions, 3)),   \n",
        "}\n",
        "\n",
        "class StudentObsActionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, stud_dataset):\n",
        "        self.states = stud_dataset['obs']\n",
        "        self.params = stud_dataset['dist_params']\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.states.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = dict()\n",
        "        sample['state'] = self.states[idx][:4] # no velocity information\n",
        "        sample['dist_params'] = self.params[idx]\n",
        "        return sample\n",
        "\n",
        "\n",
        "### TODO: Collect offline dataset of teacher observations and policy outputs\n",
        "###       from our simulated environment, `Acrobot-v1` which has access to the velocity (30 pts)\n",
        "'''\n",
        "note: to get an action out of our `PPOAgent` object, you can pass an observation as a \n",
        "tensor to the teacher_agent's actor to get a `Categorical` distribution\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEpK6eU3spiG"
      },
      "source": [
        "Awesome, now that we have our offline dataset, we just need our student policy (which will take in observations without velocities) to output the same distribution over discrete actions! Note that our `StudentObsActionDataset` wrapper is removing velocities for you already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DmRuMj1spiH"
      },
      "outputs": [],
      "source": [
        "# model \n",
        "student_body = MLP(\n",
        "    input_size=4, # no more angular velocity\n",
        "    hidden_sizes=[64, 64],\n",
        "    output_size=64,\n",
        "    hidden_act=nn.Tanh,\n",
        "    output_act=nn.Tanh\n",
        ")\n",
        "act_size = env.action_space.n\n",
        "student = CategoricalPolicy(\n",
        "    student_body,\n",
        "    in_features=64,\n",
        "    action_dim=act_size\n",
        ").to(cfg.alg.device)\n",
        "\n",
        "# setup\n",
        "optimizer = torch.optim.Adam(student.parameters(), lr=0.0005)\n",
        "max_epochs = 50\n",
        "dataloader = torch.utils.data.DataLoader(dset, batch_size=256, shuffle=True)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# train loop\n",
        "pbar = tqdm(range(max_epochs), desc='Epoch')\n",
        "losses = []\n",
        "for iter in pbar:\n",
        "    avg_loss = []\n",
        "    for batch_idx, sample in enumerate(dataloader):\n",
        "        states = sample['state'].float().to(cfg.alg.device)\n",
        "        expert_logits = sample['dist_params'].float().to(cfg.alg.device)\n",
        "\n",
        "        ### TODO: optimize the student outputs with respect to the data (10 pts)\n",
        "\n",
        "        ####\n",
        "        \n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "        losses.append(loss.item())\n",
        "\n",
        "plt.plot(losses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ96V-Y9spiH"
      },
      "source": [
        "Let's evaluate the performance of our trained student in a simulated environment with no velocity in the state before we try evaluating it on our real robot with broken velocity sensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dr769ZTspiH"
      },
      "outputs": [],
      "source": [
        "student.eval()\n",
        "\n",
        "### TODO: Evaluate the student on `NoVelocityAcrobot-v0`. Be sure to run at least 10 trials\n",
        "### Report mean and standard deviation. (10 pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfxWvstvspiH"
      },
      "outputs": [],
      "source": [
        "student.eval()\n",
        "\n",
        "### TODO: Evaluate the student on `NoVelocityAcrobotTarget-v0`. Be sure to run at least 10 trials.\n",
        "### Report mean and standard deviation. (10 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKJr8rCispiH"
      },
      "source": [
        "**Question (10 pts):**:  How does the student agent perform in the simulated environment without velocity (`NoVelocityAcrobot-v0`) in comparison to the teacher on the same environment with velocity measurements (`Acrobot-v1`)? \n",
        "\n",
        "**Answer**: \n",
        "\n",
        "**Question (10 pts):**:  How does the student agent perform in the target environment without velocity (`NoVelocityAcrobotTarget-v0`) in comparison to the other agents we've trained so far? Why is this so?\n",
        "\n",
        "**Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67SxHujFbDFA"
      },
      "source": [
        "# Results Comparison\n",
        "\n",
        "Make a single plot comparing the results of transferring an agent using our three methods to reality: the naive simulation (`Acrobot-v1`), system identification (`AcrobotSystemIDSolvedEnv`), and domain randomization (`AcrobotDREnv-v0` and `AcrobotDRCompareEnv-v0`). We won't include our knowledge distillation method as the student uses a different state space which makes it a different comparison.\n",
        "\n",
        "As always, be sure to communicate both the mean and variance of your results. Be creative in how you present these results; what is the best way of illustrating the relative strengths and weaknesses of these methods, and making the plot clearly communicate the experiments that were run?\n",
        "\n",
        "(30 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqdzcGzcbhS7"
      },
      "outputs": [],
      "source": [
        "### todo: make pretty plots.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLcgjC1HcNFe"
      },
      "source": [
        "# Survey (bonus points, 10 pts)\n",
        "Please fill out [this anonymous survey](https://docs.google.com/forms/d/e/1FAIpQLSeG6LstBLYq61NeFfhXja6Lu9-roPpEiIFP8gkJ-5fNygwrKw/viewform?usp=sf_link) and enter the code below to receive credit. Thanks!\n",
        "\n",
        "**Bonus code:** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyChpYKvspiI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission\n",
        "Generate an HTML for submission by running the cell below, ensuring that your plots/code/figures show up nicely and modifying the notebook path as needed to match your Google Drive set up. Alternatively you can run the `jupyer nbconvert` commands on your local machine after downloading this notebook as an `ipynb`."
      ],
      "metadata": {
        "id": "yGpo3i1r6rBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "!jupyter nbconvert --to html '/content/drive/My Drive/path_to_notebook.ipynb'"
      ],
      "metadata": {
        "id": "E-eI8oQG68mf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "sim2real.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}