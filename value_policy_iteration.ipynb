{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisCGit/6.484-reinforcement-learning/blob/main/value_policy_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrvVF3c0_9WM"
      },
      "source": [
        "# Spring 2022 6.484 Computational Sensorimotor Learning Assignment 7\n",
        "\n",
        "In this assignment, we will implement three principle reinforcement learning algorithms which provably converge to optimal solutions for MDPs:\n",
        "\n",
        "*   Value iteration\n",
        "*   Policy iteration\n",
        "*   Q-learning\n",
        "\n",
        "You will need to answer the bolded questions and fill in the missing code snippets (marked by **TODO**).\n",
        "\n",
        "There are **230** total points to be had in this PSET.  `ctrl-f` for \"pts\" to ensure you don't miss questions.\n",
        "\n",
        "**_Please fill in your name below:_**\n",
        "\n",
        "**Name**: Luis Costa Laveron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIwE_4jt9JgR"
      },
      "source": [
        "### Credits\n",
        "\n",
        "Some part of the code of this assignment is borrowed from the Spring 2018 CMU Deep Reinforcement Learning & Control course. We also thank Prof. [Cathy Wu](https://idss.mit.edu/staff/cathy-wu/) for polishing the content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilgy6DEx_z65"
      },
      "source": [
        "## Setup\n",
        "\n",
        "The following code sets up imports and helper functions (you can ignore this)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx58PD3jc40l"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import gym\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from copy import deepcopy\n",
        "from tqdm.notebook import tqdm\n",
        "from dataclasses import dataclass\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from typing import Any\n",
        "from collections import deque\n",
        "\n",
        "mpl.rcParams['figure.dpi']= 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgD4VlhkPeYS"
      },
      "outputs": [],
      "source": [
        "# some util functions\n",
        "def plot(logs, x_key, y_key, legend_key, **kwargs):\n",
        "    nums = len(logs[legend_key].unique())\n",
        "    palette = sns.color_palette(\"hls\", nums)\n",
        "    if 'palette' not in kwargs:\n",
        "        kwargs['palette'] = palette\n",
        "    sns.lineplot(x=x_key, y=y_key, data=logs, hue=legend_key, **kwargs)\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "# set random seed\n",
        "seed = 0\n",
        "set_random_seed(seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi-_JUbdTad7"
      },
      "source": [
        "# FrozenLake environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPZpix6-C8XN"
      },
      "source": [
        "*Winter has come.* You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
        "\n",
        "The surface is described using a grid like the following:\n",
        "\n",
        "```\n",
        "SFFF # (S: starting point, safe)\n",
        "FHFH # (F: frozen surface, safe)\n",
        "FFFH # (H: hole, fall to your doom)\n",
        "HFFG # (G: goal, where the frisbee is located)\n",
        "```\n",
        "\n",
        "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
        "\n",
        "Here's what the Frozen Lake looks like in action, when following a random agent:\n",
        "\n",
        "![](https://miro.medium.com/max/690/1*ur_42c7MLhbi6q2L3JtSqg.gif)\n",
        "\n",
        "Frozen Lake is part of OpenAI gym, a collection of open-source environments for benchmarking RL algorithms.   [Here](https://gym.openai.com/envs/FrozenLake-v0/) is a link to the gym environment (also the source of our environment description)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrX2DmlLsAkM"
      },
      "source": [
        "**Question:** What actions can the agent take at any given time step? (5 pts)\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PprRCSgFsrZI"
      },
      "source": [
        "Now, here's some code that creates the above environment through OpenAI gym, called `FrozenLake-v0`.  Note that we will be using a stochastic variant, so providing an action in a given direction will not always move you in that direction! When slippery, there is a 1/3 chance you move in the intended direction and a 1/3 chance each you move in a perpendicular direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23UivOJriyde"
      },
      "outputs": [],
      "source": [
        "## create FrozenLake environment\n",
        "MAPS = {\n",
        "    \"4x4\": [\n",
        "        \"GHFS\",\n",
        "        \"FHHF\",\n",
        "        \"FFHF\",\n",
        "        \"FFFF\"\n",
        "    ],\n",
        "    \"8x8\": [\n",
        "        \"FFFSFFFF\",\n",
        "        \"FFFFFFFF\",\n",
        "        \"HHHHFHFF\",\n",
        "        \"FFFFFFHF\",\n",
        "        \"FFFFFFFF\",\n",
        "        \"FHFFFFHF\",\n",
        "        \"FHFFHFHH\",\n",
        "        \"FGHFFFFF\"\n",
        "    ],\n",
        "}\n",
        "from gym.envs.registration import register\n",
        "env_name = 'Stochastic-4x4-FrozenLake-v0'\n",
        "if env_name not in gym.envs.registry.env_specs:\n",
        "    register(\n",
        "        id=env_name,\n",
        "        entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "        kwargs={'map_name': None,\n",
        "                'is_slippery': True,\n",
        "                'desc': MAPS['4x4']\n",
        "                },\n",
        "        max_episode_steps=20)\n",
        "\n",
        "env_name = 'Stochastic-8x8-FrozenLake-v0'\n",
        "if env_name not in gym.envs.registry.env_specs:\n",
        "    register(\n",
        "        id=env_name,\n",
        "        entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "        kwargs={'map_name': None,\n",
        "                'is_slippery': True,\n",
        "                'desc': MAPS['8x8'],\n",
        "                },\n",
        "        max_episode_steps=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCBb9JESEMqN"
      },
      "source": [
        "We would like to find a good policy for the agent (you, the brave soul). More precisely, the agent controls the movement of a character on the frozen lake. The frozen lake environment is an example of a _grid world_, since it consists of objects moving around in a discrete (grid) world. Grid world problems can constitute or approximate a wide class of problems.\n",
        "\n",
        "**Question:** Consider a racecar environment, where the goal is to get the agent (the racecar) around a race track as quickly as possible. Is this suitable for representing as a grid world problem?  Justify your answer. (5 pts)\n",
        "\n",
        "**Answer:** \n",
        "\n",
        "Now let's return to the task at hand: retrieving our frisbee!\n",
        "Remember that some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
        "\n",
        "Now let's consider some algorithms for solving this problem, i.e. finding a good policy to accomplish the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdJjohytByCv"
      },
      "source": [
        "# Value Iteration\n",
        "\n",
        "Value iteration is the first algorithm we will consider.\n",
        "\n",
        "Let's first do a sanity check. In class, we learned that value iteration is model-based and as such, it is best for problems with small state spaces.\n",
        "\n",
        "**Question:** Consider a fixed 4x4-grid FrozenLake. What is the size of the state space? (5 pts)\n",
        "\n",
        "**Answer:** \n",
        "\n",
        "The state space is 4x4=16. Not bad! The reason that we don't have an exponential state space (e.g. $5^{16}$, where $5 = |\\{S, F, H, G, A\\}|$, and $A$ denotes the location of the agent) is because much of the grid world is fixed. The only part of the state that can change during the course of an episode is the location of the agent. The agent can appear in one of 4x4 locations, and thus the state space is 16. That is, although $5^{16}$ does provide a \"first cut\" at an upper bound on the state space, considering how much of the state can actually change can help dramatically to tighten the bound. On the other hand, if the task were to solve _any_ 4x4 FrozenLake, the state space would be much larger. Fortunately, you have a specific lake to cross to retrieve your frisbee.\n",
        "\n",
        "Now recall from class that value iteration is a model-based method which starts with any guess of a value function $V_0$ and then updates it according to the optimal Bellman equation: $V_{i+1}(s) = \\mathcal{T} V_i(s) = \\max_a \\mathbb{E}_{s' \\sim p(\\cdot | s, a)}[r(s, a, s') + \\gamma V_i(s')]$.  In our case, as we have a fixed number of states, our value function is simply a mapping of square -> value (eg an array).\n",
        "\n",
        "*Note:* we choose to represent actions as integers with the following mapping:\n",
        "\n",
        "```\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "```\n",
        "\n",
        "and states as zero-indexed integers traversing from the top left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGcN5-56TdxS"
      },
      "source": [
        "## Implement Value Iteration\n",
        "\n",
        "*(20 pts)*\n",
        "\n",
        "We will now implement value iteration over the MDP with transition probabilities described by `env.P` (transition probabilites), `env.nS` (number of states), and `env.nA` (number of actions). The entry `env.P[s][a]` (where `s` is the state index and `a` is the action index) is a list of transition tuples $(p(s,a,s'), s', r(s, a, s'), \\text{episode_end})$ for each list index index `s'`.  In plain english:\n",
        "\n",
        " - $p(s,a,s')$: the probability of transitioning to state `s'` after taking action `a` in state `s`.\n",
        " - $s'$: the next state under this transition.\n",
        " - $r(s, a, s')$: the reward of taking this transition.\n",
        " - `episode_end`: a boolean representing whether taking this action ends the episode (in Frozen Lake, whether this kills you or takes you to the goal position).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjdKyhti0I-k"
      },
      "outputs": [],
      "source": [
        "# for example\n",
        "env = gym.make('Stochastic-8x8-FrozenLake-v0')\n",
        "env.P[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OknrYASeB9fC"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, gamma, max_iterations=1000, tol=1e-3):\n",
        "    \"\"\"Runs value iteration for a given gamma and environment.\n",
        "    Updates states in their 1-N order.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      The environment to compute value iteration for. Must have nS,\n",
        "      nA, and P as attributes.\n",
        "    gamma: float\n",
        "      Discount factor, must be in range [0, 1)\n",
        "    max_iterations: int\n",
        "      The maximum number of iterations to run before stopping.\n",
        "    tol: float\n",
        "      Determines when value function has converged.\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray, iteration, list\n",
        "      The value function, the number of iterations it took to converge, and a list\n",
        "      of the value functions after each iteration.\n",
        "    \"\"\"\n",
        "    value_func = np.zeros(env.nS) # initial value function: all states are zero\n",
        "    iters = 0\n",
        "    value_history = []\n",
        "    while True:\n",
        "        delta = 0 # largest change in value function at any state between two iterations\n",
        "        ###### TODO: value function update ##############\n",
        "\n",
        "        ################################################################\n",
        "        # let's also save a copy of value function after each iteration\n",
        "        value_history.append(value_func.copy())\n",
        "        iters += 1\n",
        "        if delta < tol or iters >= max_iterations:\n",
        "            break\n",
        "    return value_func, iters, value_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se-exLs_mNrQ"
      },
      "source": [
        "Actually, computing the optimal value function is not enough. What we are interested in is the optimal policy, not just how good each state is. Luckily, the optimal value function and the optimal policy are related. Let's implement this next:\n",
        "\n",
        "*(20 pts)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMV--rNqmMpJ"
      },
      "outputs": [],
      "source": [
        "def value_function_to_policy(env, gamma, value_function):\n",
        "    \"\"\"Output action numbers for each state in value_function.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      Environment to compute policy for. Must have nS, nA, and P as\n",
        "      attributes.\n",
        "    gamma: float\n",
        "      Discount factor. Number in range [0, 1)\n",
        "    value_function: np.ndarray\n",
        "      Value of each state.\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "      An array of integers. Each integer is the optimal action to take\n",
        "      in that state according to the environment dynamics and the\n",
        "      given value function.\n",
        "    \"\"\"\n",
        "    policy = np.zeros(env.nS, dtype='int')\n",
        "    for idx in range(env.nS):\n",
        "        p = env.P[idx]\n",
        "        vmax = -np.inf\n",
        "        best_act = -1\n",
        "        ###### TODO: Select the best action (best_act) ##############\n",
        "\n",
        "        ###################################################\n",
        "        policy[idx] = best_act\n",
        "    return policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGVvR2gwmuwg"
      },
      "source": [
        "**Question**: What is the difference between the value iteration algorithm and extracting a policy from a value function? (5 pts)\n",
        "\n",
        "**Answer**: \n",
        "\n",
        "OK enough talk, let's run it. We'll consider this 8x8 frozen lake:\n",
        "\n",
        "```\n",
        "FFFSFFFF\n",
        "FFFFFFFF\n",
        "HHHHFHFF\n",
        "FFFFFFHF\n",
        "FFFFFFFF\n",
        "FHFFFFHF\n",
        "FHFFHFHH\n",
        "FGHFFFFF\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ros3SnX6Th04"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Here are some helper functions for visualizing your value and policy functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmMVbUDb0Bs7"
      },
      "outputs": [],
      "source": [
        "def print_policy(policy):\n",
        "    act_dict = {0: 'L', 1: 'D', 2: 'R', 3: 'U'}\n",
        "    row = col = int(np.sqrt(policy.size))\n",
        "    for i in range(row):\n",
        "        for j in range(col):\n",
        "            print(act_dict[policy[i * col + j]], end='')\n",
        "        print('')\n",
        "\n",
        "def print_value(value):\n",
        "    row = col = int(np.sqrt(value.size))\n",
        "    for i in range(row):\n",
        "        for j in range(col):\n",
        "            print('{0:.6f}'.format(value[i * col + j]), end=' ')\n",
        "        print('')\n",
        "\n",
        "def plot_value_history(value_history):\n",
        "    row = col = int(np.sqrt(value_history[0].size))\n",
        "    images = [x.reshape(row, col) for x in value_history]\n",
        "    \n",
        "    num_images = len(images)\n",
        "    cols = int(np.sqrt(num_images))\n",
        "    rows = int(np.ceil(num_images / cols))\n",
        "    fig = plt.figure(figsize=(5, 12))\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(int(len(images) / cols) + 1, cols, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.xlabel(f'Iter: {i}')\n",
        "    fig.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieKlBNIa0I-p"
      },
      "source": [
        "## Run value iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jR2agkksCFOr"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Stochastic-8x8-FrozenLake-v0')\n",
        "gamma = 0.9\n",
        "value_info = value_iteration(env, gamma, max_iterations=int(1e3), tol=1e-4)\n",
        "value, iters, value_history = value_info\n",
        "policy = value_function_to_policy(env, gamma, value)\n",
        "print('-----Policy: ')\n",
        "print_policy(policy)\n",
        "print('-----Value: ')\n",
        "print_value(value)\n",
        "print('-----Iterations: ', iters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42PVPUZB1dht"
      },
      "outputs": [],
      "source": [
        "plot_value_history(value_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2GBerlZQSJa"
      },
      "source": [
        "**Question**: Try running the above code with gamma = 0.3.  Does the agent converge to a successful policy (yes/no)? What is the role of gamma? *(5 pts)*:\n",
        "\n",
        "**Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9_vtY44_uRz"
      },
      "source": [
        "# Policy Iteration\n",
        "\n",
        "Recall from class that value iteration is a special instance of generalized policy iteration, which alternates between 1 step of policy evaluation and 1 step of policy improvement.\n",
        "\n",
        "Now we'll consider policy iteration, which alternates between many steps of policy evaluation and 1 step of policy improvement. Does that feel silly?\n",
        "\n",
        "We will make a few notes:\n",
        "- Let's consider N steps of policy evaluation + 1 step of policy improvement to be 1 iteration of value/policy iteration.\n",
        "- It is not well understood why, but policy iteration and value iteration will attain the optimal policy in fewer iterations for different problems.\n",
        "- While policy iteration has a hidden cost of N policy evaluation steps, it turns out that a full policy evaluation can be computed efficiently, since it is a linear operation. (We will not do this in this assignment, but trust us.) If employed efficiently, policy iteration can be viewed as a super-powered value iteration, with accurate policy evaluation and without a whole lot of extra computational cost.\n",
        "\n",
        "Now, let's implement policy evaluation and policy improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PYcHg3f_11N"
      },
      "source": [
        "## Implement Policy Iteration\n",
        "\n",
        "*(10 pts)*\n",
        "\n",
        "First, implement the the high level wrapper `policy_iteration`. Here, we iteratively evaluate and then improve  our policy, an array of size `env.nS` with the action to take in every state. `evaluate_policy` returns a value function that we can feed into `improve_policy`. We will implement these submethods momentarily, but you can check out their function signatures below. We'll continue iterating on our policy until `improve_policy` tells us that the policy is stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxF5TjsN9AR9"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
        "    \"\"\"Runs policy iteration using the improve_policy and evaluate_policy methods.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      The environment to compute value iteration for. Must have nS,\n",
        "      nA, and P as attributes.\n",
        "    gamma: float\n",
        "      Discount factor, must be in range [0, 1)\n",
        "    max_iterations: int\n",
        "      The maximum number of iterations to run before stopping.\n",
        "    tol: float\n",
        "      Determines when value function has converged.\n",
        "    Returns\n",
        "    -------\n",
        "    (np.ndarray, np.ndarray, int, int, list)\n",
        "       Returns optimal policy, value function, number of policy\n",
        "       improvement iterations, number of value iterations, and a list\n",
        "       of the history value functions.\n",
        "    \"\"\"\n",
        "    policy = np.zeros(env.nS, dtype='int')\n",
        "    value_history = [] # should contain the full history of value functions.\n",
        "    policy_imp_step = 0 # number of total policy improvement iterations\n",
        "    policy_eval_step = 0 # number of total value function iterations\n",
        "    while True:\n",
        "        ### TODO: Fill in policy iteration main loop. ##########\n",
        "\n",
        "        ##########################################################\n",
        "    return policy, value_func, policy_imp_step, policy_eval_step, value_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dSV7riP4gta"
      },
      "source": [
        "Next, implement the policy evaluation and update submethods.\n",
        "\n",
        "To evaluate a policy, we calculate the value at each state by taking the action specified by the policy instead of looping over all possible actions from that state and using the maximum value (as done previously).\n",
        "\n",
        "To improve the policy, you can extract the policy from the value function generated by evaluating the policy (similar to what we did previously).\n",
        "\n",
        "*(30 pts)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4loS9sw4g_-"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(env, gamma, policy, max_iterations=int(1e3), tol=1e-5):\n",
        "    \"\"\"Performs policy evaluation.\n",
        "    Evaluates the value of a given policy by asynchronous DP.  Updates states in\n",
        "    their 1-N order.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      The environment to compute value iteration for. Must have nS,\n",
        "      nA, and P as attributes.\n",
        "    gamma: float\n",
        "      Discount factor, must be in range [0, 1)\n",
        "    policy: np.array\n",
        "      The policy to evaluate. Maps states to actions.\n",
        "    max_iterations: int\n",
        "      The maximum number of iterations to run before stopping.\n",
        "    tol: float\n",
        "      Determines when value function has converged.\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray, int\n",
        "      The value function for the given policy and the number of iterations till\n",
        "      the value function converged.\n",
        "    \"\"\"\n",
        "    value_func = np.zeros(env.nS)\n",
        "    iter = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        ###### TODO: value function update (value_func) ##############\n",
        "\n",
        "        ###################################################\n",
        "        iter += 1\n",
        "        if delta < tol or iter >= max_iterations:\n",
        "            break\n",
        "    return value_func, iter\n",
        "\n",
        "def improve_policy(env, gamma, value_func, policy):\n",
        "    \"\"\"Performs policy improvement.\n",
        "    Given a policy and value function, improves the policy.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      The environment to compute value iteration for. Must have nS,\n",
        "      nA, and P as attributes.\n",
        "    gamma: float\n",
        "      Discount factor, must be in range [0, 1)\n",
        "    value_func: np.ndarray\n",
        "      Value function for the given policy.\n",
        "    policy: dict or np.array\n",
        "      The policy to improve. Maps states to actions.\n",
        "    Returns\n",
        "    -------\n",
        "    bool, np.ndarray\n",
        "      Returns true if policy changed. Also returns the new policy.\n",
        "    \"\"\"\n",
        "    policy_stable = True\n",
        "    new_policy = np.random.randint(0, 4, (env.nS)).astype(np.int8)\n",
        "    for idx in range(env.nS):\n",
        "        old_action = policy[idx]\n",
        "        p = env.P[idx]\n",
        "        new_action = -1\n",
        "        best_q = -np.inf\n",
        "        ###### TODO: use value function to get new action (new_action) ######\n",
        "\n",
        "        ###################################################\n",
        "        new_policy[idx] = new_action\n",
        "        if new_action != old_action:\n",
        "            policy_stable = False\n",
        "    return policy_stable, new_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUXznsss_9eM"
      },
      "source": [
        "## Run Policy Iteration\n",
        "\n",
        "Assuming your above implementation is correct, you should be able to run the below code to evaluate policy iteration on Frozen Lake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7zBqnPNUEaC"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Stochastic-8x8-FrozenLake-v0')\n",
        "gamma = 0.9\n",
        "policy_info = policy_iteration(env, gamma, max_iterations=int(1e3), tol=1e-3)\n",
        "new_policy, value_func, policy_imp_step, policy_eval_step, value_history = policy_info\n",
        "policy = value_function_to_policy(env, gamma, value)\n",
        "print('New policy: ')\n",
        "print_policy(new_policy)\n",
        "print('Value: ')\n",
        "print_value(value_func)\n",
        "print('Number of policy improvement steps: ', policy_imp_step)\n",
        "print('Total number of policy evaluation steps: ', policy_eval_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO7_e9-VUZKK"
      },
      "outputs": [],
      "source": [
        "plot_value_history(value_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk2seQKESHNv"
      },
      "source": [
        "Note that both value and policy iteration can solve the `Frozen Lake` environment (if one can't, you've done something wrong). \n",
        "\n",
        "**Questions** *(20 pts)*:\n",
        "- How many iterations (i.e. policy improvement steps) were required by policy iteration? \n",
        "  - **Answer:**\n",
        "\n",
        "- How many policy improvement steps were required by value iteration? \n",
        "  - **Answer:**\n",
        "\n",
        "- If one method took longer to converge, postulate an explanation for this.\n",
        "   - **Answer**: \n",
        "  \n",
        "\n",
        "\n",
        "One common benchmark for reinforcment learning algorithms is *sample complexity*: the number of interactions the agent must have with the environment to learn a policy.  In policy iteration, we can approximate this as the number of \"actions\" for which policy improvement is run.\n",
        "\n",
        "**Question**:  Compute the sample complexity to solve 8x8 FrozenLake.  What number do you get? (10 pts)\n",
        "\n",
        "**Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p2a-jhc8MuS"
      },
      "source": [
        "# Q-learning\n",
        "\n",
        "In the above two algorithms, we had access to the entire MDP: all of the states, with all of the transition probabilities between them.  Unfortunately, this is usually not the case.\n",
        "\n",
        "Below we will implement Q-learning, which is *model free*: it does not require full knowledge of environment dynamics, and instead will try to learn a policy purely through exploration and exploitation.\n",
        "\n",
        "For pedantic purposes*, you have obtained some microspikes for your frozen lake and the environment is no longer slippery, hurrah! In other words, whatever action you decide to take you will actually take instead of randomly going perpendicularly quite often.\n",
        "\n",
        "Fill in the missing functions in the `QLearningAgent` below.\n",
        "\n",
        "*(50 pts)*\n",
        "\n",
        "$_\\text{*with a slippery environment and epsilon random exploration as we'll implement below, there's so much randomness that we don't see anything interesting from tabular rasa Q-learning in reasonable time frames. in practice, many real world environments don't have as much randomness as frozen lake (where 66\\% of the time what you act doesn't actually happen}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At0mlrcD0I-r"
      },
      "outputs": [],
      "source": [
        "env_name = 'Deterministic-8x8-FrozenLake-v0'\n",
        "if env_name not in gym.envs.registry.env_specs:\n",
        "    register(\n",
        "        id=env_name,\n",
        "        entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "        kwargs={'map_name': None,\n",
        "                'is_slippery': False,\n",
        "                'desc': MAPS['8x8'],\n",
        "                },\n",
        "        max_episode_steps=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXjEtjih8MWS"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class QLearningAgent:\n",
        "    env: gym.Env\n",
        "    learning_rate: float\n",
        "    gamma: float\n",
        "    initial_epsilon: float\n",
        "    min_epsilon: float\n",
        "    max_decay_episodes: int\n",
        "    init_q_value: float = 0.\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.num_states = env.nS\n",
        "        self.reset()\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        ### TODO: decay epsilon by ep_reduction while respecting min_epsilon ################\n",
        "        ###.      this function is called every episode\n",
        "\n",
        "        #####################################################################\n",
        "    \n",
        "    def reset(self):\n",
        "        self.epsilon = self.initial_epsilon\n",
        "        self.ep_reduction = (self.epsilon - self.min_epsilon) / float(self.max_decay_episodes)\n",
        "        self.Q = np.ones((self.num_states, self.env.nA)) * self.init_q_value\n",
        "\n",
        "    def update_Q(self, state, next_state, action, reward, done):\n",
        "        ### TODO: update self.Q given new experience. #######################\n",
        "\n",
        "        #####################################################################\n",
        "\n",
        "    def get_action(self, state):\n",
        "        ### TODO: select an action given self.Q and self.epsilon ############\n",
        "\n",
        "        #####################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upNSviFC99uV"
      },
      "source": [
        "The below code is scaffolding to instantiate and run the above Q-Learning agent.  Feel free to examine it to help you implement `QLearningAgent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxxQPNH99zVh"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class QLearningEngine:\n",
        "    env: gym.Env\n",
        "    agent: Any\n",
        "    max_episodes: int\n",
        "    \n",
        "    def run(self, n_runs=1):\n",
        "        rewards = []\n",
        "        log = []\n",
        "        for i in tqdm(range(n_runs), desc='Runs'):\n",
        "            ep_rewards = []\n",
        "            self.agent.reset()\n",
        "            # we plot the smoothed return values\n",
        "            smooth_ep_return = deque(maxlen=100)\n",
        "            for t in tqdm(range(self.max_episodes), desc='Episode'):\n",
        "                state = self.env.reset()\n",
        "                ret = 0\n",
        "                while True:\n",
        "                    action = self.agent.get_action(state)\n",
        "                    next_state, reward, done, info = self.env.step(action)\n",
        "                    true_done = done and not info.get('TimeLimit.truncated', False)\n",
        "                    self.agent.update_Q(state, next_state, action, reward, true_done)\n",
        "                    ret += reward\n",
        "                    state = next_state\n",
        "                    if done:\n",
        "                        break\n",
        "                self.agent.decay_epsilon()\n",
        "                smooth_ep_return.append(ret)\n",
        "                ep_rewards.append(np.mean(smooth_ep_return))\n",
        "            rewards.append(ep_rewards)\n",
        "            run_log = pd.DataFrame({'return': ep_rewards,  \n",
        "                                    'episode': np.arange(len(ep_rewards)), \n",
        "                                    'iqv': self.agent.init_q_value})\n",
        "            log.append(run_log)\n",
        "        return log\n",
        "\n",
        "def qlearning_sweep(init_q_values, n_runs=4, max_episodes=100000, epsilon=0.9, learning_rate=0.8):\n",
        "    logs = dict()\n",
        "    pbar = tqdm(init_q_values)\n",
        "    agents = []\n",
        "    for iqv in pbar:\n",
        "        pbar.set_description(f'Initial q value:{iqv}')\n",
        "        env=gym.make('Deterministic-8x8-FrozenLake-v0')\n",
        "        agent = QLearningAgent(env=env,\n",
        "                               learning_rate=learning_rate,\n",
        "                               gamma=0.99,\n",
        "                               initial_epsilon=epsilon,\n",
        "                               min_epsilon=0.0,\n",
        "                               max_decay_episodes=max_episodes,\n",
        "                               init_q_value=iqv)\n",
        "        engine = QLearningEngine(env=env, agent=agent, max_episodes=max_episodes)\n",
        "        ep_log = engine.run(n_runs)\n",
        "        ep_log = pd.concat(ep_log, ignore_index=True)\n",
        "        logs[f'{iqv}'] = ep_log\n",
        "\n",
        "        agents.append(agent)\n",
        "    logs = pd.concat(logs, ignore_index=True)\n",
        "    return logs, agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9jz-syH-Kze"
      },
      "source": [
        "Once the agent is implemented, run the below code to try it out on FrozenLake!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM6CAtd60I-t"
      },
      "outputs": [],
      "source": [
        "init_q_values = [0., 1.]\n",
        "eps_logs, eps_agents = qlearning_sweep(init_q_values, n_runs=3, max_episodes=60000, epsilon=0.9)\n",
        "plot(eps_logs, x_key='episode', y_key='return', legend_key='iqv', estimator='mean', ci='sd')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM_yPqh98UgW"
      },
      "source": [
        "### Questions\n",
        "\n",
        "Using the above parameters (`init_q_values = [0., 1.]`, `epsilon=0.9`):\n",
        "- Print the policy for both agents in the cell below. Do both converge? (5 pts)\n",
        "  - **Answer:**  \n",
        "- How does the performance compare between the initial Q values during the learning process? Why is there a difference if any? (10 pts)\n",
        "  - **Answer:**  \n",
        "  \n",
        "\n",
        "\n",
        "If you set `epsilon=0.0` for the same `init_q_values = [0., 1.]` and set `max_episodes=1000`:\n",
        "- How many steps does the policy take now? Why is the `iqv = 1.0` policy converging faster as compared to with a higher value of initial_epsilon? (10 pts)\n",
        "  - **Answer:** \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_W__j3b0I-t"
      },
      "outputs": [],
      "source": [
        "### TODO: print policy for agents with each of the init q_values. ########\n",
        "\n",
        "########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tiYCIqMA9Fi"
      },
      "source": [
        "## Q-Learning Sample Complexity\n",
        "\n",
        "Remember that we computed the *sample complexity* of Policy Iteration on 8x8 FrozenLake.\n",
        "\n",
        "**Question:** With Q-learning, our sample complexity requires more than 60,000 episodes with 100 steps (i.e,. actions taken by the agent and executed in the environment) per episode. Why is the sample complextiy so much higher than value and policy iteration? (20 pts)\n",
        "- **Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff_YuBHOwx5x"
      },
      "source": [
        "# Survey (bonus, 10 pts)\n",
        "Please fill out [this anonymous survey](https://docs.google.com/forms/d/e/1FAIpQLSc24Peg-H269fxBOarZ5MdNgsHFe2ZZ_t90Wd_KjO3jz2cuCQ/viewform?usp=sf_link) and enter the code below to receive credit. Thanks!\n",
        "\n",
        "**Bonus code**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpcdY5gH6IT6"
      },
      "source": [
        "# Submission\n",
        "Run the below cell to generate an html file with your notebook. You can also follow the piazza instructions for PDF generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQTGPyCy0I-v"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "!jupyter nbconvert --to html '/content/drive/My Drive/path_to_notebook.ipynb'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "value_policy_iteration.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}