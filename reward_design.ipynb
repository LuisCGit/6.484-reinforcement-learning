{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuisCGit/6.484-reinforcement-learning/blob/main/reward_design.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfYNRpcRQmrh"
      },
      "source": [
        "# Spring 2022 6.484 Computational Sensorimotor Learning Assignment 3\n",
        "\n",
        "In this assignment, we will go through the reward design process when you use reinforcement learning algorithms to solve a task. \n",
        "\n",
        "You will need to **answer the bolded questions** and **fill in the missing code snippets** (marked by **TODO**).\n",
        "\n",
        "There are **160** total points to be had in this PSET, plus 10 bonus points for filling out the survey.  `ctrl-f` for \"pts\" to ensure you don't miss questions.\n",
        "\n",
        "**_Please fill in your name below:_**\n",
        "\n",
        "**Name**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQhxNNedPbLh"
      },
      "outputs": [],
      "source": [
        "!pip install pybullet > /dev/null 2>&1\n",
        "!pip install git+https://github.com/Improbable-AI/airobot.git > /dev/null 2>&1\n",
        "!pip install git+https://github.com/taochenshh/easyrl.git@sac > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIpLQKJ8SUrp"
      },
      "outputs": [],
      "source": [
        "# You can enable the GPU by changing the runtime (Runtime -> Change runtime type -> Hardware accelerator -> GPU )\n",
        "import os\n",
        "import torch\n",
        "import gym\n",
        "import pprint\n",
        "import pybullet as p\n",
        "import pybullet_data as pd\n",
        "import airobot as ar\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from matplotlib import pylab\n",
        "from airobot import Robot\n",
        "from airobot.utils.common import quat2euler\n",
        "from airobot.utils.common import euler2quat\n",
        "from gym import spaces\n",
        "from gym.envs.registration import registry, register\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "from easyrl.agents.ppo_agent import PPOAgent\n",
        "from easyrl.configs import cfg\n",
        "from easyrl.configs import set_config\n",
        "from easyrl.configs.command_line import cfg_from_cmd\n",
        "from easyrl.engine.ppo_engine import PPOEngine\n",
        "from easyrl.models.categorical_policy import CategoricalPolicy\n",
        "from easyrl.models.diag_gaussian_policy import DiagGaussianPolicy\n",
        "from easyrl.models.mlp import MLP\n",
        "from easyrl.models.value_net import ValueNet\n",
        "from easyrl.runner.nstep_runner import EpisodicRunner\n",
        "from easyrl.utils.common import set_random_seed\n",
        "from easyrl.utils.gym_util import make_vec_env\n",
        "from easyrl.utils.common import load_from_json\n",
        "from base64 import b64encode\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNOq_A5XUQHt"
      },
      "outputs": [],
      "source": [
        "def play_video(video_dir, video_file=None):\n",
        "    if video_file is None:\n",
        "        video_dir = Path(video_dir)\n",
        "        video_files = list(video_dir.glob(f'**/render_video.mp4'))\n",
        "        video_files.sort()\n",
        "        video_file = video_files[-1]\n",
        "    else:\n",
        "        video_file = Path(video_file)\n",
        "    compressed_file = video_file.parent.joinpath('comp.mp4')\n",
        "    os.system(f\"ffmpeg -i {video_file} -filter:v 'setpts=2.0*PTS' -vcodec libx264 {compressed_file.as_posix()}\")\n",
        "    mp4 = open(compressed_file.as_posix(),'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url))\n",
        "\n",
        "\n",
        "# read tf log file\n",
        "def read_tf_log(log_dir):\n",
        "    log_dir = Path(log_dir)\n",
        "    log_files = list(log_dir.glob(f'**/events.*'))\n",
        "    if len(log_files) < 1:\n",
        "        return None\n",
        "    log_file = log_files[0]\n",
        "    event_acc = EventAccumulator(log_file.as_posix())\n",
        "    event_acc.Reload()\n",
        "    tags = event_acc.Tags()\n",
        "    try:\n",
        "        scalar_success = event_acc.Scalars('train/episode_success')\n",
        "        success_rate = [x.value for x in scalar_success]\n",
        "        steps = [x.step for x in scalar_success]\n",
        "        scalar_return = event_acc.Scalars('train/episode_return/mean')\n",
        "        returns = [x.value for x in scalar_return]\n",
        "    except:\n",
        "        return None\n",
        "    return steps, returns, success_rate\n",
        "\n",
        "\n",
        "def plot_curves(data_dict, title):\n",
        "    # {label: [x, y]}\n",
        "    fig, ax = plt.subplots(figsize=(4, 3))\n",
        "    labels = data_dict.keys()\n",
        "    for label, data in data_dict.items():\n",
        "        x = data[0]\n",
        "        y = data[1]\n",
        "        ax.plot(x, y, label=label)\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "\n",
        "def check_collision_rate(log_dir):\n",
        "    log_dir = Path(log_dir)\n",
        "    log_files = list(log_dir.glob(f'**/info.json'))\n",
        "    log_files.sort()\n",
        "    log_file = log_files[-1]\n",
        "    info_data = load_from_json(log_file)\n",
        "    collisions = [v['collision'] for k, v in info_data.items()]\n",
        "    return np.mean(collisions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqWQGeuETbA3"
      },
      "source": [
        "# Environment\n",
        "\n",
        "We will use [AIRobot](https://github.com/Improbable-AI/airobot/tree/master) for this assignment. The task here is to move the end-effector of a [UR robot](https://www.universal-robots.com/) from a fixed starting position (shown as the yellow ball in the figure below) to a fixed goal position (shown as the red ball in the figure below). The end-effector is constrained to move in $xy$-plane only (the horizontal plane). \n",
        "\n",
        "**State**: the 2D position of the end-effector tip, $(x, y)$ \n",
        "\n",
        "**Action**: 2D continuous action space, $[\\Delta x, \\Delta y]$. The end-effector can move in $x$ direction by $\\Delta x$, $y$ direction by $\\Delta y$. We scale $\\Delta x, \\Delta y$ so that they are in the range of $[-1, 1]$.\n",
        "\n",
        "\n",
        "The following figures visually show the robot environment:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/1_1.png\" height=\"300\"></iframe>\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/1_2.png\" height=\"300\"></iframe>\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/1_3.png\" height=\"300\"></iframe>"
      ],
      "metadata": {
        "id": "-lvGGB2iH4z9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_Y23ZMqeaDs"
      },
      "source": [
        "The robot environment is defined in the following class (`URRobotGym`). Your task is to design and fill in the reward functions. You don't need to change other parts of the code in this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMQ-QIc5Tknh"
      },
      "outputs": [],
      "source": [
        "class URRobotGym(gym.Env):\n",
        "    def __init__(self,\n",
        "                 action_repeat=10,\n",
        "                 use_sparse_reward=False,\n",
        "                 use_subgoal=False,\n",
        "                 with_obstacle=True,\n",
        "                 apply_collision_penalty=False,\n",
        "                 # Set 'gui' to False if you are using Colab, otherwise the session will crash as Colab does not support X window\n",
        "                 # You can set it to True for debugging purpose if you are running the notebook on a local machine.\n",
        "                 gui=False,\n",
        "                 max_episode_length=25,\n",
        "                 dist_threshold=0.05):\n",
        "        self._action_repeat = action_repeat\n",
        "        self._max_episode_length = max_episode_length\n",
        "        self._dist_threshold = dist_threshold\n",
        "        self._use_sparse_reward = use_sparse_reward\n",
        "        self._use_subgoal = use_subgoal\n",
        "        self._apply_collision_penalty = apply_collision_penalty\n",
        "        self._with_obstacle = with_obstacle\n",
        "        print(f'================================================')\n",
        "        print(f'Use sparse reward:{self._use_sparse_reward}')\n",
        "        print(f'Use subgoal:{self._use_subgoal}')\n",
        "        print(f'With obstacle in the scene:{self._with_obstacle}')\n",
        "        print(f'Apply collision penalty:{self._apply_collision_penalty}')\n",
        "        print(f'================================================')\n",
        "\n",
        "        self._xy_bounds = np.array([[0.23, 0.78],  # [xmin, xmax]\n",
        "                                    [-0.35, 0.3]])  # [ymin, ymax]\n",
        "        self.robot = Robot('ur5e_stick',\n",
        "                           pb_cfg={'gui': gui,\n",
        "                                   'realtime': False,\n",
        "                                   'opengl_render': torch.cuda.is_available()})\n",
        "        self._arm_reset_pos = np.array([-0.38337763,\n",
        "                                        -2.02650575,\n",
        "                                        -2.01989619,\n",
        "                                        -0.64477803,\n",
        "                                        1.571439041,\n",
        "                                        -0.38331266])\n",
        "        self._table_id = self.robot.pb_client.load_urdf('table/table.urdf',\n",
        "                                                        [.5, 0, 0.4],\n",
        "                                                        euler2quat([0, 0, np.pi / 2]),\n",
        "                                                        scaling=0.9)\n",
        "\n",
        "        # create a ball at the start location (for visualization purpose)\n",
        "        self._start_pos = np.array([0.45, -0.32, 1.0])\n",
        "        self._start_urdf_id = self.robot.pb_client.load_geom('sphere', size=0.04, mass=0,\n",
        "                                                             base_pos=self._start_pos,\n",
        "                                                             rgba=[1, 1, 0, 0.8])\n",
        "\n",
        "        # create a ball at the goal location\n",
        "        self._goal_pos = np.array([0.5, 0.26, 1.0])\n",
        "        self._goal_urdf_id = self.robot.pb_client.load_geom('sphere', size=0.04, mass=0,\n",
        "                                                            base_pos=self._goal_pos,\n",
        "                                                            rgba=[1, 0, 0, 0.8])\n",
        "\n",
        "        # disable the collision checking between the robot and the ball at the goal location\n",
        "        for i in range(self.robot.pb_client.getNumJoints(self.robot.arm.robot_id)):\n",
        "            self.robot.pb_client.setCollisionFilterPair(self.robot.arm.robot_id,\n",
        "                                                        self._goal_urdf_id,\n",
        "                                                        i,\n",
        "                                                        -1,\n",
        "                                                        enableCollision=0)\n",
        "        # disable the collision checking between the robot and the ball at the start location\n",
        "        for i in range(self.robot.pb_client.getNumJoints(self.robot.arm.robot_id)):\n",
        "            self.robot.pb_client.setCollisionFilterPair(self.robot.arm.robot_id,\n",
        "                                                        self._start_urdf_id,\n",
        "                                                        i,\n",
        "                                                        -1,\n",
        "                                                        enableCollision=0)\n",
        "\n",
        "        # create an obstacle\n",
        "        if self._with_obstacle:\n",
        "            self._wall_id = self.robot.pb_client.load_geom('box', size=[0.18, 0.01, 0.1], mass=0,\n",
        "                                                           base_pos=[0.5, 0.15, 1.0],\n",
        "                                                           rgba=[0.5, 0.5, 0.5, 0.8])\n",
        "\n",
        "        # create balls at subgoal locations\n",
        "        if self._use_subgoal:\n",
        "            self._subgoal_pos = np.array([[0.24, 0.15, 1.0], [0.76, 0.15, 1.0]])\n",
        "            self._subgoal_urdf_id = []\n",
        "            for pos in self._subgoal_pos:\n",
        "                self._subgoal_urdf_id.append(self.robot.pb_client.load_geom('sphere', size=0.04, mass=0,\n",
        "                                                                            base_pos=pos,\n",
        "                                                                            rgba=[0, 0.8, 0.8, 0.8]))\n",
        "            # disable the collision checking between the robot and the subgoal balls\n",
        "            for i in range(self.robot.pb_client.getNumJoints(self.robot.arm.robot_id)):\n",
        "                for sg in self._subgoal_urdf_id:\n",
        "                    self.robot.pb_client.setCollisionFilterPair(self.robot.arm.robot_id,\n",
        "                                                                sg,\n",
        "                                                                i,\n",
        "                                                                -1,\n",
        "                                                                enableCollision=0)\n",
        "\n",
        "        self._action_bound = 1.0\n",
        "        self._ee_pos_scale = 0.02\n",
        "        self._action_high = np.array([self._action_bound] * 2)\n",
        "        self.action_space = spaces.Box(low=-self._action_high,\n",
        "                                       high=self._action_high,\n",
        "                                       dtype=np.float32)\n",
        "        state_low = np.full(len(self._get_obs()), -float('inf'))\n",
        "        state_high = np.full(len(self._get_obs()), float('inf'))\n",
        "        self.observation_space = spaces.Box(state_low,\n",
        "                                            state_high,\n",
        "                                            dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.robot.arm.set_jpos(self._arm_reset_pos, ignore_physics=True)\n",
        "        self._t = 0\n",
        "        self._ref_ee_pos = self.robot.arm.get_ee_pose()[0]\n",
        "        self._ref_ee_ori = self.robot.arm.get_ee_pose()[1]\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "        collision = self._apply_action(action)\n",
        "        self._t += 1\n",
        "        state = self._get_obs()\n",
        "        done = self._t >= self._max_episode_length\n",
        "        reward, info = self._get_reward(state=state, action=action, collision=float(collision))\n",
        "        info['collision'] = collision\n",
        "        return state, reward, done, info\n",
        "\n",
        "    def _get_reward(self, state, action, collision):\n",
        "        dist_to_goal = np.linalg.norm(state - self._goal_pos[:2])\n",
        "        success = dist_to_goal < self._dist_threshold\n",
        "        if self._use_sparse_reward:\n",
        "            #### TODO: Q1 design a sparse reward\n",
        "            \n",
        "        elif self._use_subgoal:\n",
        "            reward = self._get_reward_with_subgoal(state)\n",
        "        else:\n",
        "            #### TODO: Q2 design a dense reward based on only the state and the goal position (no other information)\n",
        "\n",
        "        if self._apply_collision_penalty:\n",
        "            #### TODO: Q4 apply a collision penalty\n",
        "    \n",
        "\n",
        "        info = dict(success=success)\n",
        "        return reward, info\n",
        "\n",
        "    def _get_reward_with_subgoal(self, state):\n",
        "        #### TODO: Q5 design a reward based on the state, goal and subgoal positions\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def _get_obs(self):\n",
        "        gripper_pos = self.robot.arm.get_ee_pose()[0][:2]\n",
        "        state = gripper_pos\n",
        "        return state\n",
        "\n",
        "    def _check_collision_with_wall(self):\n",
        "        if hasattr(self, '_wall_id'):\n",
        "            return len(self.robot.pb_client.getContactPoints(self.robot.arm.robot_id, \n",
        "                                                             self._wall_id, 10, -1)) > 0\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def _apply_action(self, action):\n",
        "        jnt_poses = self.robot.arm.get_jpos()\n",
        "        if not isinstance(action, np.ndarray):\n",
        "            action = np.array(action).flatten()\n",
        "        if action.size != 2:\n",
        "            raise ValueError('Action should be [d_x, d_y].')\n",
        "        # we set dz=0\n",
        "        action = np.append(action, 0)\n",
        "        pos, quat, rot_mat, euler = self.robot.arm.get_ee_pose()\n",
        "        pos += action[:3] * self._ee_pos_scale\n",
        "        pos[2] = self._ref_ee_pos[2]\n",
        "        # if the new position is out of the bounds, then we don't apply the action\n",
        "        if not np.logical_and(np.all(pos[:2] >= self._xy_bounds[:, 0]),\n",
        "                              np.all(pos[:2] <= self._xy_bounds[:, 1])):\n",
        "            return False\n",
        "        \n",
        "        # move the end-effector to the new position\n",
        "        jnt_pos = self.robot.arm.compute_ik(pos, ori=self._ref_ee_ori)\n",
        "        for step in range(self._action_repeat):\n",
        "            self.robot.arm.set_jpos(jnt_pos)\n",
        "            self.robot.pb_client.stepSimulation()\n",
        "        \n",
        "        # if collision occurs, we reset the robot back to its original pose (before apply_action)\n",
        "        collision = False\n",
        "        if self._check_collision_with_wall():\n",
        "            self.robot.arm.set_jpos(jnt_poses, ignore_physics=True)\n",
        "            collision = True\n",
        "        return collision\n",
        "\n",
        "\n",
        "    def render(self, mode, **kwargs):\n",
        "        robot_base = self.robot.arm.robot_base_pos\n",
        "        self.robot.cam.setup_camera(focus_pt=robot_base,\n",
        "                                    dist=2,\n",
        "                                    yaw=85,\n",
        "                                    pitch=-20,\n",
        "                                    roll=0)\n",
        "        rgb, _ = self.robot.cam.get_images(get_rgb=True,\n",
        "                                           get_depth=False)\n",
        "        return rgb\n",
        "\n",
        "\n",
        "module_name = __name__\n",
        "\n",
        "env_name = 'URReacher-v1'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{module_name}:URRobotGym',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZJ8y2AGirJw"
      },
      "source": [
        "For this assignment, we will use PPO to train the policy. The training code is already complete. You don't need to modify any code here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NCd_lowruyc"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS\n",
        "def train_ppo(use_sparse_reward=False, use_subgoal=False, with_obstacle=False, apply_collision_penalty=False, push_exp=False,\n",
        "              max_steps=200000):\n",
        "    set_config('ppo')\n",
        "    cfg.alg.num_envs = 1\n",
        "    cfg.alg.episode_steps = 100\n",
        "    cfg.alg.max_steps = max_steps\n",
        "    cfg.alg.deque_size = 20\n",
        "    cfg.alg.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    cfg.alg.env_name = 'URPusher-v1' if push_exp else 'URReacher-v1'\n",
        "    cfg.alg.save_dir = Path.cwd().absolute().joinpath('data').as_posix()\n",
        "    cfg.alg.save_dir += '/'\n",
        "    if push_exp:\n",
        "        cfg.alg.save_dir += 'push'\n",
        "    else:\n",
        "        cfg.alg.save_dir += 'sparse' if use_sparse_reward else 'dense'\n",
        "        cfg.alg.save_dir += f'_ob_{str(with_obstacle)}'\n",
        "        cfg.alg.save_dir += f'_sg_{str(use_subgoal)}'\n",
        "        cfg.alg.save_dir += f'_col_{str(apply_collision_penalty)}'\n",
        "    setattr(cfg.alg, 'diff_cfg', dict(save_dir=cfg.alg.save_dir))\n",
        "\n",
        "    print(f'====================================')\n",
        "    print(f'      Device:{cfg.alg.device}')\n",
        "    print(f'      Total number of steps:{cfg.alg.max_steps}')\n",
        "    print(f'====================================')\n",
        "\n",
        "    set_random_seed(cfg.alg.seed)\n",
        "    env_kwargs=dict(use_sparse_reward=use_sparse_reward,\n",
        "                    with_obstacle=with_obstacle,\n",
        "                    use_subgoal=use_subgoal,\n",
        "                    apply_collision_penalty=apply_collision_penalty) if not push_exp else dict()\n",
        "    env = make_vec_env(cfg.alg.env_name,\n",
        "                       cfg.alg.num_envs,\n",
        "                       seed=cfg.alg.seed,\n",
        "                       env_kwargs=env_kwargs)\n",
        "    env.reset()\n",
        "    ob_size = env.observation_space.shape[0]\n",
        "\n",
        "    actor_body = MLP(input_size=ob_size,\n",
        "                     hidden_sizes=[64],\n",
        "                     output_size=64,\n",
        "                     hidden_act=nn.Tanh,\n",
        "                     output_act=nn.Tanh)\n",
        "\n",
        "    critic_body = MLP(input_size=ob_size,\n",
        "                     hidden_sizes=[64],\n",
        "                     output_size=64,\n",
        "                     hidden_act=nn.Tanh,\n",
        "                     output_act=nn.Tanh)\n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "        act_size = env.action_space.n\n",
        "        actor = CategoricalPolicy(actor_body,\n",
        "                                 in_features=64,\n",
        "                                 action_dim=act_size)\n",
        "    elif isinstance(env.action_space, gym.spaces.Box):\n",
        "        act_size = env.action_space.shape[0]\n",
        "        actor = DiagGaussianPolicy(actor_body,\n",
        "                                   in_features=64,\n",
        "                                   action_dim=act_size,\n",
        "                                   tanh_on_dist=cfg.alg.tanh_on_dist,\n",
        "                                   std_cond_in=cfg.alg.std_cond_in)\n",
        "    else:\n",
        "        raise TypeError(f'Unknown action space type: {env.action_space}')\n",
        "\n",
        "    critic = ValueNet(critic_body, in_features=64)\n",
        "    agent = PPOAgent(actor=actor, critic=critic, env=env)\n",
        "    runner = EpisodicRunner(agent=agent, env=env)\n",
        "    engine = PPOEngine(agent=agent,\n",
        "                       runner=runner)\n",
        "    engine.train()\n",
        "    stat_info, raw_traj_info = engine.eval(render=False,\n",
        "                                           save_eval_traj=True,\n",
        "                                           eval_num=1,\n",
        "                                           sleep_time=0.0)\n",
        "    pprint.pprint(stat_info)\n",
        "    return cfg.alg.save_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PES8AoqBmo3V"
      },
      "source": [
        "# Reaching Task without Obstacles\n",
        "\n",
        "The first task we are going to solve is a reaching task without obstacles. We want to learn a policy that can move the robot's end-effector from the starting position to the goal position. And there is no obstacle in the scene."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wDCCkD2hIvh"
      },
      "source": [
        "## Sparse Reward\n",
        "\n",
        "First, let's see if we can solve the reaching task by just using a sparse reward. The agent gets $+1$ reward if the end-effector tip is close to the goal position, $0$ reward otherwise.\n",
        "\n",
        "$$r(s_t, a_t) = \\begin{cases}1 \\quad \\text{if } \\left\\Vert s_t - s_g\\right\\Vert_2 < d_{thresh} \\\\ 0 \\quad \\text{otherwise}\\end{cases}$$\n",
        "where $s_g$ is the goal position, and $d_{thresh}$ is the distance threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU4-o-RhiNM5"
      },
      "source": [
        "**Q1 [20 pts]**: Fill in the code for sparse reward (`use_sparse_reward=True`), train the policy with sparse reward for 200000 steps, and plot the return curve and the sucess rate curve. Hint: you can use the utility functions: `read_tf_log` and `plot_curves`.\n",
        "\n",
        "For experiments in this assignment, if you can run them locally, then you will see that a folder `data` is created in your current working directory. And you can run `tensorboard --logdir=data` to track the experiment progress (check `train/episode_success` and `train/episode_return/mean`). If you are using colab, the `data` folder will also be created, and you can download them during the training to check the learning curves locally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FF1ROpJjGx4"
      },
      "outputs": [],
      "source": [
        "#### TODO\n",
        "# call train_ppo, just set the argument flag properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXSNrTR3lH7L"
      },
      "outputs": [],
      "source": [
        "#### TODO: plot return and success rate curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s06HUhzu6QAI"
      },
      "source": [
        "You can visually see what the robot is doing in the testing time by running `play_video(sparse_save_dir)`. You can use this function to check the trajectories of each policy you trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ihnxy1v6oop"
      },
      "outputs": [],
      "source": [
        "## assume the saving directory is `save_dir` (returned by `train_ppo`), then run:\n",
        "# play_video(save_dir)\n",
        "#### TODO: play the video (visually check what the robot ends up doing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTX21y4zlSZc"
      },
      "source": [
        "## Dense Reward\n",
        "\n",
        "As we can see from the previous section, it is easy to specify a sparse reward. However, it makes the training much harder and the agent may fail to reach the goal. Can we provide a richer learning signal to the agent? One possibility is to provide a dense reward based on the distance between the current end-effector position and the goal position.\n",
        "\n",
        "$$r(s_t, a_t) = -\\left\\Vert s_t - s_g\\right\\Vert_2$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvN7AYHBmGAy"
      },
      "source": [
        "**Q2 [20 pts]**: Fill in the code for dense reward (`use_sparse_reward=False`), train the policy, and plot the return curve and the sucess rate curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1z1_agKmOSp"
      },
      "outputs": [],
      "source": [
        "#### TODO: run train_ppo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBWE4jE2mSeM"
      },
      "outputs": [],
      "source": [
        "#### TODO: plot curves and play the video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg5r1P64md6l"
      },
      "source": [
        "# Reaching Task with an Obstacle\n",
        "\n",
        "Now that you can solve the reaching task without any obstacles in the environment. Let's make the task harder. What if there is a wall(obstacle) between the starting location and the goal location. The agent will need to learn to bypass the obstacle in order to reach the goal.\n",
        "\n",
        "Now, the environment looks like this:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/2_1.png\" height=\"300\"></iframe>\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/2_2.png\" height=\"300\"></iframe>\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/2_3.png\" height=\"300\"></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ_aDy_Snl5Y"
      },
      "source": [
        "## Simple Dense Reward\n",
        "\n",
        "As we have seen in the previous simpler task, using a sparse reward does not work. It's safe to say that using the sparse reward will not lead to success in this harder task either. So let's jump right into the dense reward case. Let's use the distance reward that is used in the previous section and try it on this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dz2CUHzobnT"
      },
      "source": [
        "**Q3 [20 pts]**: Fill in the code for dense reward when there is an obstacle (`use_sparse_reward=False, with_obstacle=True`), train the policy, and plot the return curve and the sucess rate curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKTuE-9CoUyp"
      },
      "outputs": [],
      "source": [
        "#### TODO: run train_ppo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleCpNExowZT"
      },
      "outputs": [],
      "source": [
        "#### TODO: plot curves, play the video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgvTyEkEpCcl"
      },
      "source": [
        "## Avoid the obstacle\n",
        "\n",
        "As we can see that using a dense reward based on the distance between the current end-effector position and the goal position does not solve the task sucessfully. What's even worse is that the end-effector is hitting the wall repeatedly. This can cause safety issues on a real robot. Idealy, we don't want the robot to hit the obstacles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s9wtmFMpofp"
      },
      "source": [
        "**Q4 [30 pts]**\n",
        "\n",
        "**Q4.1 [5 pts]**: Let's first check the collision rate ($\\frac{\\text{# of steps in collision}}{\\text{total # of episode steps}}$) of the policy learned in the previous case. We have saved the testing results in `data/`, all we need to do is just read out the collision information from the saved data. We have provided you with the utility function `check_collision_rate`. What's the collsion rate for the policy you trained with the simple dense reward? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8wuHSRkzsqh"
      },
      "outputs": [],
      "source": [
        "#### TODO: get the collision rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbqghTFpz00J"
      },
      "source": [
        "Now we want to train the robot to avoid the obstacle. A simple way to achieve this is to give the agent some penalty when it collides with the obstacle. \n",
        "\n",
        "**Q4.2 [20 pts]**: Apply the collision penalty when computing the reward value. Let's give $-5$ as the additional penalty whenever the agent hits the obstacle. Train the policy, and plot the return curve and the sucess rate curve. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjTmZOSg1Lxq"
      },
      "outputs": [],
      "source": [
        "#### TODO: run train_ppo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEC9Ms2y1R1j"
      },
      "outputs": [],
      "source": [
        "#### TODO: plot curves and play the video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNt8OUsV1T5M"
      },
      "source": [
        "**Q4.3 [5 pts]** What's the collision rate now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7klMK4sW1php"
      },
      "outputs": [],
      "source": [
        "#### TODO: get the collision rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjgpw65V2vcr"
      },
      "source": [
        "## Dense reward with subgoals\n",
        "\n",
        "As we can see in the previous section, if we add a collision penalty to the reward function, the agent can learn not to collide with the obstacle. However, it is still unable to reach the goal position as it gets stuck on the left side of the wall and never gets a chance to bypass it. We would need to design a better reward function.\n",
        "\n",
        "Let's assume that we know two subgoal locations in the scene (shown as the light blue ball in the figures below). They are on the two sides of the obstacle. Can we use these two subgoal locations to design a better reward function such that the robot can finally reach the goal location?\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/3_1.png\" height=\"300\"></iframe>\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/3_2.png\" height=\"300\"></iframe>\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/3_3.png\" height=\"300\"></iframe>\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/3_4.png\" height=\"300\"></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X13OT-k748s3"
      },
      "source": [
        "**Q5 [30 pts]**: Can you come up with a reward function with the subgoal information (`use_subgoal=True`) and make the robot reach the goal location? Write down the reward function $r(s_t, a_t)$ mathmatically. Same as before, train the policy and plot the return curve and the sucess rate curve. What's the collision rate in this case?\n",
        "\n",
        "**A**:\n",
        "$$r(s_t, a_t)=???$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4lG0kLb54Dd"
      },
      "outputs": [],
      "source": [
        "#### TODO: run train_ppo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fLquALd57Zg"
      },
      "outputs": [],
      "source": [
        "#### TODO: plot curves and play the video\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOLMgdow7PSP"
      },
      "outputs": [],
      "source": [
        "#### TODO: get the collision rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9aVL8x7ewPN"
      },
      "source": [
        "# Pushing Task\n",
        "\n",
        "Now that you can solve the reaching task, let's try another harder task. We would like our robot to push a box (the pink object shown in the figure below) on the table from its initial position to a goal position (the red ball in the figure).\n",
        "\n",
        "**State**: $[x_e, y_e, x_o, y_o]$, where $[x_e, y_e]$ is the 2D position of the end-effector tip, $[x_o, y_o]$ is the 2D position of the box.\n",
        "\n",
        "**Action**: same as before.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/dxyang/csl_imgs/master/hw3/4_1.png\"  height=\"300\"></iframe>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvHijgE-3BXU"
      },
      "outputs": [],
      "source": [
        "class URRobotPusherGym(gym.Env):\n",
        "    def __init__(self,\n",
        "                 action_repeat=10,\n",
        "                 gui=False,\n",
        "                 max_episode_length=25,\n",
        "                 dist_threshold=0.05):\n",
        "        self._action_repeat = action_repeat\n",
        "        self._max_episode_length = max_episode_length\n",
        "        self._dist_threshold = dist_threshold\n",
        "\n",
        "        self._xy_bounds = np.array([[0.23, 0.78],  # [xmin, xmax]\n",
        "                                    [-0.35, 0.3]])  # [ymin, ymax]\n",
        "        self.robot = Robot('ur5e_stick',\n",
        "                           pb_cfg={'gui': gui,\n",
        "                                   'realtime': False,\n",
        "                                   'opengl_render': torch.cuda.is_available()})\n",
        "        self._arm_reset_pos = np.array([-0.38337763,\n",
        "                                        -2.02650575,\n",
        "                                        -2.01989619,\n",
        "                                        -0.64477803,\n",
        "                                        1.571439041,\n",
        "                                        -0.38331266])\n",
        "        self._table_id = self.robot.pb_client.load_urdf('table/table.urdf',\n",
        "                                                        [.5, 0, 0.4],\n",
        "                                                        euler2quat([0, 0, np.pi / 2]),\n",
        "                                                        scaling=0.9)\n",
        "\n",
        "        # create a ball at the start location (for visualization purpose)\n",
        "        self._start_pos = np.array([0.45, -0.32, 1.0])\n",
        "        self._start_urdf_id = self.robot.pb_client.load_geom('sphere', size=0.04, mass=0,\n",
        "                                                             base_pos=self._start_pos,\n",
        "                                                             rgba=[1, 1, 0, 0.8])\n",
        "\n",
        "        # create a ball at the goal location\n",
        "        self._goal_pos = np.array([0.5, 0.2, 1.0])\n",
        "        self._goal_urdf_id = self.robot.pb_client.load_geom('sphere', size=0.04, mass=0,\n",
        "                                                            base_pos=self._goal_pos,\n",
        "                                                            rgba=[1, 0, 0, 0.8])\n",
        "\n",
        "        # disable the collision checking between the robot and the ball at the goal location\n",
        "        for i in range(self.robot.pb_client.getNumJoints(self.robot.arm.robot_id)):\n",
        "            self.robot.pb_client.setCollisionFilterPair(self.robot.arm.robot_id,\n",
        "                                                        self._goal_urdf_id,\n",
        "                                                        i,\n",
        "                                                        -1,\n",
        "                                                        enableCollision=0)\n",
        "        # disable the collision checking between the robot and the ball at the start location\n",
        "        for i in range(self.robot.pb_client.getNumJoints(self.robot.arm.robot_id)):\n",
        "            self.robot.pb_client.setCollisionFilterPair(self.robot.arm.robot_id,\n",
        "                                                        self._start_urdf_id,\n",
        "                                                        i,\n",
        "                                                        -1,\n",
        "                                                        enableCollision=0)\n",
        "\n",
        "        self._box_pos = np.array([0.35, -0.1, 0.996])\n",
        "        self._box_id = self.robot.pb_client.load_geom('cylinder', size=[0.05, 0.05], mass=1.,\n",
        "                                                      base_pos=self._box_pos,\n",
        "                                                      rgba=[1., 0.6, 0.6, 1])\n",
        "\n",
        "        self.robot.pb_client.changeDynamics(self._box_id, -1, lateralFriction=0.9)\n",
        "\n",
        "        self.robot.pb_client.setCollisionFilterPair(self._box_id,\n",
        "                                                    self._start_urdf_id,\n",
        "                                                    -1,\n",
        "                                                    -1,\n",
        "                                                    enableCollision=0)\n",
        "        self.robot.pb_client.setCollisionFilterPair(self._box_id,\n",
        "                                                    self._goal_urdf_id,\n",
        "                                                    -1,\n",
        "                                                    -1,\n",
        "                                                    enableCollision=0)\n",
        "\n",
        "        self._action_bound = 1.0\n",
        "        self._ee_pos_scale = 0.02\n",
        "        self._action_high = np.array([self._action_bound] * 2)\n",
        "        self.action_space = spaces.Box(low=-self._action_high,\n",
        "                                       high=self._action_high,\n",
        "                                       dtype=np.float32)\n",
        "        state_low = np.full(len(self._get_obs()), -float('inf'))\n",
        "        state_high = np.full(len(self._get_obs()), float('inf'))\n",
        "        self.observation_space = spaces.Box(state_low,\n",
        "                                            state_high,\n",
        "                                            dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.robot.arm.set_jpos(self._arm_reset_pos, ignore_physics=True)\n",
        "        self.robot.pb_client.reset_body(self._box_id, base_pos=self._box_pos)\n",
        "        self._t = 0\n",
        "        self._ref_ee_pos = self.robot.arm.get_ee_pose()[0]\n",
        "        self._ref_ee_ori = self.robot.arm.get_ee_pose()[1]\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "        previous_state = self._get_obs()\n",
        "        collision = self._apply_action(action)\n",
        "        self._t += 1\n",
        "        state = self._get_obs()\n",
        "        done = self._t >= self._max_episode_length\n",
        "        reward, info = self._get_reward(state=state, action=action, previous_state=previous_state)\n",
        "        info['collision'] = collision\n",
        "        return state, reward, done, info\n",
        "\n",
        "    def _get_reward(self, state, action, previous_state):\n",
        "        object_pos = state[2:4]\n",
        "        dist_to_goal = np.linalg.norm(object_pos - self._goal_pos[:2])\n",
        "        success = dist_to_goal < self._dist_threshold\n",
        "        #### TODO: design the reward given state, action, and previous state\n",
        "\n",
        "        ####\n",
        "        info = dict(success=success)\n",
        "        return reward, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        gripper_pos = self.robot.arm.get_ee_pose()[0][:2]\n",
        "        object_pos, object_quat = self.robot.pb_client.get_body_state(self._box_id)[:2]\n",
        "        state = np.concatenate([gripper_pos, object_pos[:2]])\n",
        "        return state\n",
        "\n",
        "    def _apply_action(self, action):\n",
        "        if not isinstance(action, np.ndarray):\n",
        "            action = np.array(action).flatten()\n",
        "        if action.size != 2:\n",
        "            raise ValueError('Action should be [d_x, d_y].')\n",
        "        # we set dz=0\n",
        "        action = np.append(action, 0)\n",
        "        pos, quat, rot_mat, euler = self.robot.arm.get_ee_pose()\n",
        "        pos += action[:3] * self._ee_pos_scale\n",
        "        pos[2] = self._ref_ee_pos[2]\n",
        "        # if the new position is out of the bounds, then we don't apply the action\n",
        "        if not np.logical_and(np.all(pos[:2] >= self._xy_bounds[:, 0]),\n",
        "                              np.all(pos[:2] <= self._xy_bounds[:, 1])):\n",
        "            return False\n",
        "\n",
        "        # move the end-effector to the new position\n",
        "        jnt_pos = self.robot.arm.compute_ik(pos, ori=self._ref_ee_ori)\n",
        "        for step in range(self._action_repeat):\n",
        "            self.robot.arm.set_jpos(jnt_pos)\n",
        "            self.robot.pb_client.stepSimulation()\n",
        "\n",
        "        return False\n",
        "\n",
        "    def render(self, mode, **kwargs):\n",
        "        robot_base = self.robot.arm.robot_base_pos\n",
        "        self.robot.cam.setup_camera(focus_pt=robot_base,\n",
        "                                    dist=2,\n",
        "                                    yaw=85,\n",
        "                                    pitch=-20,\n",
        "                                    roll=0)\n",
        "        rgb, _ = self.robot.cam.get_images(get_rgb=True,\n",
        "                                           get_depth=False)\n",
        "        return rgb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "module_name = __name__\n",
        "\n",
        "env_name = 'URPusher-v1'\n",
        "if env_name in registry.env_specs:\n",
        "    del registry.env_specs[env_name]\n",
        "register(\n",
        "    id=env_name,\n",
        "    entry_point=f'{module_name}:URRobotPusherGym',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiH89Cc4f-xx"
      },
      "source": [
        "**Q6 [40 pts]**: Can you design a reward function so that the robot can push the box to the goal position? Write down the reward function $r(s_t, a_t)$ mathmatically. Same as before, train the policy and plot the return curve and the sucess rate curve. If the reward function is good, you will see that the success rate will be at least above $0.8$ and the curve does not collapse to $0$ (or close to $0$) as the training proceeds. **HINTS**: If you use the negative of the distance between the object and the goal location as the reward function, does it work? What if you add another term to encourage the gripper to be close to the object? If the policy still does not learn, what other reward shaping terms are helpful for the training? The information you can use include the state before and after the robot executes the action, the action that the robot takes, the radius of the object is 0.05m, the radius of the end-effector stick is about $0.02\\sim0.03$m.\n",
        "\n",
        "**A**:\n",
        "$$r(s_t, a_t)=???$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljZbtmpwf_DK"
      },
      "outputs": [],
      "source": [
        "#### TODO: run train_ppo\n",
        "# you can reduce the number of steps for debugging purpose\n",
        "# but for the submission, you should run the experiment for at least 350000 steps (**max_steps=350000**)!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWDof69Pct_p"
      },
      "outputs": [],
      "source": [
        "#### TODO: plot the curves and play the video\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8hupn-S7nhu"
      },
      "source": [
        "# Feedback Survey (optional)\n",
        "\n",
        "Please enter the bonus code you get after filling out the [anonymous assignment survey](https://docs.google.com/forms/d/e/1FAIpQLScQE0Mlg5qzPZL6p2VhWqyAmWPlx8XFXfBgMoaJAoewOy7DZA/viewform?usp=sf_link). (10 pts).\n",
        "\n",
        "**Bonus code**: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9_leQg-u1z1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "reward_design.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}